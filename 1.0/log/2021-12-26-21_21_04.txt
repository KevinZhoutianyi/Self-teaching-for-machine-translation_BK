2021-12-26 21:21:05,631 |	  Reusing dataset opus_euconst (/home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)
2021-12-26 21:21:05,668 |	  DatasetDict({
    train: Dataset({
        features: ['translation'],
        num_rows: 10104
    })
})
2021-12-26 21:21:05,670 |	  {'translation': {'en': 'CONSIDERING that Article IV-437(2)(e) of the Constitution provides that the Treaty of 16 April 2003 concerning the accessions referred to above shall be repealed;  ', 'fr': "CONSIDÉRANT que l'article\xa0IV-437, paragraphe\xa02, point\xa0e), de la Constitution prévoit l'abrogation du traité du 16\xa0avril 2003 relatif aux adhésions visées ci-dessus;  "}}
2021-12-26 21:21:09,260 |	  Loading cached shuffled indices for dataset at /home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908/cache-774986f0005795ce.arrow
2021-12-26 21:21:09,698 |	  train len: 7578
2021-12-26 21:21:09,699 |	  valid len: 1263
2021-12-26 21:21:09,701 |	  test len: 1263
2021-12-26 21:21:09,701 |	  {'en': 'translate English to French:, on the basis of Article\xa02, and shall report thereon at least once a year.  ', 'fr': "L'Agence européenne de défense contribue à l'évaluation régulière des contributions des États membres participants en matière de capacités, en particulier des contributions fournies suivant les critères qui seront établis, entre autres, sur la base de l'article\xa02, et en fait rapport au moins une fois par an.  "}
2021-12-26 21:21:26,038 |	  Step count: 0
2021-12-26 21:21:45,916 |	  loss_w (train):3.4845939808292314e-05
2021-12-26 21:21:49,298 |	  v_loss (train):333.8587341308594
2021-12-26 21:21:49,652 |	  model_w_in_main test loss : 0.837824
2021-12-26 21:21:49,722 |	  model_v_in_main test loss : 0.831462
2021-12-26 21:21:49,726 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0099, -0.0099, -0.0099,  ..., -0.0099, -0.0099, -0.0099],
       device='cuda:0', requires_grad=True))
2021-12-26 21:21:49,728 |	  Step count: 1
2021-12-26 21:22:01,125 |	  loss_w (train):6.766241131117567e-05
2021-12-26 21:22:02,463 |	  v_loss (train):111.9421157836914
2021-12-26 21:22:02,853 |	  model_w_in_main test loss : 0.837778
2021-12-26 21:22:02,926 |	  model_v_in_main test loss : 0.868355
2021-12-26 21:22:02,930 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0182, -0.0182, -0.0182,  ..., -0.0182, -0.0182, -0.0182],
       device='cuda:0', requires_grad=True))
2021-12-26 21:22:02,933 |	  Step count: 2
2021-12-26 21:22:20,681 |	  loss_w (train):1.554307709739078e-05
2021-12-26 21:22:23,451 |	  v_loss (train):263.779052734375
2021-12-26 21:22:23,784 |	  model_w_in_main test loss : 0.837844
2021-12-26 21:22:23,842 |	  model_v_in_main test loss : 0.852040
2021-12-26 21:22:23,845 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0227, -0.0227, -0.0227,  ..., -0.0227, -0.0227, -0.0227],
       device='cuda:0', requires_grad=True))
2021-12-26 21:22:23,846 |	  Step count: 3
2021-12-26 21:22:33,633 |	  loss_w (train):1.6379937733290717e-05
2021-12-26 21:22:34,118 |	  v_loss (train):90.01312255859375
2021-12-26 21:22:34,474 |	  model_w_in_main test loss : 0.837775
2021-12-26 21:22:34,537 |	  model_v_in_main test loss : 0.853961
2021-12-26 21:22:34,540 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0257, -0.0257, -0.0257,  ..., -0.0257, -0.0257, -0.0257],
       device='cuda:0', requires_grad=True))
2021-12-26 21:22:34,542 |	  Step count: 4
2021-12-26 21:22:44,280 |	  loss_w (train):2.1330436084099347e-06
2021-12-26 21:22:44,817 |	  v_loss (train):107.91200256347656
2021-12-26 21:22:45,140 |	  model_w_in_main test loss : 0.837734
2021-12-26 21:22:45,204 |	  model_v_in_main test loss : 0.878200
2021-12-26 21:22:45,207 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0247, -0.0247, -0.0247,  ..., -0.0247, -0.0247, -0.0247],
       device='cuda:0', requires_grad=True))
2021-12-26 21:22:45,212 |	  Step count: 5
2021-12-26 21:22:56,348 |	  loss_w (train):7.858308526920155e-05
2021-12-26 21:22:57,350 |	  v_loss (train):158.43104553222656
2021-12-26 21:22:57,709 |	  model_w_in_main test loss : 0.837713
2021-12-26 21:22:57,774 |	  model_v_in_main test loss : 0.876339
2021-12-26 21:22:57,778 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0259, -0.0259, -0.0259,  ..., -0.0259, -0.0259, -0.0259],
       device='cuda:0', requires_grad=True))
2021-12-26 21:22:57,781 |	  Step count: 6
2021-12-26 21:23:07,133 |	  loss_w (train):8.64094981807284e-05
2021-12-26 21:23:07,637 |	  v_loss (train):82.34506225585938
2021-12-26 21:23:07,998 |	  model_w_in_main test loss : 0.837737
2021-12-26 21:23:08,061 |	  model_v_in_main test loss : 0.880960
2021-12-26 21:23:08,064 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0262, -0.0262, -0.0262,  ..., -0.0262, -0.0262, -0.0262],
       device='cuda:0', requires_grad=True))
2021-12-26 21:23:08,066 |	  Step count: 7
2021-12-26 21:23:18,220 |	  loss_w (train):4.451858785614604e-06
2021-12-26 21:23:19,000 |	  v_loss (train):143.1834259033203
2021-12-26 21:23:19,353 |	  model_w_in_main test loss : 0.837820
2021-12-26 21:23:19,419 |	  model_v_in_main test loss : 0.887228
2021-12-26 21:23:19,422 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0264, -0.0264, -0.0264,  ..., -0.0264, -0.0264, -0.0264],
       device='cuda:0', requires_grad=True))
2021-12-26 21:23:19,424 |	  Step count: 8
2021-12-26 21:23:28,920 |	  loss_w (train):2.421941280772444e-05
2021-12-26 21:23:29,407 |	  v_loss (train):76.80359649658203
2021-12-26 21:23:29,748 |	  model_w_in_main test loss : 0.837707
2021-12-26 21:23:29,812 |	  model_v_in_main test loss : 0.871066
2021-12-26 21:23:29,815 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0135, -0.0135, -0.0135,  ..., -0.0135, -0.0135, -0.0135],
       device='cuda:0', requires_grad=True))
2021-12-26 21:23:29,817 |	  Step count: 9
2021-12-26 21:23:47,021 |	  loss_w (train):8.365855137526523e-06
2021-12-26 21:23:49,759 |	  v_loss (train):198.57867431640625
2021-12-26 21:23:50,112 |	  model_w_in_main test loss : 0.837758
2021-12-26 21:23:50,173 |	  model_v_in_main test loss : 0.880966
2021-12-26 21:23:50,176 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0065, -0.0065, -0.0065,  ..., -0.0065, -0.0065, -0.0065],
       device='cuda:0', requires_grad=True))
2021-12-26 21:23:50,179 |	  Step count: 10
2021-12-26 21:23:59,611 |	  loss_w (train):3.635210305219516e-05
2021-12-26 21:24:00,116 |	  v_loss (train):103.07505798339844
2021-12-26 21:24:00,467 |	  model_w_in_main test loss : 0.837781
2021-12-26 21:24:00,531 |	  model_v_in_main test loss : 0.901313
2021-12-26 21:24:00,534 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],
       device='cuda:0', requires_grad=True))
2021-12-26 21:24:00,536 |	  Step count: 11
2021-12-26 21:24:14,287 |	  loss_w (train):1.7604561435291544e-05
2021-12-26 21:24:16,170 |	  v_loss (train):205.65298461914062
2021-12-26 21:24:16,494 |	  model_w_in_main test loss : 0.837805
2021-12-26 21:24:16,551 |	  model_v_in_main test loss : 0.910425
2021-12-26 21:24:16,554 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0022, -0.0022, -0.0022,  ..., -0.0022, -0.0022, -0.0022],
       device='cuda:0', requires_grad=True))
2021-12-26 21:24:16,555 |	  Step count: 12
2021-12-26 21:24:28,081 |	  loss_w (train):1.0456403288117144e-05
2021-12-26 21:24:29,458 |	  v_loss (train):153.5947265625
2021-12-26 21:24:29,784 |	  model_w_in_main test loss : 0.837647
2021-12-26 21:24:29,841 |	  model_v_in_main test loss : 0.903791
2021-12-26 21:24:29,844 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],
       device='cuda:0', requires_grad=True))
2021-12-26 21:24:29,846 |	  Step count: 13
2021-12-26 21:24:45,853 |	  loss_w (train):0.00010887128883041441
2021-12-26 21:24:48,389 |	  v_loss (train):195.16326904296875
2021-12-26 21:24:48,752 |	  model_w_in_main test loss : 0.837806
2021-12-26 21:24:48,815 |	  model_v_in_main test loss : 0.895039
2021-12-26 21:24:48,818 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],
       device='cuda:0', requires_grad=True))
2021-12-26 21:24:48,820 |	  Step count: 14
2021-12-26 21:24:58,978 |	  loss_w (train):1.320046249020379e-05
2021-12-26 21:24:59,705 |	  v_loss (train):68.32789611816406
2021-12-26 21:25:00,040 |	  model_w_in_main test loss : 0.837695
2021-12-26 21:25:00,101 |	  model_v_in_main test loss : 0.912959
2021-12-26 21:25:00,104 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],
       device='cuda:0', requires_grad=True))
2021-12-26 21:25:00,107 |	  Step count: 15
2021-12-26 21:25:10,208 |	  loss_w (train):5.452481218526373e-06
2021-12-26 21:25:11,270 |	  v_loss (train):61.9101676940918
2021-12-26 21:25:11,610 |	  model_w_in_main test loss : 0.837778
2021-12-26 21:25:11,673 |	  model_v_in_main test loss : 0.950460
2021-12-26 21:25:11,677 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],
       device='cuda:0', requires_grad=True))
2021-12-26 21:25:11,678 |	  Step count: 16
2021-12-26 21:25:22,224 |	  loss_w (train):3.573971116566099e-05
2021-12-26 21:25:23,081 |	  v_loss (train):107.85946655273438
2021-12-26 21:25:23,439 |	  model_w_in_main test loss : 0.837789
2021-12-26 21:25:23,506 |	  model_v_in_main test loss : 0.944061
2021-12-26 21:25:23,509 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0017, -0.0017, -0.0017,  ..., -0.0017, -0.0017, -0.0017],
       device='cuda:0', requires_grad=True))
2021-12-26 21:25:23,511 |	  Step count: 17
2021-12-26 21:25:36,389 |	  loss_w (train):4.6973482312751e-06
2021-12-26 21:25:38,110 |	  v_loss (train):180.8990478515625
2021-12-26 21:25:38,453 |	  model_w_in_main test loss : 0.837776
2021-12-26 21:25:38,514 |	  model_v_in_main test loss : 0.912931
2021-12-26 21:25:38,517 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],
       device='cuda:0', requires_grad=True))
2021-12-26 21:25:38,519 |	  Step count: 18
2021-12-26 21:25:51,703 |	  loss_w (train):2.2036351765564177e-06
2021-12-26 21:25:53,165 |	  v_loss (train):92.52146911621094
2021-12-26 21:25:53,504 |	  model_w_in_main test loss : 0.837776
2021-12-26 21:25:53,564 |	  model_v_in_main test loss : 0.939659
2021-12-26 21:25:53,567 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0022, -0.0022, -0.0022,  ..., -0.0022, -0.0022, -0.0022],
       device='cuda:0', requires_grad=True))
2021-12-26 21:25:53,568 |	  Step count: 19
2021-12-26 21:26:07,162 |	  loss_w (train):1.5511142919422127e-05
2021-12-26 21:26:08,934 |	  v_loss (train):142.01950073242188
2021-12-26 21:26:09,277 |	  model_w_in_main test loss : 0.837709
2021-12-26 21:26:09,336 |	  model_v_in_main test loss : 0.928027
2021-12-26 21:26:09,339 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],
       device='cuda:0', requires_grad=True))
2021-12-26 21:26:09,341 |	  Step count: 20
2021-12-26 21:26:22,375 |	  loss_w (train):3.401488356757909e-05
2021-12-26 21:26:23,983 |	  v_loss (train):106.68106079101562
2021-12-26 21:26:24,350 |	  model_w_in_main test loss : 0.837801
2021-12-26 21:26:24,417 |	  model_v_in_main test loss : 0.932936
2021-12-26 21:26:24,421 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003], device='cuda:0',
       requires_grad=True))
2021-12-26 21:26:24,423 |	  Step count: 21
2021-12-26 21:26:34,934 |	  loss_w (train):4.3723008275264874e-05
2021-12-26 21:26:36,114 |	  v_loss (train):38.077720642089844
2021-12-26 21:26:36,456 |	  model_w_in_main test loss : 0.837717
2021-12-26 21:26:36,518 |	  model_v_in_main test loss : 0.961371
2021-12-26 21:26:36,521 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0011, 0.0011, 0.0011,  ..., 0.0011, 0.0011, 0.0011], device='cuda:0',
       requires_grad=True))
2021-12-26 21:26:36,523 |	  Step count: 22
2021-12-26 21:26:45,702 |	  loss_w (train):1.103351451092749e-06
2021-12-26 21:26:46,202 |	  v_loss (train):34.52643585205078
2021-12-26 21:26:46,552 |	  model_w_in_main test loss : 0.837741
2021-12-26 21:26:46,618 |	  model_v_in_main test loss : 0.984266
2021-12-26 21:26:46,621 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0024, 0.0024, 0.0024,  ..., 0.0024, 0.0024, 0.0024], device='cuda:0',
       requires_grad=True))
2021-12-26 21:26:46,623 |	  Step count: 23
2021-12-26 21:26:58,659 |	  loss_w (train):8.425085979979485e-05
2021-12-26 21:27:00,094 |	  v_loss (train):96.24021911621094
2021-12-26 21:27:00,432 |	  model_w_in_main test loss : 0.837799
2021-12-26 21:27:00,492 |	  model_v_in_main test loss : 0.949818
2021-12-26 21:27:00,495 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0028, 0.0028, 0.0028,  ..., 0.0028, 0.0028, 0.0028], device='cuda:0',
       requires_grad=True))
2021-12-26 21:27:00,496 |	  Step count: 24
2021-12-26 21:27:10,454 |	  loss_w (train):1.5749733393022325e-06
2021-12-26 21:27:11,137 |	  v_loss (train):54.09656524658203
2021-12-26 21:27:11,488 |	  model_w_in_main test loss : 0.837681
2021-12-26 21:27:11,553 |	  model_v_in_main test loss : 1.011402
2021-12-26 21:27:11,556 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0024, 0.0024, 0.0024,  ..., 0.0024, 0.0024, 0.0024], device='cuda:0',
       requires_grad=True))
2021-12-26 21:27:11,558 |	  Step count: 25
2021-12-26 21:27:24,909 |	  loss_w (train):8.713848365005106e-05
2021-12-26 21:27:26,642 |	  v_loss (train):147.3315887451172
2021-12-26 21:27:26,991 |	  model_w_in_main test loss : 0.837700
2021-12-26 21:27:27,051 |	  model_v_in_main test loss : 0.998505
2021-12-26 21:27:27,054 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0026, 0.0026], device='cuda:0',
       requires_grad=True))
2021-12-26 21:27:27,056 |	  Step count: 26
2021-12-26 21:27:37,642 |	  loss_w (train):1.184666643894161e-06
2021-12-26 21:27:38,420 |	  v_loss (train):48.96347427368164
2021-12-26 21:27:38,753 |	  model_w_in_main test loss : 0.837680
2021-12-26 21:27:38,812 |	  model_v_in_main test loss : 0.935457
2021-12-26 21:27:38,815 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0029, 0.0029, 0.0029,  ..., 0.0029, 0.0029, 0.0029], device='cuda:0',
       requires_grad=True))
2021-12-26 21:27:38,817 |	  Step count: 27
2021-12-26 21:27:48,090 |	  loss_w (train):8.2224010839127e-05
2021-12-26 21:27:48,583 |	  v_loss (train):50.80058288574219
2021-12-26 21:27:48,932 |	  model_w_in_main test loss : 0.837611
2021-12-26 21:27:48,996 |	  model_v_in_main test loss : 0.993098
2021-12-26 21:27:48,999 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0016, 0.0016, 0.0016,  ..., 0.0016, 0.0016, 0.0016], device='cuda:0',
       requires_grad=True))
2021-12-26 21:27:49,001 |	  Step count: 28
2021-12-26 21:28:02,700 |	  loss_w (train):1.830031214922201e-05
2021-12-26 21:28:04,492 |	  v_loss (train):124.5927734375
2021-12-26 21:28:04,856 |	  model_w_in_main test loss : 0.837759
2021-12-26 21:28:04,921 |	  model_v_in_main test loss : 1.019274
2021-12-26 21:28:04,924 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002], device='cuda:0',
       requires_grad=True))
2021-12-26 21:28:04,926 |	  Step count: 29
2021-12-26 21:28:14,198 |	  loss_w (train):0.00014213498798198998
2021-12-26 21:28:14,689 |	  v_loss (train):29.31704330444336
2021-12-26 21:28:15,037 |	  model_w_in_main test loss : 0.837740
2021-12-26 21:28:15,102 |	  model_v_in_main test loss : 1.001963
2021-12-26 21:28:15,105 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0037, -0.0037, -0.0037,  ..., -0.0037, -0.0037, -0.0037],
       device='cuda:0', requires_grad=True))
2021-12-26 21:28:15,107 |	  Step count: 30
2021-12-26 21:28:26,583 |	  loss_w (train):1.830716610129457e-05
2021-12-26 21:28:27,789 |	  v_loss (train):97.48507690429688
2021-12-26 21:28:28,136 |	  model_w_in_main test loss : 0.837924
2021-12-26 21:28:28,198 |	  model_v_in_main test loss : 0.968046
2021-12-26 21:28:28,201 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125], device='cuda:0',
       requires_grad=True))
2021-12-26 21:28:28,203 |	  Step count: 31
2021-12-26 21:28:39,297 |	  loss_w (train):2.6750687538878992e-05
2021-12-26 21:28:40,491 |	  v_loss (train):15.24324893951416
2021-12-26 21:28:40,845 |	  model_w_in_main test loss : 0.837886
2021-12-26 21:28:40,912 |	  model_v_in_main test loss : 0.997801
2021-12-26 21:28:40,915 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0207, 0.0207, 0.0207,  ..., 0.0207, 0.0207, 0.0207], device='cuda:0',
       requires_grad=True))
2021-12-26 21:28:40,917 |	  Step count: 32
2021-12-26 21:28:53,701 |	  loss_w (train):2.0969067918485962e-05
2021-12-26 21:28:55,222 |	  v_loss (train):105.4897232055664
2021-12-26 21:28:55,584 |	  model_w_in_main test loss : 0.837904
2021-12-26 21:28:55,653 |	  model_v_in_main test loss : 1.046971
2021-12-26 21:28:55,658 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0248, 0.0248, 0.0248,  ..., 0.0248, 0.0248, 0.0248], device='cuda:0',
       requires_grad=True))
2021-12-26 21:28:55,660 |	  Step count: 33
2021-12-26 21:29:06,609 |	  loss_w (train):0.0001240108540514484
2021-12-26 21:29:07,554 |	  v_loss (train):64.3377685546875
2021-12-26 21:29:07,917 |	  model_w_in_main test loss : 0.837773
2021-12-26 21:29:07,984 |	  model_v_in_main test loss : 0.997889
2021-12-26 21:29:07,988 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0269, 0.0269, 0.0269,  ..., 0.0269, 0.0269, 0.0269], device='cuda:0',
       requires_grad=True))
2021-12-26 21:29:07,991 |	  Step count: 34
2021-12-26 21:29:17,448 |	  loss_w (train):0.00017095920338761061
2021-12-26 21:29:17,945 |	  v_loss (train):16.632179260253906
2021-12-26 21:29:18,294 |	  model_w_in_main test loss : 0.837803
2021-12-26 21:29:18,358 |	  model_v_in_main test loss : 1.022869
2021-12-26 21:29:18,361 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0368, 0.0368, 0.0368,  ..., 0.0368, 0.0368, 0.0368], device='cuda:0',
       requires_grad=True))
2021-12-26 21:29:18,363 |	  Step count: 35
2021-12-26 21:29:32,657 |	  loss_w (train):4.735474874451029e-07
2021-12-26 21:29:34,575 |	  v_loss (train):56.63972473144531
2021-12-26 21:29:34,947 |	  model_w_in_main test loss : 0.837810
2021-12-26 21:29:35,013 |	  model_v_in_main test loss : 0.964959
2021-12-26 21:29:35,016 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0420, 0.0420, 0.0420,  ..., 0.0420, 0.0420, 0.0420], device='cuda:0',
       requires_grad=True))
2021-12-26 21:29:35,017 |	  Step count: 36
2021-12-26 21:29:44,225 |	  loss_w (train):2.9185324819991365e-05
2021-12-26 21:29:44,718 |	  v_loss (train):25.564388275146484
2021-12-26 21:29:45,089 |	  model_w_in_main test loss : 0.837845
2021-12-26 21:29:45,153 |	  model_v_in_main test loss : 0.991259
2021-12-26 21:29:45,157 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0500, 0.0500, 0.0500,  ..., 0.0500, 0.0500, 0.0500], device='cuda:0',
       requires_grad=True))
2021-12-26 21:29:45,158 |	  Step count: 37
2021-12-26 21:29:58,800 |	  loss_w (train):1.5116717804630753e-05
2021-12-26 21:30:00,835 |	  v_loss (train):107.11442565917969
2021-12-26 21:30:01,198 |	  model_w_in_main test loss : 0.837843
2021-12-26 21:30:01,265 |	  model_v_in_main test loss : 0.929151
2021-12-26 21:30:01,268 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.0534, 0.0534, 0.0534,  ..., 0.0534, 0.0534, 0.0534], device='cuda:0',
       requires_grad=True))
2021-12-26 21:30:01,270 |	  Step count: 38
