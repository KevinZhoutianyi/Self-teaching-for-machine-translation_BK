2021-12-26 20:33:30,549 |	  Reusing dataset opus_euconst (/home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)
2021-12-26 20:33:30,578 |	  DatasetDict({
    train: Dataset({
        features: ['translation'],
        num_rows: 10104
    })
})
2021-12-26 20:33:30,580 |	  {'translation': {'en': 'CONSIDERING that Article IV-437(2)(e) of the Constitution provides that the Treaty of 16 April 2003 concerning the accessions referred to above shall be repealed;  ', 'fr': "CONSIDÉRANT que l'article\xa0IV-437, paragraphe\xa02, point\xa0e), de la Constitution prévoit l'abrogation du traité du 16\xa0avril 2003 relatif aux adhésions visées ci-dessus;  "}}
2021-12-26 20:33:33,339 |	  Loading cached shuffled indices for dataset at /home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908/cache-774986f0005795ce.arrow
2021-12-26 20:33:33,703 |	  train len: 7578
2021-12-26 20:33:33,705 |	  valid len: 1263
2021-12-26 20:33:33,705 |	  test len: 1263
2021-12-26 20:33:33,706 |	  {'en': 'translate English to French:, on the basis of Article\xa02, and shall report thereon at least once a year.  ', 'fr': "L'Agence européenne de défense contribue à l'évaluation régulière des contributions des États membres participants en matière de capacités, en particulier des contributions fournies suivant les critères qui seront établis, entre autres, sur la base de l'article\xa02, et en fait rapport au moins une fois par an.  "}
2021-12-26 20:33:44,607 |	  Step count: 0
2021-12-26 20:34:02,276 |	  loss_w (train):3.4845939808292314e-05
2021-12-26 20:34:05,178 |	  v_loss (train):333.8587341308594
2021-12-26 20:34:05,536 |	  model_w_in_main test loss : 0.837824
2021-12-26 20:34:05,602 |	  model_v_in_main test loss : 0.831462
2021-12-26 20:34:05,606 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0099, -0.0099, -0.0099,  ..., -0.0099, -0.0099, -0.0099],
       device='cuda:0', requires_grad=True))
2021-12-26 20:34:05,608 |	  Step count: 1
2021-12-26 20:34:15,135 |	  loss_w (train):6.766241131117567e-05
2021-12-26 20:34:16,317 |	  v_loss (train):111.9421157836914
2021-12-26 20:34:16,656 |	  model_w_in_main test loss : 0.837778
2021-12-26 20:34:16,717 |	  model_v_in_main test loss : 0.868355
2021-12-26 20:34:16,720 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0182, -0.0182, -0.0182,  ..., -0.0182, -0.0182, -0.0182],
       device='cuda:0', requires_grad=True))
2021-12-26 20:34:16,721 |	  Step count: 2
2021-12-26 20:34:33,639 |	  loss_w (train):1.554307709739078e-05
2021-12-26 20:34:36,442 |	  v_loss (train):263.779052734375
2021-12-26 20:34:36,794 |	  model_w_in_main test loss : 0.837844
2021-12-26 20:34:36,858 |	  model_v_in_main test loss : 0.852040
2021-12-26 20:34:36,862 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0227, -0.0227, -0.0227,  ..., -0.0227, -0.0227, -0.0227],
       device='cuda:0', requires_grad=True))
2021-12-26 20:34:36,864 |	  Step count: 3
2021-12-26 20:34:46,344 |	  loss_w (train):1.6379937733290717e-05
2021-12-26 20:34:46,820 |	  v_loss (train):90.01312255859375
2021-12-26 20:34:47,168 |	  model_w_in_main test loss : 0.837775
2021-12-26 20:34:47,232 |	  model_v_in_main test loss : 0.853961
2021-12-26 20:34:47,236 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0257, -0.0257, -0.0257,  ..., -0.0257, -0.0257, -0.0257],
       device='cuda:0', requires_grad=True))
2021-12-26 20:34:47,237 |	  Step count: 4
2021-12-26 20:34:56,736 |	  loss_w (train):2.1330436084099347e-06
2021-12-26 20:34:57,273 |	  v_loss (train):107.91200256347656
2021-12-26 20:34:57,629 |	  model_w_in_main test loss : 0.837734
2021-12-26 20:34:57,697 |	  model_v_in_main test loss : 0.878200
2021-12-26 20:34:57,701 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0247, -0.0247, -0.0247,  ..., -0.0247, -0.0247, -0.0247],
       device='cuda:0', requires_grad=True))
2021-12-26 20:34:57,703 |	  Step count: 5
2021-12-26 20:53:43,063 |	  Step count: 0
2021-12-26 20:53:52,518 |	  loss_w (train):2.1258689230307937e-05
2021-12-26 20:53:53,030 |	  v_loss (train):100.71737670898438
2021-12-26 20:53:53,344 |	  model_w_in_main test loss : 0.837812
2021-12-26 20:53:53,405 |	  model_v_in_main test loss : 0.919903
2021-12-26 20:53:53,408 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0231, -0.0231, -0.0231,  ..., -0.0231, -0.0231, -0.0231],
       device='cuda:0', requires_grad=True))
2021-12-26 20:53:53,410 |	  Step count: 1
2021-12-26 20:54:03,810 |	  loss_w (train):2.3242271709023044e-05
2021-12-26 20:54:04,961 |	  v_loss (train):81.34518432617188
2021-12-26 20:54:05,316 |	  model_w_in_main test loss : 0.837757
2021-12-26 20:54:05,381 |	  model_v_in_main test loss : 0.960314
2021-12-26 20:54:05,384 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0309, -0.0309, -0.0309,  ..., -0.0309, -0.0309, -0.0309],
       device='cuda:0', requires_grad=True))
2021-12-26 20:54:05,386 |	  Step count: 2
2021-12-26 20:54:15,139 |	  loss_w (train):2.4199707695515826e-05
2021-12-26 20:54:15,665 |	  v_loss (train):57.49793243408203
2021-12-26 20:54:16,016 |	  model_w_in_main test loss : 0.837760
2021-12-26 20:54:16,081 |	  model_v_in_main test loss : 0.964726
2021-12-26 20:54:16,085 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0351, -0.0351, -0.0351,  ..., -0.0351, -0.0351, -0.0351],
       device='cuda:0', requires_grad=True))
2021-12-26 20:54:16,086 |	  Step count: 3
2021-12-26 20:54:33,922 |	  loss_w (train):1.0486286555533297e-05
2021-12-26 20:54:37,071 |	  v_loss (train):383.91607666015625
2021-12-26 20:54:37,430 |	  model_w_in_main test loss : 0.837754
2021-12-26 20:54:37,495 |	  model_v_in_main test loss : 0.946024
2021-12-26 20:54:37,498 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0379, -0.0379, -0.0379,  ..., -0.0379, -0.0379, -0.0379],
       device='cuda:0', requires_grad=True))
2021-12-26 20:54:37,500 |	  Step count: 4
2021-12-26 20:54:53,588 |	  loss_w (train):5.374143802328035e-05
2021-12-26 20:54:55,616 |	  v_loss (train):197.5071563720703
2021-12-26 20:54:55,972 |	  model_w_in_main test loss : 0.837743
2021-12-26 20:54:56,037 |	  model_v_in_main test loss : 0.912684
2021-12-26 20:54:56,040 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0393, -0.0393, -0.0393,  ..., -0.0393, -0.0393, -0.0393],
       device='cuda:0', requires_grad=True))
2021-12-26 20:54:56,042 |	  Step count: 5
2021-12-26 20:55:08,537 |	  loss_w (train):8.592565791332163e-06
2021-12-26 20:55:10,114 |	  v_loss (train):227.71585083007812
2021-12-26 20:55:10,459 |	  model_w_in_main test loss : 0.837790
2021-12-26 20:55:10,519 |	  model_v_in_main test loss : 0.935839
2021-12-26 20:55:10,522 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0409, -0.0409, -0.0409,  ..., -0.0409, -0.0409, -0.0409],
       device='cuda:0', requires_grad=True))
2021-12-26 20:55:10,524 |	  Step count: 6
2021-12-26 20:55:19,790 |	  loss_w (train):1.2985557987121865e-05
2021-12-26 20:55:20,278 |	  v_loss (train):55.46699523925781
2021-12-26 20:55:20,619 |	  model_w_in_main test loss : 0.837767
2021-12-26 20:55:20,682 |	  model_v_in_main test loss : 0.912613
2021-12-26 20:55:20,685 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0422, -0.0422, -0.0422,  ..., -0.0422, -0.0422, -0.0422],
       device='cuda:0', requires_grad=True))
2021-12-26 20:55:20,687 |	  Step count: 7
2021-12-26 20:55:31,506 |	  loss_w (train):1.8672981241252273e-05
2021-12-26 20:55:32,479 |	  v_loss (train):82.14041137695312
2021-12-26 20:55:32,841 |	  model_w_in_main test loss : 0.837784
2021-12-26 20:55:32,908 |	  model_v_in_main test loss : 0.903399
2021-12-26 20:55:32,913 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0449, -0.0449, -0.0449,  ..., -0.0449, -0.0449, -0.0449],
       device='cuda:0', requires_grad=True))
2021-12-26 20:55:32,916 |	  Step count: 8
2021-12-26 20:55:47,930 |	  loss_w (train):2.7832093110191636e-05
2021-12-26 20:55:50,193 |	  v_loss (train):191.81614685058594
2021-12-26 20:55:50,531 |	  model_w_in_main test loss : 0.837723
2021-12-26 20:55:50,591 |	  model_v_in_main test loss : 0.994174
2021-12-26 20:55:50,594 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0452, -0.0452, -0.0452,  ..., -0.0452, -0.0452, -0.0452],
       device='cuda:0', requires_grad=True))
2021-12-26 20:55:50,596 |	  Step count: 9
2021-12-26 20:56:03,842 |	  loss_w (train):1.2416255231073592e-05
2021-12-26 20:56:05,493 |	  v_loss (train):243.30165100097656
2021-12-26 20:56:05,851 |	  model_w_in_main test loss : 0.837795
2021-12-26 20:56:05,917 |	  model_v_in_main test loss : 1.310081
2021-12-26 20:56:05,920 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0528, -0.0528, -0.0528,  ..., -0.0528, -0.0528, -0.0528],
       device='cuda:0', requires_grad=True))
2021-12-26 20:56:05,922 |	  Step count: 10
2021-12-26 20:56:15,982 |	  loss_w (train):2.876980147448194e-07
2021-12-26 20:56:16,685 |	  v_loss (train):165.3719024658203
2021-12-26 20:56:17,041 |	  model_w_in_main test loss : 0.837803
2021-12-26 20:56:17,106 |	  model_v_in_main test loss : 0.963221
2021-12-26 20:56:17,109 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0567, -0.0567, -0.0567,  ..., -0.0567, -0.0567, -0.0567],
       device='cuda:0', requires_grad=True))
2021-12-26 20:56:17,111 |	  Step count: 11
2021-12-26 20:56:27,330 |	  loss_w (train):1.117266401706729e-05
2021-12-26 20:56:28,044 |	  v_loss (train):50.1684455871582
2021-12-26 20:56:28,366 |	  model_w_in_main test loss : 0.837862
2021-12-26 20:56:28,427 |	  model_v_in_main test loss : 0.960163
2021-12-26 20:56:28,433 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0588, -0.0588, -0.0588,  ..., -0.0588, -0.0588, -0.0588],
       device='cuda:0', requires_grad=True))
2021-12-26 20:56:28,435 |	  Step count: 12
2021-12-26 20:56:40,011 |	  loss_w (train):1.729809696371376e-06
2021-12-26 20:56:41,194 |	  v_loss (train):71.6160659790039
2021-12-26 20:56:41,511 |	  model_w_in_main test loss : 0.837818
2021-12-26 20:56:41,571 |	  model_v_in_main test loss : 0.950976
2021-12-26 20:56:41,574 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0598, -0.0598, -0.0598,  ..., -0.0598, -0.0598, -0.0598],
       device='cuda:0', requires_grad=True))
2021-12-26 20:56:41,576 |	  Step count: 13
2021-12-26 20:56:53,566 |	  loss_w (train):8.881332905730233e-05
2021-12-26 20:56:54,879 |	  v_loss (train):69.24544525146484
2021-12-26 20:56:55,213 |	  model_w_in_main test loss : 0.837830
2021-12-26 20:56:55,274 |	  model_v_in_main test loss : 0.965004
2021-12-26 20:56:55,277 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0595, -0.0595, -0.0595,  ..., -0.0595, -0.0595, -0.0595],
       device='cuda:0', requires_grad=True))
2021-12-26 20:56:55,279 |	  Step count: 14
2021-12-26 20:57:07,220 |	  loss_w (train):1.723842797218822e-05
2021-12-26 20:57:08,429 |	  v_loss (train):54.730140686035156
2021-12-26 20:57:08,766 |	  model_w_in_main test loss : 0.837881
2021-12-26 20:57:08,824 |	  model_v_in_main test loss : 0.959934
2021-12-26 20:57:08,827 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0593, -0.0593, -0.0593,  ..., -0.0593, -0.0593, -0.0593],
       device='cuda:0', requires_grad=True))
2021-12-26 20:57:08,828 |	  Step count: 15
2021-12-26 20:57:18,920 |	  loss_w (train):1.8349892343394458e-05
2021-12-26 20:57:19,387 |	  v_loss (train):27.157272338867188
2021-12-26 20:57:19,787 |	  model_w_in_main test loss : 0.837706
2021-12-26 20:57:19,854 |	  model_v_in_main test loss : 0.963813
2021-12-26 20:57:19,858 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0601, -0.0601, -0.0601,  ..., -0.0601, -0.0601, -0.0601],
       device='cuda:0', requires_grad=True))
2021-12-26 20:57:19,860 |	  Step count: 16
2021-12-26 20:57:33,773 |	  loss_w (train):2.733071414695587e-05
2021-12-26 20:57:35,531 |	  v_loss (train):146.78591918945312
2021-12-26 20:57:35,877 |	  model_w_in_main test loss : 0.837735
2021-12-26 20:57:35,936 |	  model_v_in_main test loss : 0.953933
2021-12-26 20:57:35,939 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0615, -0.0615, -0.0615,  ..., -0.0615, -0.0615, -0.0615],
       device='cuda:0', requires_grad=True))
2021-12-26 20:57:35,941 |	  Step count: 17
2021-12-26 20:57:48,551 |	  loss_w (train):4.4193122448632494e-05
2021-12-26 20:57:49,771 |	  v_loss (train):69.36725616455078
2021-12-26 20:57:50,138 |	  model_w_in_main test loss : 0.837760
2021-12-26 20:57:50,204 |	  model_v_in_main test loss : 0.940952
2021-12-26 20:57:50,207 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0631, -0.0631, -0.0631,  ..., -0.0631, -0.0631, -0.0631],
       device='cuda:0', requires_grad=True))
2021-12-26 20:57:50,209 |	  Step count: 18
2021-12-26 20:58:07,205 |	  loss_w (train):4.745645492221229e-05
2021-12-26 20:58:10,071 |	  v_loss (train):181.0144805908203
2021-12-26 20:58:10,432 |	  model_w_in_main test loss : 0.837689
2021-12-26 20:58:10,496 |	  model_v_in_main test loss : 0.904876
2021-12-26 20:58:10,500 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0636, -0.0636, -0.0636,  ..., -0.0636, -0.0636, -0.0636],
       device='cuda:0', requires_grad=True))
2021-12-26 20:58:10,502 |	  Step count: 19
2021-12-26 20:58:20,551 |	  loss_w (train):1.3229270734882448e-05
2021-12-26 20:58:21,284 |	  v_loss (train):31.64769744873047
2021-12-26 20:58:21,628 |	  model_w_in_main test loss : 0.837800
2021-12-26 20:58:21,691 |	  model_v_in_main test loss : 0.922470
2021-12-26 20:58:21,694 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0639, -0.0639, -0.0639,  ..., -0.0639, -0.0639, -0.0639],
       device='cuda:0', requires_grad=True))
2021-12-26 20:58:21,696 |	  Step count: 20
2021-12-26 20:58:36,516 |	  loss_w (train):2.8649992600549012e-06
2021-12-26 20:58:38,028 |	  v_loss (train):162.3281707763672
2021-12-26 20:58:38,389 |	  model_w_in_main test loss : 0.837686
2021-12-26 20:58:38,453 |	  model_v_in_main test loss : 0.927362
2021-12-26 20:58:38,457 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0639, -0.0639, -0.0639,  ..., -0.0639, -0.0639, -0.0639],
       device='cuda:0', requires_grad=True))
2021-12-26 20:58:38,458 |	  Step count: 21
2021-12-26 20:58:53,266 |	  loss_w (train):2.046485860773828e-06
2021-12-26 20:58:55,309 |	  v_loss (train):205.79266357421875
2021-12-26 20:58:55,653 |	  model_w_in_main test loss : 0.837721
2021-12-26 20:58:55,712 |	  model_v_in_main test loss : 0.897792
2021-12-26 20:58:55,715 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0636, -0.0636, -0.0636,  ..., -0.0636, -0.0636, -0.0636],
       device='cuda:0', requires_grad=True))
2021-12-26 20:58:55,716 |	  Step count: 22
2021-12-26 20:59:06,075 |	  loss_w (train):1.3182443581172265e-05
2021-12-26 20:59:06,875 |	  v_loss (train):25.42675018310547
2021-12-26 20:59:07,232 |	  model_w_in_main test loss : 0.837698
2021-12-26 20:59:07,298 |	  model_v_in_main test loss : 0.918617
2021-12-26 20:59:07,301 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0635, -0.0635, -0.0635,  ..., -0.0635, -0.0635, -0.0635],
       device='cuda:0', requires_grad=True))
2021-12-26 20:59:07,303 |	  Step count: 23
2021-12-26 20:59:16,415 |	  loss_w (train):5.327998223947361e-05
2021-12-26 20:59:16,867 |	  v_loss (train):25.59775161743164
2021-12-26 20:59:17,183 |	  model_w_in_main test loss : 0.837715
2021-12-26 20:59:17,244 |	  model_v_in_main test loss : 0.934719
2021-12-26 20:59:17,247 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0622, -0.0622, -0.0622,  ..., -0.0622, -0.0622, -0.0622],
       device='cuda:0', requires_grad=True))
2021-12-26 20:59:17,249 |	  Step count: 24
2021-12-26 20:59:29,970 |	  loss_w (train):3.981052145718422e-07
2021-12-26 20:59:31,612 |	  v_loss (train):89.40339660644531
2021-12-26 20:59:31,958 |	  model_w_in_main test loss : 0.837666
2021-12-26 20:59:32,019 |	  model_v_in_main test loss : 0.914667
2021-12-26 20:59:32,022 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0615, -0.0615, -0.0615,  ..., -0.0615, -0.0615, -0.0615],
       device='cuda:0', requires_grad=True))
2021-12-26 20:59:32,024 |	  Step count: 25
2021-12-26 20:59:44,567 |	  loss_w (train):5.282615802570945e-07
2021-12-26 20:59:46,041 |	  v_loss (train):79.0453109741211
2021-12-26 20:59:46,404 |	  model_w_in_main test loss : 0.837734
2021-12-26 20:59:46,473 |	  model_v_in_main test loss : 0.913232
2021-12-26 20:59:46,476 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0611, -0.0611, -0.0611,  ..., -0.0611, -0.0611, -0.0611],
       device='cuda:0', requires_grad=True))
2021-12-26 20:59:46,478 |	  Step count: 26
2021-12-26 21:00:01,215 |	  loss_w (train):7.601622655784013e-06
2021-12-26 21:00:03,328 |	  v_loss (train):147.34532165527344
2021-12-26 21:00:03,669 |	  model_w_in_main test loss : 0.837706
2021-12-26 21:00:03,729 |	  model_v_in_main test loss : 0.878458
2021-12-26 21:00:03,733 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0613, -0.0613, -0.0613,  ..., -0.0613, -0.0613, -0.0613],
       device='cuda:0', requires_grad=True))
2021-12-26 21:00:03,734 |	  Step count: 27
2021-12-26 21:00:14,713 |	  loss_w (train):8.539191185263917e-05
2021-12-26 21:00:15,890 |	  v_loss (train):42.54327392578125
2021-12-26 21:00:16,226 |	  model_w_in_main test loss : 0.837600
2021-12-26 21:00:16,286 |	  model_v_in_main test loss : 0.874502
2021-12-26 21:00:16,289 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0618, -0.0618, -0.0618,  ..., -0.0618, -0.0618, -0.0618],
       device='cuda:0', requires_grad=True))
2021-12-26 21:00:16,290 |	  Step count: 28
2021-12-26 21:00:27,754 |	  loss_w (train):0.00010121380910277367
2021-12-26 21:00:28,766 |	  v_loss (train):43.35649108886719
2021-12-26 21:00:29,126 |	  model_w_in_main test loss : 0.837682
2021-12-26 21:00:29,190 |	  model_v_in_main test loss : 0.894679
2021-12-26 21:00:29,193 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0631, -0.0631, -0.0631,  ..., -0.0631, -0.0631, -0.0631],
       device='cuda:0', requires_grad=True))
2021-12-26 21:00:29,195 |	  Step count: 29
2021-12-26 21:00:47,061 |	  loss_w (train):1.963378508662572e-06
2021-12-26 21:00:50,184 |	  v_loss (train):773.9845581054688
2021-12-26 21:00:50,532 |	  model_w_in_main test loss : 0.837700
2021-12-26 21:00:50,591 |	  model_v_in_main test loss : 0.939733
2021-12-26 21:00:50,594 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0645, -0.0645, -0.0645,  ..., -0.0645, -0.0645, -0.0645],
       device='cuda:0', requires_grad=True))
2021-12-26 21:00:50,596 |	  Step count: 30
2021-12-26 21:01:02,456 |	  loss_w (train):2.1805357391713187e-05
2021-12-26 21:01:03,643 |	  v_loss (train):71.64421844482422
2021-12-26 21:01:03,997 |	  model_w_in_main test loss : 0.837596
2021-12-26 21:01:04,062 |	  model_v_in_main test loss : 0.947910
2021-12-26 21:01:04,065 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0613, -0.0613, -0.0613,  ..., -0.0613, -0.0613, -0.0613],
       device='cuda:0', requires_grad=True))
2021-12-26 21:01:04,067 |	  Step count: 31
2021-12-26 21:01:17,985 |	  loss_w (train):7.190342876128852e-05
2021-12-26 21:01:20,196 |	  v_loss (train):95.49214172363281
2021-12-26 21:01:20,559 |	  model_w_in_main test loss : 0.837609
2021-12-26 21:01:20,624 |	  model_v_in_main test loss : 0.937417
2021-12-26 21:01:20,627 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0587, -0.0587, -0.0587,  ..., -0.0587, -0.0587, -0.0587],
       device='cuda:0', requires_grad=True))
2021-12-26 21:01:20,629 |	  Step count: 32
2021-12-26 21:01:30,624 |	  loss_w (train):6.404121813829988e-05
2021-12-26 21:01:31,372 |	  v_loss (train):65.3325424194336
2021-12-26 21:01:31,730 |	  model_w_in_main test loss : 0.837561
2021-12-26 21:01:31,794 |	  model_v_in_main test loss : 0.957365
2021-12-26 21:01:31,797 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0564, -0.0564, -0.0564,  ..., -0.0564, -0.0564, -0.0564],
       device='cuda:0', requires_grad=True))
2021-12-26 21:01:31,799 |	  Step count: 33
2021-12-26 21:01:42,470 |	  loss_w (train):2.144017162208911e-05
2021-12-26 21:01:43,459 |	  v_loss (train):63.839813232421875
2021-12-26 21:01:43,803 |	  model_w_in_main test loss : 0.837635
2021-12-26 21:01:43,867 |	  model_v_in_main test loss : 0.949214
2021-12-26 21:01:43,871 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0551, -0.0551, -0.0551,  ..., -0.0551, -0.0551, -0.0551],
       device='cuda:0', requires_grad=True))
2021-12-26 21:01:43,873 |	  Step count: 34
2021-12-26 21:01:59,917 |	  loss_w (train):1.8199152691522613e-05
2021-12-26 21:02:02,522 |	  v_loss (train):172.3160400390625
2021-12-26 21:02:02,882 |	  model_w_in_main test loss : 0.837699
2021-12-26 21:02:02,946 |	  model_v_in_main test loss : 0.951304
2021-12-26 21:02:02,949 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.0544, -0.0544, -0.0544,  ..., -0.0544, -0.0544, -0.0544],
       device='cuda:0', requires_grad=True))
2021-12-26 21:02:02,951 |	  Step count: 35
