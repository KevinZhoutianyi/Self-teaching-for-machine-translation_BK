2021-12-28 14:27:38,507 |	  Reusing dataset opus_euconst (/home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)
2021-12-28 14:27:38,544 |	  DatasetDict({
    train: Dataset({
        features: ['translation'],
        num_rows: 10104
    })
})
2021-12-28 14:27:38,546 |	  {'translation': {'en': 'CONSIDERING that Article IV-437(2)(e) of the Constitution provides that the Treaty of 16 April 2003 concerning the accessions referred to above shall be repealed;  ', 'fr': "CONSIDÉRANT que l'article\xa0IV-437, paragraphe\xa02, point\xa0e), de la Constitution prévoit l'abrogation du traité du 16\xa0avril 2003 relatif aux adhésions visées ci-dessus;  "}}
2021-12-28 14:27:41,338 |	  Loading cached shuffled indices for dataset at /home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908/cache-774986f0005795ce.arrow
2021-12-28 14:27:41,828 |	  train len: 7578
2021-12-28 14:27:41,830 |	  valid len: 1263
2021-12-28 14:27:41,831 |	  test len: 1263
2021-12-28 14:27:41,832 |	  {'en': 'translate English to French:, on the basis of Article\xa02, and shall report thereon at least once a year.  ', 'fr': "L'Agence européenne de défense contribue à l'évaluation régulière des contributions des États membres participants en matière de capacités, en particulier des contributions fournies suivant les critères qui seront établis, entre autres, sur la base de l'article\xa02, et en fait rapport au moins une fois par an.  "}
2021-12-28 14:27:58,017 |	  Step count: 0
2021-12-28 14:27:58,086 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:08,568 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:08,876 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:10,174 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:10,258 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:10,569 |	  attentionweight:tensor([1.7863e-05, 1.7863e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:10,777 |	  loss_w (train):4.650828941521468e-06
2021-12-28 14:28:11,635 |	  v_loss (train):133.09278869628906
2021-12-28 14:28:12,177 |	  model_w_in_main test loss : 0.837779
2021-12-28 14:28:12,263 |	  model_v_in_main test loss : 0.831018
2021-12-28 14:28:12,267 |	  ('Attention Weights A : ', Parameter containing:
tensor([1.0001, 1.0001, 1.0001,  ..., 1.0001, 1.0001, 1.0001], device='cuda:0',
       requires_grad=True))
2021-12-28 14:28:12,274 |	  Step count: 1
2021-12-28 14:28:12,278 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:23,013 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:23,108 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:24,538 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:24,694 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:25,140 |	  attentionweight:tensor([4.3294e-05, 2.8516e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:25,198 |	  loss_w (train):7.832504343241453e-05
2021-12-28 14:28:26,441 |	  v_loss (train):132.05392456054688
2021-12-28 14:28:26,842 |	  model_w_in_main test loss : 0.838016
2021-12-28 14:28:27,122 |	  model_v_in_main test loss : 0.826753
2021-12-28 14:28:27,125 |	  ('Attention Weights A : ', Parameter containing:
tensor([1.1724, 1.1724, 1.1724,  ..., 1.1724, 1.1724, 1.1724], device='cuda:0',
       requires_grad=True))
2021-12-28 14:28:27,128 |	  Step count: 2
2021-12-28 14:28:27,131 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:39,641 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:39,805 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:41,386 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:41,467 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:41,860 |	  attentionweight:tensor([7.8025e-05, 5.6407e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:41,991 |	  loss_w (train):0.00019922330102417618
2021-12-28 14:28:43,401 |	  v_loss (train):163.48753356933594
2021-12-28 14:28:44,020 |	  model_w_in_main test loss : 0.837868
2021-12-28 14:28:44,076 |	  model_v_in_main test loss : 0.822383
2021-12-28 14:28:44,080 |	  ('Attention Weights A : ', Parameter containing:
tensor([0.7084, 0.7084, 0.7084,  ..., 0.7084, 0.7084, 0.7084], device='cuda:0',
       requires_grad=True))
2021-12-28 14:28:44,083 |	  Step count: 3
2021-12-28 14:28:44,093 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:54,700 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:54,769 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:55,959 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:56,143 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:56,524 |	  attentionweight:tensor([0.0011, 0.0011], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:28:56,648 |	  loss_w (train):0.0005346003454178572
2021-12-28 14:28:57,661 |	  v_loss (train):108.14949035644531
2021-12-28 14:28:58,283 |	  model_w_in_main test loss : 0.834970
2021-12-28 14:28:58,419 |	  model_v_in_main test loss : 0.822504
2021-12-28 14:28:58,423 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.3725, -0.3725, -0.3725,  ..., -0.3725, -0.3725, -0.3725],
       device='cuda:0', requires_grad=True))
2021-12-28 14:28:58,425 |	  Step count: 4
2021-12-28 14:28:58,427 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:12,806 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:13,110 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:16,899 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:17,089 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:17,480 |	  attentionweight:tensor([7.0197e-05, 5.8482e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:17,609 |	  loss_w (train):4.6752938942518085e-06
2021-12-28 14:29:20,979 |	  v_loss (train):244.98779296875
2021-12-28 14:29:21,566 |	  model_w_in_main test loss : 0.834966
2021-12-28 14:29:21,613 |	  model_v_in_main test loss : 0.826606
2021-12-28 14:29:21,617 |	  ('Attention Weights A : ', Parameter containing:
tensor([-0.9560, -0.9560, -0.9560,  ..., -0.9560, -0.9560, -0.9560],
       device='cuda:0', requires_grad=True))
2021-12-28 14:29:21,619 |	  Step count: 5
2021-12-28 14:29:21,622 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:36,739 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:36,920 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:41,075 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:41,157 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:41,613 |	  attentionweight:tensor([9.5713e-04, 7.9719e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:41,734 |	  loss_w (train):0.001229416229762137
2021-12-28 14:29:45,321 |	  v_loss (train):231.06304931640625
2021-12-28 14:29:45,854 |	  model_w_in_main test loss : 0.830379
2021-12-28 14:29:45,961 |	  model_v_in_main test loss : 0.823365
2021-12-28 14:29:45,964 |	  ('Attention Weights A : ', Parameter containing:
tensor([-1.7435, -1.7435, -1.7435,  ..., -1.7435, -1.7435, -1.7435],
       device='cuda:0', requires_grad=True))
2021-12-28 14:29:45,966 |	  Step count: 6
2021-12-28 14:29:45,970 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:55,543 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:55,672 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:56,835 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:56,928 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:57,336 |	  attentionweight:tensor([4.9797e-05, 6.4112e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:29:57,456 |	  loss_w (train):2.1596501937892754e-06
2021-12-28 14:29:58,236 |	  v_loss (train):116.8395004272461
2021-12-28 14:29:58,889 |	  model_w_in_main test loss : 0.830507
2021-12-28 14:29:58,947 |	  model_v_in_main test loss : 0.817785
2021-12-28 14:29:58,951 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.1664, -2.1664, -2.1664,  ..., -2.1664, -2.1664, -2.1664],
       device='cuda:0', requires_grad=True))
2021-12-28 14:29:58,953 |	  Step count: 7
2021-12-28 14:29:58,956 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:09,802 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:09,887 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:11,451 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:11,770 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:11,941 |	  attentionweight:tensor([4.1024e-05, 7.0312e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:12,076 |	  loss_w (train):0.00011569751222850755
2021-12-28 14:30:13,430 |	  v_loss (train):121.29312133789062
2021-12-28 14:30:13,955 |	  model_w_in_main test loss : 0.831156
2021-12-28 14:30:14,087 |	  model_v_in_main test loss : 0.818125
2021-12-28 14:30:14,092 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.5357, -2.5357, -2.5357,  ..., -2.5357, -2.5357, -2.5357],
       device='cuda:0', requires_grad=True))
2021-12-28 14:30:14,094 |	  Step count: 8
2021-12-28 14:30:14,099 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:35,654 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:35,840 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:42,816 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:42,915 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:43,329 |	  attentionweight:tensor([2.5940e-05, 5.2031e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:30:43,453 |	  loss_w (train):8.770095155341551e-05
2021-12-28 14:30:49,528 |	  v_loss (train):2250.16845703125
2021-12-28 14:30:50,158 |	  model_w_in_main test loss : 0.831747
2021-12-28 14:30:50,415 |	  model_v_in_main test loss : 0.813440
2021-12-28 14:30:50,419 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.5388, -2.5388, -2.5388,  ..., -2.5388, -2.5388, -2.5388],
       device='cuda:0', requires_grad=True))
2021-12-28 14:30:50,421 |	  Step count: 9
2021-12-28 14:30:50,423 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:00,072 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:00,261 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:01,495 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:01,779 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:01,976 |	  attentionweight:tensor([5.5932e-04, 2.3776e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:02,066 |	  loss_w (train):4.008775704278378e-06
2021-12-28 14:31:03,053 |	  v_loss (train):106.2115707397461
2021-12-28 14:31:03,665 |	  model_w_in_main test loss : 0.831602
2021-12-28 14:31:03,806 |	  model_v_in_main test loss : 0.819151
2021-12-28 14:31:03,810 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.5447, -2.5447, -2.5447,  ..., -2.5447, -2.5447, -2.5447],
       device='cuda:0', requires_grad=True))
2021-12-28 14:31:03,812 |	  Step count: 10
2021-12-28 14:31:03,818 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:14,174 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:14,259 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:16,112 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:16,206 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:16,595 |	  attentionweight:tensor([2.1785e-05, 2.1632e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:16,699 |	  loss_w (train):4.980180960956204e-07
2021-12-28 14:31:17,660 |	  v_loss (train):110.51311492919922
2021-12-28 14:31:18,231 |	  model_w_in_main test loss : 0.831651
2021-12-28 14:31:18,304 |	  model_v_in_main test loss : 0.825398
2021-12-28 14:31:18,308 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.5385, -2.5385, -2.5385,  ..., -2.5385, -2.5385, -2.5385],
       device='cuda:0', requires_grad=True))
2021-12-28 14:31:18,310 |	  Step count: 11
2021-12-28 14:31:18,347 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:35,840 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:35,927 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:40,647 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:40,710 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:40,889 |	  attentionweight:tensor([0.0007, 0.0007], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:41,149 |	  loss_w (train):0.00027885849704034626
2021-12-28 14:31:44,963 |	  v_loss (train):218.27999877929688
2021-12-28 14:31:45,546 |	  model_w_in_main test loss : 0.833588
2021-12-28 14:31:45,703 |	  model_v_in_main test loss : 0.829126
2021-12-28 14:31:45,707 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.5865, -2.5865, -2.5865,  ..., -2.5865, -2.5865, -2.5865],
       device='cuda:0', requires_grad=True))
2021-12-28 14:31:45,709 |	  Step count: 12
2021-12-28 14:31:45,713 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:58,815 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:31:58,953 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:01,913 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:02,198 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:02,392 |	  attentionweight:tensor([1.9081e-05, 1.9466e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:02,501 |	  loss_w (train):2.296444563398836e-06
2021-12-28 14:32:04,922 |	  v_loss (train):168.95895385742188
2021-12-28 14:32:05,461 |	  model_w_in_main test loss : 0.833516
2021-12-28 14:32:05,531 |	  model_v_in_main test loss : 0.841209
2021-12-28 14:32:05,534 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.6059, -2.6059, -2.6059,  ..., -2.6059, -2.6059, -2.6059],
       device='cuda:0', requires_grad=True))
2021-12-28 14:32:05,535 |	  Step count: 13
2021-12-28 14:32:05,599 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:23,601 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:23,693 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:29,398 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:29,490 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:29,901 |	  attentionweight:tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:30,022 |	  loss_w (train):0.00014473828196059912
2021-12-28 14:32:34,436 |	  v_loss (train):229.6787567138672
2021-12-28 14:32:35,013 |	  model_w_in_main test loss : 0.833953
2021-12-28 14:32:35,171 |	  model_v_in_main test loss : 0.837329
2021-12-28 14:32:35,177 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.7369, -2.7369, -2.7369,  ..., -2.7369, -2.7369, -2.7369],
       device='cuda:0', requires_grad=True))
2021-12-28 14:32:35,180 |	  Step count: 14
2021-12-28 14:32:35,184 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:47,584 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:47,769 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:50,230 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:50,512 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:50,700 |	  attentionweight:tensor([2.7471e-05, 2.7471e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:32:50,833 |	  loss_w (train):2.6225802685075905e-07
2021-12-28 14:32:52,952 |	  v_loss (train):127.22348022460938
2021-12-28 14:32:53,567 |	  model_w_in_main test loss : 0.833895
2021-12-28 14:32:53,620 |	  model_v_in_main test loss : 0.833182
2021-12-28 14:32:53,624 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.8042, -2.8042, -2.8042,  ..., -2.8042, -2.8042, -2.8042],
       device='cuda:0', requires_grad=True))
2021-12-28 14:32:53,626 |	  Step count: 15
2021-12-28 14:32:53,628 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:05,569 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:05,661 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:08,606 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:08,706 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:09,087 |	  attentionweight:tensor([1.5518e-05, 7.6410e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:09,196 |	  loss_w (train):5.159928605280584e-06
2021-12-28 14:33:11,116 |	  v_loss (train):143.71151733398438
2021-12-28 14:33:11,717 |	  model_w_in_main test loss : 0.834352
2021-12-28 14:33:11,872 |	  model_v_in_main test loss : 0.829889
2021-12-28 14:33:11,876 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.8326, -2.8326, -2.8326,  ..., -2.8326, -2.8326, -2.8326],
       device='cuda:0', requires_grad=True))
2021-12-28 14:33:11,878 |	  Step count: 16
2021-12-28 14:33:11,882 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:33,292 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:33,411 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:40,345 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:40,524 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:40,958 |	  attentionweight:tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:33:41,008 |	  loss_w (train):1.4677153558295686e-05
2021-12-28 14:33:47,169 |	  v_loss (train):402.7598876953125
2021-12-28 14:33:47,783 |	  model_w_in_main test loss : 0.834630
2021-12-28 14:33:47,836 |	  model_v_in_main test loss : 0.831444
2021-12-28 14:33:47,841 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.8610, -2.8610, -2.8610,  ..., -2.8610, -2.8610, -2.8610],
       device='cuda:0', requires_grad=True))
2021-12-28 14:33:47,843 |	  Step count: 17
2021-12-28 14:33:47,845 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:01,946 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:02,272 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:05,412 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:05,478 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:05,859 |	  attentionweight:tensor([1.3262e-05, 1.2897e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:05,961 |	  loss_w (train):1.6736709085307666e-07
2021-12-28 14:34:09,060 |	  v_loss (train):140.18133544921875
2021-12-28 14:34:09,648 |	  model_w_in_main test loss : 0.834713
2021-12-28 14:34:09,696 |	  model_v_in_main test loss : 0.830181
2021-12-28 14:34:09,699 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.8367, -2.8367, -2.8367,  ..., -2.8367, -2.8367, -2.8367],
       device='cuda:0', requires_grad=True))
2021-12-28 14:34:09,701 |	  Step count: 18
2021-12-28 14:34:09,703 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:30,195 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:30,384 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:36,985 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:37,279 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:37,480 |	  attentionweight:tensor([9.8834e-04, 1.2894e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:34:37,615 |	  loss_w (train):1.3401388969214167e-05
2021-12-28 14:34:43,388 |	  v_loss (train):275.57891845703125
2021-12-28 14:34:43,986 |	  model_w_in_main test loss : 0.834569
2021-12-28 14:34:44,039 |	  model_v_in_main test loss : 0.825639
2021-12-28 14:34:44,043 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.8766, -2.8766, -2.8766,  ..., -2.8766, -2.8766, -2.8766],
       device='cuda:0', requires_grad=True))
2021-12-28 14:34:44,046 |	  Step count: 19
2021-12-28 14:34:44,048 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:01,793 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:01,866 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:04,390 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:04,475 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:04,851 |	  attentionweight:tensor([1.3794e-05, 1.0340e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:04,953 |	  loss_w (train):0.00011084411380579695
2021-12-28 14:35:07,027 |	  v_loss (train):114.80343627929688
2021-12-28 14:35:07,590 |	  model_w_in_main test loss : 0.836426
2021-12-28 14:35:07,746 |	  model_v_in_main test loss : 0.831073
2021-12-28 14:35:07,750 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.9141, -2.9141, -2.9141,  ..., -2.9141, -2.9141, -2.9141],
       device='cuda:0', requires_grad=True))
2021-12-28 14:35:07,751 |	  Step count: 20
2021-12-28 14:35:07,754 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:21,447 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:21,631 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:25,545 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:25,629 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:25,989 |	  attentionweight:tensor([1.1553e-05, 8.2253e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:26,091 |	  loss_w (train):1.7705183381622192e-06
2021-12-28 14:35:28,671 |	  v_loss (train):154.60145568847656
2021-12-28 14:35:29,277 |	  model_w_in_main test loss : 0.836721
2021-12-28 14:35:29,389 |	  model_v_in_main test loss : 0.832018
2021-12-28 14:35:29,393 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.9325, -2.9325, -2.9325,  ..., -2.9325, -2.9325, -2.9325],
       device='cuda:0', requires_grad=True))
2021-12-28 14:35:29,394 |	  Step count: 21
2021-12-28 14:35:29,397 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:44,278 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:44,463 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:48,338 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:48,399 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:48,809 |	  attentionweight:tensor([1.0260e-05, 1.0872e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:35:48,873 |	  loss_w (train):5.099595000501722e-06
2021-12-28 14:35:52,138 |	  v_loss (train):124.33726501464844
2021-12-28 14:35:52,762 |	  model_w_in_main test loss : 0.836610
2021-12-28 14:35:52,811 |	  model_v_in_main test loss : 0.831329
2021-12-28 14:35:52,814 |	  ('Attention Weights A : ', Parameter containing:
tensor([-2.9312, -2.9312, -2.9312,  ..., -2.9312, -2.9312, -2.9312],
       device='cuda:0', requires_grad=True))
2021-12-28 14:35:52,816 |	  Step count: 22
2021-12-28 14:35:52,826 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:03,866 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:03,957 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:06,963 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:07,051 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:07,432 |	  attentionweight:tensor([0.0015, 0.0015], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:07,533 |	  loss_w (train):7.80098489485681e-05
2021-12-28 14:36:08,855 |	  v_loss (train):106.40972900390625
2021-12-28 14:36:09,449 |	  model_w_in_main test loss : 0.836359
2021-12-28 14:36:09,563 |	  model_v_in_main test loss : 0.831589
2021-12-28 14:36:09,568 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.2241, -3.2241, -3.2241,  ..., -3.2241, -3.2241, -3.2241],
       device='cuda:0', requires_grad=True))
2021-12-28 14:36:09,570 |	  Step count: 23
2021-12-28 14:36:09,574 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:21,703 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:21,858 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:23,019 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:23,142 |	  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:23,489 |	  attentionweight:tensor([1.3690e-03, 1.0490e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:23,631 |	  loss_w (train):9.334636706626043e-05
2021-12-28 14:36:24,366 |	  v_loss (train):73.84162902832031
2021-12-28 14:36:25,014 |	  model_w_in_main test loss : 0.833865
2021-12-28 14:36:25,073 |	  model_v_in_main test loss : 0.832602
2021-12-28 14:36:25,077 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.4066, -3.4066, -3.4066,  ..., -3.4066, -3.4066, -3.4066],
       device='cuda:0', requires_grad=True))
2021-12-28 14:36:25,079 |	  Step count: 24
2021-12-28 14:36:25,089 |	  attentionweight:tensor([9.9923e-05, 9.9923e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:35,019 |	  attentionweight:tensor([9.9923e-05, 9.9923e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:35,112 |	  attentionweight:tensor([9.9923e-05, 9.9923e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:36,210 |	  attentionweight:tensor([9.9923e-05, 9.9923e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:36,387 |	  attentionweight:tensor([9.9923e-05, 9.9923e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:36,733 |	  attentionweight:tensor([1.3458e-03, 1.8171e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:36,819 |	  loss_w (train):0.0003591426648199558
2021-12-28 14:36:37,802 |	  v_loss (train):88.36308288574219
2021-12-28 14:36:38,420 |	  model_w_in_main test loss : 0.832477
2021-12-28 14:36:38,473 |	  model_v_in_main test loss : 0.831209
2021-12-28 14:36:38,477 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.5596, -3.5596, -3.5596,  ..., -3.5596, -3.5596, -3.5596],
       device='cuda:0', requires_grad=True))
2021-12-28 14:36:38,479 |	  Step count: 25
2021-12-28 14:36:38,481 |	  attentionweight:tensor([9.6258e-05, 9.6258e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:50,531 |	  attentionweight:tensor([9.6258e-05, 9.6258e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:50,635 |	  attentionweight:tensor([9.6258e-05, 9.6258e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:52,519 |	  attentionweight:tensor([9.6258e-05, 9.6258e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:52,613 |	  attentionweight:tensor([9.6258e-05, 9.6258e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:53,021 |	  attentionweight:tensor([8.3254e-06, 1.3221e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:36:53,135 |	  loss_w (train):0.0001531729503767565
2021-12-28 14:36:54,504 |	  v_loss (train):85.40553283691406
2021-12-28 14:36:55,131 |	  model_w_in_main test loss : 0.830908
2021-12-28 14:36:55,190 |	  model_v_in_main test loss : 0.832645
2021-12-28 14:36:55,195 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.6844, -3.6844, -3.6844,  ..., -3.6844, -3.6844, -3.6844],
       device='cuda:0', requires_grad=True))
2021-12-28 14:36:55,197 |	  Step count: 26
2021-12-28 14:36:55,208 |	  attentionweight:tensor([9.2653e-05, 9.2653e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:11,567 |	  attentionweight:tensor([9.2653e-05, 9.2653e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:11,661 |	  attentionweight:tensor([9.2653e-05, 9.2653e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:15,762 |	  attentionweight:tensor([9.2653e-05, 9.2653e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:15,854 |	  attentionweight:tensor([9.2653e-05, 9.2653e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:16,246 |	  attentionweight:tensor([7.0213e-06, 7.0144e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:16,368 |	  loss_w (train):1.8360618696533493e-06
2021-12-28 14:37:19,979 |	  v_loss (train):174.27005004882812
2021-12-28 14:37:20,577 |	  model_w_in_main test loss : 0.830981
2021-12-28 14:37:20,631 |	  model_v_in_main test loss : 0.838072
2021-12-28 14:37:20,635 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.7101, -3.7101, -3.7101,  ..., -3.7101, -3.7101, -3.7101],
       device='cuda:0', requires_grad=True))
2021-12-28 14:37:20,637 |	  Step count: 27
2021-12-28 14:37:20,639 |	  attentionweight:tensor([9.0368e-05, 9.0368e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:37,396 |	  attentionweight:tensor([9.0368e-05, 9.0368e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:37,584 |	  attentionweight:tensor([9.0368e-05, 9.0368e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:42,120 |	  attentionweight:tensor([9.0368e-05, 9.0368e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:42,383 |	  attentionweight:tensor([9.0368e-05, 9.0368e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:42,574 |	  attentionweight:tensor([7.2751e-06, 1.3941e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:42,833 |	  loss_w (train):0.0010533416643738747
2021-12-28 14:37:46,921 |	  v_loss (train):198.41473388671875
2021-12-28 14:37:47,525 |	  model_w_in_main test loss : 0.833388
2021-12-28 14:37:47,579 |	  model_v_in_main test loss : 0.838477
2021-12-28 14:37:47,583 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.8571, -3.8571, -3.8571,  ..., -3.8571, -3.8571, -3.8571],
       device='cuda:0', requires_grad=True))
2021-12-28 14:37:47,585 |	  Step count: 28
2021-12-28 14:37:47,587 |	  attentionweight:tensor([8.6928e-05, 8.6928e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:57,687 |	  attentionweight:tensor([8.6928e-05, 8.6928e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:57,779 |	  attentionweight:tensor([8.6928e-05, 8.6928e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:59,055 |	  attentionweight:tensor([8.6928e-05, 8.6928e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:59,133 |	  attentionweight:tensor([8.6928e-05, 8.6928e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:59,525 |	  attentionweight:tensor([6.2442e-06, 1.3070e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:37:59,646 |	  loss_w (train):0.0002308709517819807
2021-12-28 14:38:00,447 |	  v_loss (train):80.1788330078125
2021-12-28 14:38:01,097 |	  model_w_in_main test loss : 0.833450
2021-12-28 14:38:01,156 |	  model_v_in_main test loss : 0.832034
2021-12-28 14:38:01,159 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9267, -3.9267, -3.9267,  ..., -3.9267, -3.9267, -3.9267],
       device='cuda:0', requires_grad=True))
2021-12-28 14:38:01,162 |	  Step count: 29
2021-12-28 14:38:01,164 |	  attentionweight:tensor([8.4507e-05, 8.4507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:11,952 |	  attentionweight:tensor([8.4507e-05, 8.4507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:12,036 |	  attentionweight:tensor([8.4507e-05, 8.4507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:13,729 |	  attentionweight:tensor([8.4507e-05, 8.4507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:13,802 |	  attentionweight:tensor([8.4507e-05, 8.4507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:13,994 |	  attentionweight:tensor([5.4404e-06, 1.1696e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:14,084 |	  loss_w (train):3.8022369608370354e-06
2021-12-28 14:38:15,326 |	  v_loss (train):96.56929016113281
2021-12-28 14:38:15,891 |	  model_w_in_main test loss : 0.833462
2021-12-28 14:38:15,985 |	  model_v_in_main test loss : 0.831493
2021-12-28 14:38:15,988 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9193, -3.9193, -3.9193,  ..., -3.9193, -3.9193, -3.9193],
       device='cuda:0', requires_grad=True))
2021-12-28 14:38:15,990 |	  Step count: 30
2021-12-28 14:38:15,993 |	  attentionweight:tensor([8.3121e-05, 8.3121e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:27,096 |	  attentionweight:tensor([8.3121e-05, 8.3121e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:27,213 |	  attentionweight:tensor([8.3121e-05, 8.3121e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:28,876 |	  attentionweight:tensor([8.3121e-05, 8.3121e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:28,997 |	  attentionweight:tensor([8.3121e-05, 8.3121e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:29,422 |	  attentionweight:tensor([5.1251e-06, 1.2833e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:29,468 |	  loss_w (train):4.397518205223605e-05
2021-12-28 14:38:30,748 |	  v_loss (train):85.16763305664062
2021-12-28 14:38:31,307 |	  model_w_in_main test loss : 0.833646
2021-12-28 14:38:31,460 |	  model_v_in_main test loss : 0.833637
2021-12-28 14:38:31,465 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9122, -3.9122, -3.9122,  ..., -3.9122, -3.9122, -3.9122],
       device='cuda:0', requires_grad=True))
2021-12-28 14:38:31,467 |	  Step count: 31
2021-12-28 14:38:31,472 |	  attentionweight:tensor([8.1723e-05, 8.1723e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:47,131 |	  attentionweight:tensor([8.1723e-05, 8.1723e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:47,215 |	  attentionweight:tensor([8.1723e-05, 8.1723e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:51,554 |	  attentionweight:tensor([8.1723e-05, 8.1723e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:51,630 |	  attentionweight:tensor([8.1723e-05, 8.1723e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:51,818 |	  attentionweight:tensor([0.0011, 0.0013], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:38:51,890 |	  loss_w (train):5.790100840386003e-05
2021-12-28 14:38:55,789 |	  v_loss (train):223.63954162597656
2021-12-28 14:38:56,377 |	  model_w_in_main test loss : 0.835419
2021-12-28 14:38:56,464 |	  model_v_in_main test loss : 0.835330
2021-12-28 14:38:56,467 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9140, -3.9140, -3.9140,  ..., -3.9140, -3.9140, -3.9140],
       device='cuda:0', requires_grad=True))
2021-12-28 14:38:56,469 |	  Step count: 32
2021-12-28 14:38:56,472 |	  attentionweight:tensor([8.0167e-05, 8.0167e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:14,499 |	  attentionweight:tensor([8.0167e-05, 8.0167e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:14,812 |	  attentionweight:tensor([8.0167e-05, 8.0167e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:19,744 |	  attentionweight:tensor([8.0167e-05, 8.0167e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:19,813 |	  attentionweight:tensor([8.0167e-05, 8.0167e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:20,192 |	  attentionweight:tensor([1.3512e-03, 4.6284e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:20,303 |	  loss_w (train):0.0002788364654406905
2021-12-28 14:39:24,439 |	  v_loss (train):248.5216827392578
2021-12-28 14:39:25,035 |	  model_w_in_main test loss : 0.835033
2021-12-28 14:39:25,081 |	  model_v_in_main test loss : 0.834536
2021-12-28 14:39:25,085 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9164, -3.9164, -3.9164,  ..., -3.9164, -3.9164, -3.9164],
       device='cuda:0', requires_grad=True))
2021-12-28 14:39:25,087 |	  Step count: 33
2021-12-28 14:39:25,089 |	  attentionweight:tensor([7.8457e-05, 7.8457e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:40,069 |	  attentionweight:tensor([7.8457e-05, 7.8457e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:40,173 |	  attentionweight:tensor([7.8457e-05, 7.8457e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:43,966 |	  attentionweight:tensor([7.8457e-05, 7.8457e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:44,257 |	  attentionweight:tensor([7.8457e-05, 7.8457e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:44,451 |	  attentionweight:tensor([4.2214e-06, 1.3578e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:39:44,576 |	  loss_w (train):0.0001773988624336198
2021-12-28 14:39:47,934 |	  v_loss (train):181.38778686523438
2021-12-28 14:39:48,483 |	  model_w_in_main test loss : 0.835322
2021-12-28 14:39:48,585 |	  model_v_in_main test loss : 0.835058
2021-12-28 14:39:48,588 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9117, -3.9117, -3.9117,  ..., -3.9117, -3.9117, -3.9117],
       device='cuda:0', requires_grad=True))
2021-12-28 14:39:48,590 |	  Step count: 34
2021-12-28 14:39:48,592 |	  attentionweight:tensor([7.6366e-05, 7.6366e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:03,132 |	  attentionweight:tensor([7.6366e-05, 7.6366e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:03,456 |	  attentionweight:tensor([7.6366e-05, 7.6366e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:07,754 |	  attentionweight:tensor([7.6366e-05, 7.6366e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:07,831 |	  attentionweight:tensor([7.6366e-05, 7.6366e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:08,212 |	  attentionweight:tensor([3.9111e-06, 1.1928e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:08,312 |	  loss_w (train):0.00011097492097178474
2021-12-28 14:40:11,986 |	  v_loss (train):114.36007690429688
2021-12-28 14:40:12,570 |	  model_w_in_main test loss : 0.835423
2021-12-28 14:40:12,619 |	  model_v_in_main test loss : 0.831300
2021-12-28 14:40:12,623 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9045, -3.9045, -3.9045,  ..., -3.9045, -3.9045, -3.9045],
       device='cuda:0', requires_grad=True))
2021-12-28 14:40:12,624 |	  Step count: 35
2021-12-28 14:40:12,627 |	  attentionweight:tensor([7.3814e-05, 7.3814e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:25,945 |	  attentionweight:tensor([7.3814e-05, 7.3814e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:26,024 |	  attentionweight:tensor([7.3814e-05, 7.3814e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:27,198 |	  attentionweight:tensor([7.3814e-05, 7.3814e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:27,345 |	  attentionweight:tensor([7.3814e-05, 7.3814e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:27,788 |	  attentionweight:tensor([0.0015, 0.0003], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:27,841 |	  loss_w (train):0.0001854471192928031
2021-12-28 14:40:29,803 |	  v_loss (train):71.40040588378906
2021-12-28 14:40:30,350 |	  model_w_in_main test loss : 0.834848
2021-12-28 14:40:30,562 |	  model_v_in_main test loss : 0.825497
2021-12-28 14:40:30,586 |	  ('Attention Weights A : ', Parameter containing:
tensor([-3.9243, -3.9243, -3.9243,  ..., -3.9243, -3.9243, -3.9243],
       device='cuda:0', requires_grad=True))
2021-12-28 14:40:30,588 |	  Step count: 36
2021-12-28 14:40:30,612 |	  attentionweight:tensor([7.4644e-05, 7.4644e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:50,944 |	  attentionweight:tensor([7.4644e-05, 7.4644e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:51,047 |	  attentionweight:tensor([7.4644e-05, 7.4644e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:57,023 |	  attentionweight:tensor([7.4644e-05, 7.4644e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:57,195 |	  attentionweight:tensor([7.4644e-05, 7.4644e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:57,579 |	  attentionweight:tensor([0.0018, 0.0018], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:40:57,634 |	  loss_w (train):0.0005774289602413774
2021-12-28 14:41:02,409 |	  v_loss (train):204.45773315429688
2021-12-28 14:41:03,053 |	  model_w_in_main test loss : 0.832306
2021-12-28 14:41:03,157 |	  model_v_in_main test loss : 0.829263
2021-12-28 14:41:03,163 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.0575, -4.0575, -4.0575,  ..., -4.0575, -4.0575, -4.0575],
       device='cuda:0', requires_grad=True))
2021-12-28 14:41:03,165 |	  Step count: 37
2021-12-28 14:41:03,171 |	  attentionweight:tensor([7.6890e-05, 7.6890e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:14,920 |	  attentionweight:tensor([7.6890e-05, 7.6890e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:15,004 |	  attentionweight:tensor([7.6890e-05, 7.6890e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:16,295 |	  attentionweight:tensor([7.6890e-05, 7.6890e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:16,379 |	  attentionweight:tensor([7.6890e-05, 7.6890e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:16,610 |	  attentionweight:tensor([0.0018, 0.0018], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:16,687 |	  loss_w (train):0.0010307105258107185
2021-12-28 14:41:18,895 |	  v_loss (train):129.95521545410156
2021-12-28 14:41:19,463 |	  model_w_in_main test loss : 0.836970
2021-12-28 14:41:19,580 |	  model_v_in_main test loss : 0.834625
2021-12-28 14:41:19,586 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1630, -4.1630, -4.1630,  ..., -4.1630, -4.1630, -4.1630],
       device='cuda:0', requires_grad=True))
2021-12-28 14:41:19,589 |	  Step count: 38
2021-12-28 14:41:19,592 |	  attentionweight:tensor([7.4593e-05, 7.4593e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:34,222 |	  attentionweight:tensor([7.4593e-05, 7.4593e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:34,295 |	  attentionweight:tensor([7.4593e-05, 7.4593e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:37,306 |	  attentionweight:tensor([7.4593e-05, 7.4593e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:37,372 |	  attentionweight:tensor([7.4593e-05, 7.4593e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:37,550 |	  attentionweight:tensor([3.4846e-06, 1.6580e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:37,664 |	  loss_w (train):0.002705095801502466
2021-12-28 14:41:40,562 |	  v_loss (train):104.17266845703125
2021-12-28 14:41:41,299 |	  model_w_in_main test loss : 0.836322
2021-12-28 14:41:41,348 |	  model_v_in_main test loss : 0.838124
2021-12-28 14:41:41,351 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2239, -4.2239, -4.2239,  ..., -4.2239, -4.2239, -4.2239],
       device='cuda:0', requires_grad=True))
2021-12-28 14:41:41,353 |	  Step count: 39
2021-12-28 14:41:41,355 |	  attentionweight:tensor([7.0932e-05, 7.0932e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:52,350 |	  attentionweight:tensor([7.0932e-05, 7.0932e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:52,536 |	  attentionweight:tensor([7.0932e-05, 7.0932e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:54,402 |	  attentionweight:tensor([7.0932e-05, 7.0932e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:54,551 |	  attentionweight:tensor([7.0932e-05, 7.0932e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:54,968 |	  attentionweight:tensor([0.0016, 0.0013], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:41:55,139 |	  loss_w (train):0.00042771827429533005
2021-12-28 14:41:56,513 |	  v_loss (train):75.38343811035156
2021-12-28 14:41:57,182 |	  model_w_in_main test loss : 0.837037
2021-12-28 14:41:57,278 |	  model_v_in_main test loss : 0.843443
2021-12-28 14:41:57,326 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2581, -4.2581, -4.2581,  ..., -4.2581, -4.2581, -4.2581],
       device='cuda:0', requires_grad=True))
2021-12-28 14:41:57,328 |	  Step count: 40
2021-12-28 14:41:57,408 |	  attentionweight:tensor([6.7073e-05, 6.7073e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:13,098 |	  attentionweight:tensor([6.7073e-05, 6.7073e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:13,183 |	  attentionweight:tensor([6.7073e-05, 6.7073e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:15,285 |	  attentionweight:tensor([6.7073e-05, 6.7073e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:15,582 |	  attentionweight:tensor([6.7073e-05, 6.7073e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:15,857 |	  attentionweight:tensor([2.0416e-04, 2.7455e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:16,127 |	  loss_w (train):1.0049446927951067e-06
2021-12-28 14:42:17,619 |	  v_loss (train):129.72409057617188
2021-12-28 14:42:18,189 |	  model_w_in_main test loss : 0.836955
2021-12-28 14:42:18,342 |	  model_v_in_main test loss : 0.849604
2021-12-28 14:42:18,349 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2732, -4.2732, -4.2732,  ..., -4.2732, -4.2732, -4.2732],
       device='cuda:0', requires_grad=True))
2021-12-28 14:42:18,352 |	  Step count: 41
2021-12-28 14:42:18,356 |	  attentionweight:tensor([6.3514e-05, 6.3514e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:32,571 |	  attentionweight:tensor([6.3514e-05, 6.3514e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:32,653 |	  attentionweight:tensor([6.3514e-05, 6.3514e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:36,281 |	  attentionweight:tensor([6.3514e-05, 6.3514e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:36,409 |	  attentionweight:tensor([6.3514e-05, 6.3514e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:36,585 |	  attentionweight:tensor([2.5201e-06, 2.4513e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:36,711 |	  loss_w (train):8.748905599986756e-08
2021-12-28 14:42:39,747 |	  v_loss (train):151.11709594726562
2021-12-28 14:42:40,390 |	  model_w_in_main test loss : 0.836870
2021-12-28 14:42:40,488 |	  model_v_in_main test loss : 0.849988
2021-12-28 14:42:40,494 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2751, -4.2751, -4.2751,  ..., -4.2751, -4.2751, -4.2751],
       device='cuda:0', requires_grad=True))
2021-12-28 14:42:40,496 |	  Step count: 42
2021-12-28 14:42:40,499 |	  attentionweight:tensor([6.0279e-05, 6.0279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:55,369 |	  attentionweight:tensor([6.0279e-05, 6.0279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:55,670 |	  attentionweight:tensor([6.0279e-05, 6.0279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:59,342 |	  attentionweight:tensor([6.0279e-05, 6.0279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:59,453 |	  attentionweight:tensor([6.0279e-05, 6.0279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:59,821 |	  attentionweight:tensor([1.6251e-06, 1.0534e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:42:59,892 |	  loss_w (train):7.857917807996273e-05
2021-12-28 14:43:02,866 |	  v_loss (train):153.651611328125
2021-12-28 14:43:03,465 |	  model_w_in_main test loss : 0.837240
2021-12-28 14:43:03,512 |	  model_v_in_main test loss : 0.849805
2021-12-28 14:43:03,516 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2252, -4.2252, -4.2252,  ..., -4.2252, -4.2252, -4.2252],
       device='cuda:0', requires_grad=True))
2021-12-28 14:43:03,518 |	  Step count: 43
2021-12-28 14:43:03,520 |	  attentionweight:tensor([4.3820e-05, 4.3820e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:18,248 |	  attentionweight:tensor([4.3820e-05, 4.3820e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:18,380 |	  attentionweight:tensor([4.3820e-05, 4.3820e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:22,083 |	  attentionweight:tensor([4.3820e-05, 4.3820e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:22,174 |	  attentionweight:tensor([4.3820e-05, 4.3820e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:22,593 |	  attentionweight:tensor([1.1518e-03, 1.6800e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:22,710 |	  loss_w (train):0.00030838086968287826
2021-12-28 14:43:26,071 |	  v_loss (train):110.04248809814453
2021-12-28 14:43:26,683 |	  model_w_in_main test loss : 0.834516
2021-12-28 14:43:26,731 |	  model_v_in_main test loss : 0.852024
2021-12-28 14:43:26,734 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2034, -4.2034, -4.2034,  ..., -4.2034, -4.2034, -4.2034],
       device='cuda:0', requires_grad=True))
2021-12-28 14:43:26,736 |	  Step count: 44
2021-12-28 14:43:26,738 |	  attentionweight:tensor([4.4273e-05, 4.4273e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:36,290 |	  attentionweight:tensor([4.4273e-05, 4.4273e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:36,382 |	  attentionweight:tensor([4.4273e-05, 4.4273e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:37,646 |	  attentionweight:tensor([4.4273e-05, 4.4273e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:37,740 |	  attentionweight:tensor([4.4273e-05, 4.4273e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:38,157 |	  attentionweight:tensor([0.0005, 0.0008], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:38,275 |	  loss_w (train):2.5623153305787127e-06
2021-12-28 14:43:39,049 |	  v_loss (train):70.13687133789062
2021-12-28 14:43:39,704 |	  model_w_in_main test loss : 0.834656
2021-12-28 14:43:39,763 |	  model_v_in_main test loss : 0.843612
2021-12-28 14:43:39,767 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1929, -4.1929, -4.1929,  ..., -4.1929, -4.1929, -4.1929],
       device='cuda:0', requires_grad=True))
2021-12-28 14:43:39,769 |	  Step count: 45
2021-12-28 14:43:39,771 |	  attentionweight:tensor([4.3008e-05, 4.3008e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:52,244 |	  attentionweight:tensor([4.3008e-05, 4.3008e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:52,428 |	  attentionweight:tensor([4.3008e-05, 4.3008e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:54,815 |	  attentionweight:tensor([4.3008e-05, 4.3008e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:54,918 |	  attentionweight:tensor([4.3008e-05, 4.3008e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:55,262 |	  attentionweight:tensor([1.0702e-06, 1.0730e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:43:55,390 |	  loss_w (train):4.719786090845446e-07
2021-12-28 14:43:57,410 |	  v_loss (train):98.325927734375
2021-12-28 14:43:58,036 |	  model_w_in_main test loss : 0.834494
2021-12-28 14:43:58,090 |	  model_v_in_main test loss : 0.844519
2021-12-28 14:43:58,093 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1805, -4.1805, -4.1805,  ..., -4.1805, -4.1805, -4.1805],
       device='cuda:0', requires_grad=True))
2021-12-28 14:43:58,095 |	  Step count: 46
2021-12-28 14:43:58,098 |	  attentionweight:tensor([3.0922e-05, 3.0922e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:15,353 |	  attentionweight:tensor([3.0922e-05, 3.0922e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:15,452 |	  attentionweight:tensor([3.0922e-05, 3.0922e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:19,666 |	  attentionweight:tensor([3.0922e-05, 3.0922e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:19,756 |	  attentionweight:tensor([3.0922e-05, 3.0922e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:19,969 |	  attentionweight:tensor([0.0013, 0.0013], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:20,188 |	  loss_w (train):0.00034575455356389284
2021-12-28 14:44:23,590 |	  v_loss (train):204.30795288085938
2021-12-28 14:44:24,173 |	  model_w_in_main test loss : 0.832057
2021-12-28 14:44:24,319 |	  model_v_in_main test loss : 0.850159
2021-12-28 14:44:24,324 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1804, -4.1804, -4.1804,  ..., -4.1804, -4.1804, -4.1804],
       device='cuda:0', requires_grad=True))
2021-12-28 14:44:24,326 |	  Step count: 47
2021-12-28 14:44:24,329 |	  attentionweight:tensor([4.3580e-05, 4.3580e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:35,919 |	  attentionweight:tensor([4.3580e-05, 4.3580e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:36,047 |	  attentionweight:tensor([4.3580e-05, 4.3580e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:37,937 |	  attentionweight:tensor([4.3580e-05, 4.3580e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:38,017 |	  attentionweight:tensor([4.3580e-05, 4.3580e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:38,415 |	  attentionweight:tensor([1.4254e-06, 1.8026e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:38,522 |	  loss_w (train):6.014967084411182e-07
2021-12-28 14:44:39,895 |	  v_loss (train):83.69935607910156
2021-12-28 14:44:40,482 |	  model_w_in_main test loss : 0.832088
2021-12-28 14:44:40,562 |	  model_v_in_main test loss : 0.848060
2021-12-28 14:44:40,568 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1765, -4.1765, -4.1765,  ..., -4.1765, -4.1765, -4.1765],
       device='cuda:0', requires_grad=True))
2021-12-28 14:44:40,570 |	  Step count: 48
2021-12-28 14:44:40,574 |	  attentionweight:tensor([4.3775e-05, 4.3775e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:57,676 |	  attentionweight:tensor([4.3775e-05, 4.3775e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:44:57,766 |	  attentionweight:tensor([4.3775e-05, 4.3775e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:02,104 |	  attentionweight:tensor([4.3775e-05, 4.3775e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:02,290 |	  attentionweight:tensor([4.3775e-05, 4.3775e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:02,676 |	  attentionweight:tensor([1.1069e-03, 3.3172e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:02,725 |	  loss_w (train):1.8564326182968216e-06
2021-12-28 14:45:06,936 |	  v_loss (train):141.13143920898438
2021-12-28 14:45:07,368 |	  model_w_in_main test loss : 0.832192
2021-12-28 14:45:07,654 |	  model_v_in_main test loss : 0.851945
2021-12-28 14:45:07,658 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1747, -4.1747, -4.1747,  ..., -4.1747, -4.1747, -4.1747],
       device='cuda:0', requires_grad=True))
2021-12-28 14:45:07,660 |	  Step count: 49
2021-12-28 14:45:07,663 |	  attentionweight:tensor([4.3050e-05, 4.3050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:24,312 |	  attentionweight:tensor([4.3050e-05, 4.3050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:24,593 |	  attentionweight:tensor([4.3050e-05, 4.3050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:29,409 |	  attentionweight:tensor([4.3050e-05, 4.3050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:29,541 |	  attentionweight:tensor([4.3050e-05, 4.3050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:29,863 |	  attentionweight:tensor([1.2378e-06, 1.5966e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:29,982 |	  loss_w (train):1.6222737031057477e-05
2021-12-28 14:45:33,868 |	  v_loss (train):121.75047302246094
2021-12-28 14:45:34,419 |	  model_w_in_main test loss : 0.831936
2021-12-28 14:45:34,674 |	  model_v_in_main test loss : 0.852215
2021-12-28 14:45:34,677 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1715, -4.1715, -4.1715,  ..., -4.1715, -4.1715, -4.1715],
       device='cuda:0', requires_grad=True))
2021-12-28 14:45:34,679 |	  Step count: 50
2021-12-28 14:45:34,682 |	  attentionweight:tensor([4.0539e-05, 4.0539e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:52,952 |	  attentionweight:tensor([4.0539e-05, 4.0539e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:53,245 |	  attentionweight:tensor([4.0539e-05, 4.0539e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:59,396 |	  attentionweight:tensor([4.0539e-05, 4.0539e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:59,458 |	  attentionweight:tensor([4.0539e-05, 4.0539e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:59,844 |	  attentionweight:tensor([0.0014, 0.0014], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:45:59,914 |	  loss_w (train):0.0008121962891891599
2021-12-28 14:46:05,195 |	  v_loss (train):178.835205078125
2021-12-28 14:46:05,810 |	  model_w_in_main test loss : 0.834618
2021-12-28 14:46:05,949 |	  model_v_in_main test loss : 0.850536
2021-12-28 14:46:05,955 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1737, -4.1737, -4.1737,  ..., -4.1737, -4.1737, -4.1737],
       device='cuda:0', requires_grad=True))
2021-12-28 14:46:05,957 |	  Step count: 51
2021-12-28 14:46:05,961 |	  attentionweight:tensor([4.1850e-05, 4.1850e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:18,495 |	  attentionweight:tensor([4.1850e-05, 4.1850e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:18,797 |	  attentionweight:tensor([4.1850e-05, 4.1850e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:21,189 |	  attentionweight:tensor([4.1850e-05, 4.1850e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:21,269 |	  attentionweight:tensor([4.1850e-05, 4.1850e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:21,490 |	  attentionweight:tensor([1.1355e-06, 1.1735e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:21,732 |	  loss_w (train):9.316458715602494e-09
2021-12-28 14:46:23,825 |	  v_loss (train):157.99134826660156
2021-12-28 14:46:24,390 |	  model_w_in_main test loss : 0.834580
2021-12-28 14:46:24,504 |	  model_v_in_main test loss : 0.849721
2021-12-28 14:46:24,508 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1715, -4.1715, -4.1715,  ..., -4.1715, -4.1715, -4.1715],
       device='cuda:0', requires_grad=True))
2021-12-28 14:46:24,509 |	  Step count: 52
2021-12-28 14:46:24,512 |	  attentionweight:tensor([3.9722e-05, 3.9722e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:38,319 |	  attentionweight:tensor([3.9722e-05, 3.9722e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:38,404 |	  attentionweight:tensor([3.9722e-05, 3.9722e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:41,684 |	  attentionweight:tensor([3.9722e-05, 3.9722e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:41,794 |	  attentionweight:tensor([3.9722e-05, 3.9722e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:42,206 |	  attentionweight:tensor([0.0015, 0.0004], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:42,300 |	  loss_w (train):8.491929293086287e-06
2021-12-28 14:46:44,969 |	  v_loss (train):91.20552062988281
2021-12-28 14:46:45,524 |	  model_w_in_main test loss : 0.834816
2021-12-28 14:46:45,636 |	  model_v_in_main test loss : 0.847124
2021-12-28 14:46:45,640 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1766, -4.1766, -4.1766,  ..., -4.1766, -4.1766, -4.1766],
       device='cuda:0', requires_grad=True))
2021-12-28 14:46:45,642 |	  Step count: 53
2021-12-28 14:46:45,644 |	  attentionweight:tensor([4.2137e-05, 4.2137e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:57,577 |	  attentionweight:tensor([4.2137e-05, 4.2137e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:57,656 |	  attentionweight:tensor([4.2137e-05, 4.2137e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:59,574 |	  attentionweight:tensor([4.2137e-05, 4.2137e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:46:59,668 |	  attentionweight:tensor([4.2137e-05, 4.2137e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:00,004 |	  attentionweight:tensor([1.4497e-06, 1.7068e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:00,116 |	  loss_w (train):0.00043109027319587767
2021-12-28 14:47:01,731 |	  v_loss (train):105.0806884765625
2021-12-28 14:47:02,332 |	  model_w_in_main test loss : 0.835287
2021-12-28 14:47:02,408 |	  model_v_in_main test loss : 0.851662
2021-12-28 14:47:02,411 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1854, -4.1854, -4.1854,  ..., -4.1854, -4.1854, -4.1854],
       device='cuda:0', requires_grad=True))
2021-12-28 14:47:02,413 |	  Step count: 54
2021-12-28 14:47:02,415 |	  attentionweight:tensor([4.5085e-05, 4.5085e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:16,768 |	  attentionweight:tensor([4.5085e-05, 4.5085e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:16,850 |	  attentionweight:tensor([4.5085e-05, 4.5085e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:19,510 |	  attentionweight:tensor([4.5085e-05, 4.5085e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:19,641 |	  attentionweight:tensor([4.5085e-05, 4.5085e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:20,037 |	  attentionweight:tensor([2.4268e-05, 1.1759e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:20,083 |	  loss_w (train):1.6794976431810937e-07
2021-12-28 14:47:22,581 |	  v_loss (train):70.12017059326172
2021-12-28 14:47:23,182 |	  model_w_in_main test loss : 0.835281
2021-12-28 14:47:23,230 |	  model_v_in_main test loss : 0.850063
2021-12-28 14:47:23,234 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.1881, -4.1881, -4.1881,  ..., -4.1881, -4.1881, -4.1881],
       device='cuda:0', requires_grad=True))
2021-12-28 14:47:23,235 |	  Step count: 55
2021-12-28 14:47:23,238 |	  attentionweight:tensor([4.4713e-05, 4.4713e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:39,071 |	  attentionweight:tensor([4.4713e-05, 4.4713e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:39,373 |	  attentionweight:tensor([4.4713e-05, 4.4713e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:43,279 |	  attentionweight:tensor([4.4713e-05, 4.4713e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:43,578 |	  attentionweight:tensor([4.4713e-05, 4.4713e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:43,778 |	  attentionweight:tensor([0.0019, 0.0018], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:47:43,851 |	  loss_w (train):0.0003462113090790808
2021-12-28 14:47:47,111 |	  v_loss (train):196.56430053710938
2021-12-28 14:47:47,699 |	  model_w_in_main test loss : 0.833232
2021-12-28 14:47:47,746 |	  model_v_in_main test loss : 0.843935
2021-12-28 14:47:47,749 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2023, -4.2023, -4.2023,  ..., -4.2023, -4.2023, -4.2023],
       device='cuda:0', requires_grad=True))
2021-12-28 14:47:47,751 |	  Step count: 56
2021-12-28 14:47:47,753 |	  attentionweight:tensor([4.7519e-05, 4.7519e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:02,238 |	  attentionweight:tensor([4.7519e-05, 4.7519e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:02,391 |	  attentionweight:tensor([4.7519e-05, 4.7519e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:06,766 |	  attentionweight:tensor([4.7519e-05, 4.7519e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:06,838 |	  attentionweight:tensor([4.7519e-05, 4.7519e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:07,021 |	  attentionweight:tensor([2.0274e-03, 1.1905e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:07,108 |	  loss_w (train):0.0007860790938138962
2021-12-28 14:48:10,130 |	  v_loss (train):146.4631805419922
2021-12-28 14:48:10,660 |	  model_w_in_main test loss : 0.829907
2021-12-28 14:48:10,746 |	  model_v_in_main test loss : 0.846536
2021-12-28 14:48:10,749 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2154, -4.2154, -4.2154,  ..., -4.2154, -4.2154, -4.2154],
       device='cuda:0', requires_grad=True))
2021-12-28 14:48:10,751 |	  Step count: 57
2021-12-28 14:48:10,754 |	  attentionweight:tensor([4.8399e-05, 4.8399e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:31,492 |	  attentionweight:tensor([4.8399e-05, 4.8399e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:31,614 |	  attentionweight:tensor([4.8399e-05, 4.8399e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:38,597 |	  attentionweight:tensor([4.8399e-05, 4.8399e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:38,688 |	  attentionweight:tensor([4.8399e-05, 4.8399e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:39,082 |	  attentionweight:tensor([0.0023, 0.0016], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:39,186 |	  loss_w (train):0.0003024313482455909
2021-12-28 14:48:44,901 |	  v_loss (train):225.3513641357422
2021-12-28 14:48:45,445 |	  model_w_in_main test loss : 0.831198
2021-12-28 14:48:45,544 |	  model_v_in_main test loss : 0.846627
2021-12-28 14:48:45,548 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.2531, -4.2531, -4.2531,  ..., -4.2531, -4.2531, -4.2531],
       device='cuda:0', requires_grad=True))
2021-12-28 14:48:45,550 |	  Step count: 58
2021-12-28 14:48:45,552 |	  attentionweight:tensor([5.2209e-05, 5.2209e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:57,634 |	  attentionweight:tensor([5.2209e-05, 5.2209e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:57,722 |	  attentionweight:tensor([5.2209e-05, 5.2209e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:48:59,772 |	  attentionweight:tensor([5.2209e-05, 5.2209e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:00,073 |	  attentionweight:tensor([5.2209e-05, 5.2209e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:00,247 |	  attentionweight:tensor([0.0026, 0.0026], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:00,396 |	  loss_w (train):0.002328430535271764
2021-12-28 14:49:02,299 |	  v_loss (train):77.66590118408203
2021-12-28 14:49:02,862 |	  model_w_in_main test loss : 0.830107
2021-12-28 14:49:03,138 |	  model_v_in_main test loss : 0.844598
2021-12-28 14:49:03,141 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.3286, -4.3286, -4.3286,  ..., -4.3286, -4.3286, -4.3286],
       device='cuda:0', requires_grad=True))
2021-12-28 14:49:03,143 |	  Step count: 59
2021-12-28 14:49:03,149 |	  attentionweight:tensor([5.5712e-05, 5.5712e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:15,181 |	  attentionweight:tensor([5.5712e-05, 5.5712e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:15,255 |	  attentionweight:tensor([5.5712e-05, 5.5712e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:17,829 |	  attentionweight:tensor([5.5712e-05, 5.5712e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:17,998 |	  attentionweight:tensor([5.5712e-05, 5.5712e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:18,409 |	  attentionweight:tensor([1.1968e-06, 1.1986e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:18,460 |	  loss_w (train):3.91625221141112e-09
2021-12-28 14:49:20,399 |	  v_loss (train):73.64849853515625
2021-12-28 14:49:21,022 |	  model_w_in_main test loss : 0.830128
2021-12-28 14:49:21,074 |	  model_v_in_main test loss : 0.850318
2021-12-28 14:49:21,077 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.3587, -4.3587, -4.3587,  ..., -4.3587, -4.3587, -4.3587],
       device='cuda:0', requires_grad=True))
2021-12-28 14:49:21,078 |	  Step count: 60
2021-12-28 14:49:21,081 |	  attentionweight:tensor([5.2636e-05, 5.2636e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:31,144 |	  attentionweight:tensor([5.2636e-05, 5.2636e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:31,245 |	  attentionweight:tensor([5.2636e-05, 5.2636e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:33,396 |	  attentionweight:tensor([5.2636e-05, 5.2636e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:33,485 |	  attentionweight:tensor([5.2636e-05, 5.2636e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:33,899 |	  attentionweight:tensor([1.1497e-06, 2.2507e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:33,951 |	  loss_w (train):4.492098742048256e-05
2021-12-28 14:49:35,385 |	  v_loss (train):72.35411834716797
2021-12-28 14:49:35,950 |	  model_w_in_main test loss : 0.832243
2021-12-28 14:49:36,197 |	  model_v_in_main test loss : 0.845994
2021-12-28 14:49:36,200 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.3752, -4.3752, -4.3752,  ..., -4.3752, -4.3752, -4.3752],
       device='cuda:0', requires_grad=True))
2021-12-28 14:49:36,202 |	  Step count: 61
2021-12-28 14:49:36,205 |	  attentionweight:tensor([4.8158e-05, 4.8158e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:55,339 |	  attentionweight:tensor([4.8158e-05, 4.8158e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:55,419 |	  attentionweight:tensor([4.8158e-05, 4.8158e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:59,897 |	  attentionweight:tensor([4.8158e-05, 4.8158e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:49:59,986 |	  attentionweight:tensor([4.8158e-05, 4.8158e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:00,221 |	  attentionweight:tensor([0.0024, 0.0024], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:00,528 |	  loss_w (train):9.52460050029913e-06
2021-12-28 14:50:05,019 |	  v_loss (train):207.07162475585938
2021-12-28 14:50:05,647 |	  model_w_in_main test loss : 0.832454
2021-12-28 14:50:05,700 |	  model_v_in_main test loss : 0.851326
2021-12-28 14:50:05,704 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4089, -4.4089, -4.4089,  ..., -4.4089, -4.4089, -4.4089],
       device='cuda:0', requires_grad=True))
2021-12-28 14:50:05,706 |	  Step count: 62
2021-12-28 14:50:05,708 |	  attentionweight:tensor([4.7352e-05, 4.7352e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:15,825 |	  attentionweight:tensor([4.7352e-05, 4.7352e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:16,078 |	  attentionweight:tensor([4.7352e-05, 4.7352e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:17,059 |	  attentionweight:tensor([4.7352e-05, 4.7352e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:17,391 |	  attentionweight:tensor([4.7352e-05, 4.7352e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:17,615 |	  attentionweight:tensor([9.1323e-07, 2.1468e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:17,735 |	  loss_w (train):5.032917670177994e-06
2021-12-28 14:50:18,902 |	  v_loss (train):60.4945068359375
2021-12-28 14:50:19,541 |	  model_w_in_main test loss : 0.832414
2021-12-28 14:50:19,654 |	  model_v_in_main test loss : 0.847907
2021-12-28 14:50:19,660 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4248, -4.4248, -4.4248,  ..., -4.4248, -4.4248, -4.4248],
       device='cuda:0', requires_grad=True))
2021-12-28 14:50:19,662 |	  Step count: 63
2021-12-28 14:50:19,668 |	  attentionweight:tensor([4.4279e-05, 4.4279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:32,392 |	  attentionweight:tensor([4.4279e-05, 4.4279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:32,481 |	  attentionweight:tensor([4.4279e-05, 4.4279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:35,055 |	  attentionweight:tensor([4.4279e-05, 4.4279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:35,231 |	  attentionweight:tensor([4.4279e-05, 4.4279e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:35,651 |	  attentionweight:tensor([7.4645e-07, 7.2698e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:35,717 |	  loss_w (train):1.397485078769023e-07
2021-12-28 14:50:37,797 |	  v_loss (train):111.51719665527344
2021-12-28 14:50:38,412 |	  model_w_in_main test loss : 0.832464
2021-12-28 14:50:38,538 |	  model_v_in_main test loss : 0.845098
2021-12-28 14:50:38,544 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4229, -4.4229, -4.4229,  ..., -4.4229, -4.4229, -4.4229],
       device='cuda:0', requires_grad=True))
2021-12-28 14:50:38,546 |	  Step count: 64
2021-12-28 14:50:38,552 |	  attentionweight:tensor([3.7342e-05, 3.7342e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:48,185 |	  attentionweight:tensor([3.7342e-05, 3.7342e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:48,277 |	  attentionweight:tensor([3.7342e-05, 3.7342e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:49,466 |	  attentionweight:tensor([3.7342e-05, 3.7342e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:49,560 |	  attentionweight:tensor([3.7342e-05, 3.7342e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:49,951 |	  attentionweight:tensor([4.0376e-04, 5.0198e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:50:50,061 |	  loss_w (train):1.7660705680100364e-06
2021-12-28 14:50:51,985 |	  v_loss (train):48.03339385986328
2021-12-28 14:50:52,598 |	  model_w_in_main test loss : 0.832461
2021-12-28 14:50:52,653 |	  model_v_in_main test loss : 0.843307
2021-12-28 14:50:52,657 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4186, -4.4186, -4.4186,  ..., -4.4186, -4.4186, -4.4186],
       device='cuda:0', requires_grad=True))
2021-12-28 14:50:52,659 |	  Step count: 65
2021-12-28 14:50:52,661 |	  attentionweight:tensor([2.6553e-05, 2.6553e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:08,991 |	  attentionweight:tensor([2.6553e-05, 2.6553e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:09,160 |	  attentionweight:tensor([2.6553e-05, 2.6553e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:13,949 |	  attentionweight:tensor([2.6553e-05, 2.6553e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:14,222 |	  attentionweight:tensor([2.6553e-05, 2.6553e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:14,370 |	  attentionweight:tensor([0.0020, 0.0020], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:14,523 |	  loss_w (train):0.0019269405165687203
2021-12-28 14:51:18,517 |	  v_loss (train):152.26637268066406
2021-12-28 14:51:19,138 |	  model_w_in_main test loss : 0.826877
2021-12-28 14:51:19,194 |	  model_v_in_main test loss : 0.844609
2021-12-28 14:51:19,200 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4227, -4.4227, -4.4227,  ..., -4.4227, -4.4227, -4.4227],
       device='cuda:0', requires_grad=True))
2021-12-28 14:51:19,203 |	  Step count: 66
2021-12-28 14:51:19,212 |	  attentionweight:tensor([3.5931e-05, 3.5931e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:34,347 |	  attentionweight:tensor([3.5931e-05, 3.5931e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:34,629 |	  attentionweight:tensor([3.5931e-05, 3.5931e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:38,332 |	  attentionweight:tensor([3.5931e-05, 3.5931e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:38,505 |	  attentionweight:tensor([3.5931e-05, 3.5931e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:38,926 |	  attentionweight:tensor([7.0819e-07, 6.9786e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:38,991 |	  loss_w (train):2.829087009104114e-07
2021-12-28 14:51:42,409 |	  v_loss (train):152.6293487548828
2021-12-28 14:51:42,841 |	  model_w_in_main test loss : 0.826888
2021-12-28 14:51:43,097 |	  model_v_in_main test loss : 0.847806
2021-12-28 14:51:43,100 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4236, -4.4236, -4.4236,  ..., -4.4236, -4.4236, -4.4236],
       device='cuda:0', requires_grad=True))
2021-12-28 14:51:43,101 |	  Step count: 67
2021-12-28 14:51:43,103 |	  attentionweight:tensor([3.5113e-05, 3.5113e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:57,441 |	  attentionweight:tensor([3.5113e-05, 3.5113e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:51:57,552 |	  attentionweight:tensor([3.5113e-05, 3.5113e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:00,561 |	  attentionweight:tensor([3.5113e-05, 3.5113e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:00,641 |	  attentionweight:tensor([3.5113e-05, 3.5113e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:01,031 |	  attentionweight:tensor([2.1258e-03, 6.5532e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:01,136 |	  loss_w (train):5.347761543816887e-05
2021-12-28 14:52:03,691 |	  v_loss (train):88.77687072753906
2021-12-28 14:52:04,288 |	  model_w_in_main test loss : 0.828892
2021-12-28 14:52:04,342 |	  model_v_in_main test loss : 0.850233
2021-12-28 14:52:04,350 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4334, -4.4334, -4.4334,  ..., -4.4334, -4.4334, -4.4334],
       device='cuda:0', requires_grad=True))
2021-12-28 14:52:04,351 |	  Step count: 68
2021-12-28 14:52:04,362 |	  attentionweight:tensor([3.6507e-05, 3.6507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:16,862 |	  attentionweight:tensor([3.6507e-05, 3.6507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:16,954 |	  attentionweight:tensor([3.6507e-05, 3.6507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:19,352 |	  attentionweight:tensor([3.6507e-05, 3.6507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:19,518 |	  attentionweight:tensor([3.6507e-05, 3.6507e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:19,922 |	  attentionweight:tensor([6.0419e-07, 1.2194e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:19,971 |	  loss_w (train):3.2503609759260144e-07
2021-12-28 14:52:21,906 |	  v_loss (train):69.00794219970703
2021-12-28 14:52:22,514 |	  model_w_in_main test loss : 0.828895
2021-12-28 14:52:22,646 |	  model_v_in_main test loss : 0.851722
2021-12-28 14:52:22,652 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4366, -4.4366, -4.4366,  ..., -4.4366, -4.4366, -4.4366],
       device='cuda:0', requires_grad=True))
2021-12-28 14:52:22,654 |	  Step count: 69
2021-12-28 14:52:22,657 |	  attentionweight:tensor([3.5120e-05, 3.5120e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:35,685 |	  attentionweight:tensor([3.5120e-05, 3.5120e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:35,861 |	  attentionweight:tensor([3.5120e-05, 3.5120e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:38,641 |	  attentionweight:tensor([3.5120e-05, 3.5120e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:38,716 |	  attentionweight:tensor([3.5120e-05, 3.5120e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:39,103 |	  attentionweight:tensor([1.5096e-06, 5.7625e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:52:39,208 |	  loss_w (train):1.5527479746424433e-08
2021-12-28 14:52:41,361 |	  v_loss (train):55.024330139160156
2021-12-28 14:52:41,940 |	  model_w_in_main test loss : 0.828939
2021-12-28 14:52:42,013 |	  model_v_in_main test loss : 0.850277
2021-12-28 14:52:42,016 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4372, -4.4372, -4.4372,  ..., -4.4372, -4.4372, -4.4372],
       device='cuda:0', requires_grad=True))
2021-12-28 14:52:42,018 |	  Step count: 70
2021-12-28 14:52:42,069 |	  attentionweight:tensor([3.2961e-05, 3.2961e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:01,304 |	  attentionweight:tensor([3.2961e-05, 3.2961e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:01,376 |	  attentionweight:tensor([3.2961e-05, 3.2961e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:07,484 |	  attentionweight:tensor([3.2961e-05, 3.2961e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:07,639 |	  attentionweight:tensor([3.2961e-05, 3.2961e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:08,035 |	  attentionweight:tensor([2.1132e-04, 5.4948e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:08,081 |	  loss_w (train):6.773859240638558e-06
2021-12-28 14:53:13,073 |	  v_loss (train):158.54429626464844
2021-12-28 14:53:13,581 |	  model_w_in_main test loss : 0.829078
2021-12-28 14:53:13,702 |	  model_v_in_main test loss : 0.853200
2021-12-28 14:53:13,706 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4370, -4.4370, -4.4370,  ..., -4.4370, -4.4370, -4.4370],
       device='cuda:0', requires_grad=True))
2021-12-28 14:53:13,707 |	  Step count: 71
2021-12-28 14:53:13,710 |	  attentionweight:tensor([3.1050e-05, 3.1050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:30,627 |	  attentionweight:tensor([3.1050e-05, 3.1050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:30,689 |	  attentionweight:tensor([3.1050e-05, 3.1050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:35,526 |	  attentionweight:tensor([3.1050e-05, 3.1050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:35,610 |	  attentionweight:tensor([3.1050e-05, 3.1050e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:36,017 |	  attentionweight:tensor([1.8904e-03, 5.4723e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:36,130 |	  loss_w (train):0.00033391459146514535
2021-12-28 14:53:40,123 |	  v_loss (train):120.582275390625
2021-12-28 14:53:40,691 |	  model_w_in_main test loss : 0.829327
2021-12-28 14:53:40,800 |	  model_v_in_main test loss : 0.861521
2021-12-28 14:53:40,804 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4374, -4.4374, -4.4374,  ..., -4.4374, -4.4374, -4.4374],
       device='cuda:0', requires_grad=True))
2021-12-28 14:53:40,806 |	  Step count: 72
2021-12-28 14:53:40,810 |	  attentionweight:tensor([3.0231e-05, 3.0231e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:56,967 |	  attentionweight:tensor([3.0231e-05, 3.0231e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:53:57,061 |	  attentionweight:tensor([3.0231e-05, 3.0231e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:01,097 |	  attentionweight:tensor([3.0231e-05, 3.0231e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:01,189 |	  attentionweight:tensor([3.0231e-05, 3.0231e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:01,615 |	  attentionweight:tensor([0.0019, 0.0022], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:01,669 |	  loss_w (train):0.00019119474745821208
2021-12-28 14:54:05,247 |	  v_loss (train):120.71842193603516
2021-12-28 14:54:05,852 |	  model_w_in_main test loss : 0.831424
2021-12-28 14:54:05,905 |	  model_v_in_main test loss : 0.855407
2021-12-28 14:54:05,910 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4433, -4.4433, -4.4433,  ..., -4.4433, -4.4433, -4.4433],
       device='cuda:0', requires_grad=True))
2021-12-28 14:54:05,912 |	  Step count: 73
2021-12-28 14:54:05,914 |	  attentionweight:tensor([3.2967e-05, 3.2967e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:19,112 |	  attentionweight:tensor([3.2967e-05, 3.2967e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:19,394 |	  attentionweight:tensor([3.2967e-05, 3.2967e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:22,105 |	  attentionweight:tensor([3.2967e-05, 3.2967e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:22,381 |	  attentionweight:tensor([3.2967e-05, 3.2967e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:22,561 |	  attentionweight:tensor([4.6403e-07, 6.1173e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:22,661 |	  loss_w (train):3.714503691298887e-07
2021-12-28 14:54:25,006 |	  v_loss (train):102.5428466796875
2021-12-28 14:54:25,579 |	  model_w_in_main test loss : 0.831450
2021-12-28 14:54:25,842 |	  model_v_in_main test loss : 0.856761
2021-12-28 14:54:25,846 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4441, -4.4441, -4.4441,  ..., -4.4441, -4.4441, -4.4441],
       device='cuda:0', requires_grad=True))
2021-12-28 14:54:25,848 |	  Step count: 74
2021-12-28 14:54:25,853 |	  attentionweight:tensor([3.1419e-05, 3.1419e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:36,496 |	  attentionweight:tensor([3.1419e-05, 3.1419e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:36,571 |	  attentionweight:tensor([3.1419e-05, 3.1419e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:38,436 |	  attentionweight:tensor([3.1419e-05, 3.1419e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:38,714 |	  attentionweight:tensor([3.1419e-05, 3.1419e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:38,903 |	  attentionweight:tensor([6.3176e-06, 2.1478e-03], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:39,020 |	  loss_w (train):0.0009230119758285582
2021-12-28 14:54:40,569 |	  v_loss (train):46.78327941894531
2021-12-28 14:54:41,185 |	  model_w_in_main test loss : 0.832742
2021-12-28 14:54:41,238 |	  model_v_in_main test loss : 0.852597
2021-12-28 14:54:41,242 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4470, -4.4470, -4.4470,  ..., -4.4470, -4.4470, -4.4470],
       device='cuda:0', requires_grad=True))
2021-12-28 14:54:41,244 |	  Step count: 75
2021-12-28 14:54:41,247 |	  attentionweight:tensor([3.0688e-05, 3.0688e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:52,350 |	  attentionweight:tensor([3.0688e-05, 3.0688e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:52,440 |	  attentionweight:tensor([3.0688e-05, 3.0688e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:54,262 |	  attentionweight:tensor([3.0688e-05, 3.0688e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:54,353 |	  attentionweight:tensor([3.0688e-05, 3.0688e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:54,725 |	  attentionweight:tensor([0.0014, 0.0021], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:54:54,845 |	  loss_w (train):0.0004180778923910111
2021-12-28 14:54:56,188 |	  v_loss (train):48.91557693481445
2021-12-28 14:54:56,789 |	  model_w_in_main test loss : 0.834803
2021-12-28 14:54:56,851 |	  model_v_in_main test loss : 0.861988
2021-12-28 14:54:56,857 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4498, -4.4498, -4.4498,  ..., -4.4498, -4.4498, -4.4498],
       device='cuda:0', requires_grad=True))
2021-12-28 14:54:56,859 |	  Step count: 76
2021-12-28 14:54:56,861 |	  attentionweight:tensor([3.0248e-05, 3.0248e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:09,216 |	  attentionweight:tensor([3.0248e-05, 3.0248e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:09,349 |	  attentionweight:tensor([3.0248e-05, 3.0248e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:11,801 |	  attentionweight:tensor([3.0248e-05, 3.0248e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:12,090 |	  attentionweight:tensor([3.0248e-05, 3.0248e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:12,252 |	  attentionweight:tensor([1.5043e-03, 5.2208e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:12,402 |	  loss_w (train):0.0010737550910562277
2021-12-28 14:55:14,532 |	  v_loss (train):69.23898315429688
2021-12-28 14:55:15,134 |	  model_w_in_main test loss : 0.844931
2021-12-28 14:55:15,182 |	  model_v_in_main test loss : 0.864625
2021-12-28 14:55:15,186 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4512, -4.4512, -4.4512,  ..., -4.4512, -4.4512, -4.4512],
       device='cuda:0', requires_grad=True))
2021-12-28 14:55:15,187 |	  Step count: 77
2021-12-28 14:55:15,190 |	  attentionweight:tensor([2.8383e-05, 2.8383e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:36,083 |	  attentionweight:tensor([2.8383e-05, 2.8383e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:36,175 |	  attentionweight:tensor([2.8383e-05, 2.8383e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:42,184 |	  attentionweight:tensor([2.8383e-05, 2.8383e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:42,370 |	  attentionweight:tensor([2.8383e-05, 2.8383e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:42,780 |	  attentionweight:tensor([2.4109e-07, 2.5288e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:55:42,848 |	  loss_w (train):2.751438614723156e-07
2021-12-28 14:55:48,039 |	  v_loss (train):163.68186950683594
2021-12-28 14:55:48,642 |	  model_w_in_main test loss : 0.844765
2021-12-28 14:55:48,693 |	  model_v_in_main test loss : 0.863433
2021-12-28 14:55:48,696 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4449, -4.4449, -4.4449,  ..., -4.4449, -4.4449, -4.4449],
       device='cuda:0', requires_grad=True))
2021-12-28 14:55:48,698 |	  Step count: 78
2021-12-28 14:55:48,700 |	  attentionweight:tensor([1.8449e-05, 1.8449e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:04,419 |	  attentionweight:tensor([1.8449e-05, 1.8449e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:04,533 |	  attentionweight:tensor([1.8449e-05, 1.8449e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:08,965 |	  attentionweight:tensor([1.8449e-05, 1.8449e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:09,055 |	  attentionweight:tensor([1.8449e-05, 1.8449e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:09,370 |	  attentionweight:tensor([9.8648e-08, 5.7542e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:09,541 |	  loss_w (train):0.00023904289992060512
2021-12-28 14:56:12,872 |	  v_loss (train):114.34908294677734
2021-12-28 14:56:13,448 |	  model_w_in_main test loss : 0.842922
2021-12-28 14:56:13,517 |	  model_v_in_main test loss : 0.859639
2021-12-28 14:56:13,554 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4407, -4.4407, -4.4407,  ..., -4.4407, -4.4407, -4.4407],
       device='cuda:0', requires_grad=True))
2021-12-28 14:56:13,556 |	  Step count: 79
2021-12-28 14:56:13,585 |	  attentionweight:tensor([7.6978e-06, 7.6978e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:26,277 |	  attentionweight:tensor([7.6978e-06, 7.6978e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:26,383 |	  attentionweight:tensor([7.6978e-06, 7.6978e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:29,159 |	  attentionweight:tensor([7.6978e-06, 7.6978e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:29,240 |	  attentionweight:tensor([7.6978e-06, 7.6978e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:29,436 |	  attentionweight:tensor([7.7257e-05, 4.2860e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:29,702 |	  loss_w (train):2.649618977557111e-07
2021-12-28 14:56:31,656 |	  v_loss (train):62.22911071777344
2021-12-28 14:56:32,241 |	  model_w_in_main test loss : 0.842933
2021-12-28 14:56:32,287 |	  model_v_in_main test loss : 0.859933
2021-12-28 14:56:32,291 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4386, -4.4386, -4.4386,  ..., -4.4386, -4.4386, -4.4386],
       device='cuda:0', requires_grad=True))
2021-12-28 14:56:32,293 |	  Step count: 80
2021-12-28 14:56:32,295 |	  attentionweight:tensor([4.0829e-06, 4.0829e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:46,207 |	  attentionweight:tensor([4.0829e-06, 4.0829e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:46,383 |	  attentionweight:tensor([4.0829e-06, 4.0829e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:49,157 |	  attentionweight:tensor([4.0829e-06, 4.0829e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:49,241 |	  attentionweight:tensor([4.0829e-06, 4.0829e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:49,620 |	  attentionweight:tensor([7.2153e-08, 3.3610e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:56:49,727 |	  loss_w (train):1.4453321455221158e-05
2021-12-28 14:56:51,861 |	  v_loss (train):41.516056060791016
2021-12-28 14:56:52,439 |	  model_w_in_main test loss : 0.842924
2021-12-28 14:56:52,659 |	  model_v_in_main test loss : 0.866466
2021-12-28 14:56:52,664 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4376, -4.4376, -4.4376,  ..., -4.4376, -4.4376, -4.4376],
       device='cuda:0', requires_grad=True))
2021-12-28 14:56:52,666 |	  Step count: 81
2021-12-28 14:56:52,697 |	  attentionweight:tensor([4.5427e-06, 4.5427e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:08,296 |	  attentionweight:tensor([4.5427e-06, 4.5427e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:08,375 |	  attentionweight:tensor([4.5427e-06, 4.5427e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:13,415 |	  attentionweight:tensor([4.5427e-06, 4.5427e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:13,698 |	  attentionweight:tensor([4.5427e-06, 4.5427e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:13,874 |	  attentionweight:tensor([2.7724e-05, 1.1653e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:14,006 |	  loss_w (train):2.587244125606958e-06
2021-12-28 14:57:17,971 |	  v_loss (train):146.04608154296875
2021-12-28 14:57:18,586 |	  model_w_in_main test loss : 0.843084
2021-12-28 14:57:18,641 |	  model_v_in_main test loss : 0.872919
2021-12-28 14:57:18,645 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4371, -4.4371, -4.4371,  ..., -4.4371, -4.4371, -4.4371],
       device='cuda:0', requires_grad=True))
2021-12-28 14:57:18,647 |	  Step count: 82
2021-12-28 14:57:18,649 |	  attentionweight:tensor([3.9987e-06, 3.9987e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:29,511 |	  attentionweight:tensor([3.9987e-06, 3.9987e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:29,655 |	  attentionweight:tensor([3.9987e-06, 3.9987e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:31,680 |	  attentionweight:tensor([3.9987e-06, 3.9987e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:31,771 |	  attentionweight:tensor([3.9987e-06, 3.9987e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:32,149 |	  attentionweight:tensor([4.5154e-06, 2.7044e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:32,254 |	  loss_w (train):9.262908133678138e-05
2021-12-28 14:57:33,756 |	  v_loss (train):32.01194763183594
2021-12-28 14:57:34,334 |	  model_w_in_main test loss : 0.843331
2021-12-28 14:57:34,411 |	  model_v_in_main test loss : 0.879956
2021-12-28 14:57:34,414 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4369, -4.4369, -4.4369,  ..., -4.4369, -4.4369, -4.4369],
       device='cuda:0', requires_grad=True))
2021-12-28 14:57:34,416 |	  Step count: 83
2021-12-28 14:57:34,474 |	  attentionweight:tensor([5.1494e-06, 5.1494e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:44,192 |	  attentionweight:tensor([5.1494e-06, 5.1494e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:44,515 |	  attentionweight:tensor([5.1494e-06, 5.1494e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:45,844 |	  attentionweight:tensor([5.1494e-06, 5.1494e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:46,023 |	  attentionweight:tensor([5.1494e-06, 5.1494e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:46,390 |	  attentionweight:tensor([1.3543e-05, 5.2293e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:57:46,491 |	  loss_w (train):4.6695907940375037e-07
2021-12-28 14:57:47,946 |	  v_loss (train):23.057947158813477
2021-12-28 14:57:48,562 |	  model_w_in_main test loss : 0.843189
2021-12-28 14:57:48,620 |	  model_v_in_main test loss : 0.879811
2021-12-28 14:57:48,626 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4368, -4.4368, -4.4368,  ..., -4.4368, -4.4368, -4.4368],
       device='cuda:0', requires_grad=True))
2021-12-28 14:57:48,630 |	  Step count: 84
2021-12-28 14:57:48,634 |	  attentionweight:tensor([5.7107e-06, 5.7107e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:03,498 |	  attentionweight:tensor([5.7107e-06, 5.7107e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:03,588 |	  attentionweight:tensor([5.7107e-06, 5.7107e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:06,645 |	  attentionweight:tensor([5.7107e-06, 5.7107e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:06,817 |	  attentionweight:tensor([5.7107e-06, 5.7107e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:07,218 |	  attentionweight:tensor([7.0669e-06, 8.5470e-05], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:07,266 |	  loss_w (train):1.7948897266251151e-06
2021-12-28 14:58:10,099 |	  v_loss (train):117.79676055908203
2021-12-28 14:58:10,697 |	  model_w_in_main test loss : 0.843247
2021-12-28 14:58:10,755 |	  model_v_in_main test loss : 0.880456
2021-12-28 14:58:10,759 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4367, -4.4367, -4.4367,  ..., -4.4367, -4.4367, -4.4367],
       device='cuda:0', requires_grad=True))
2021-12-28 14:58:10,761 |	  Step count: 85
2021-12-28 14:58:10,764 |	  attentionweight:tensor([6.2482e-06, 6.2482e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:29,141 |	  attentionweight:tensor([6.2482e-06, 6.2482e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:29,216 |	  attentionweight:tensor([6.2482e-06, 6.2482e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:34,601 |	  attentionweight:tensor([6.2482e-06, 6.2482e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:34,753 |	  attentionweight:tensor([6.2482e-06, 6.2482e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:35,187 |	  attentionweight:tensor([6.0121e-08, 7.6444e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:35,236 |	  loss_w (train):2.1241103098645908e-08
2021-12-28 14:58:40,036 |	  v_loss (train):112.6534423828125
2021-12-28 14:58:40,659 |	  model_w_in_main test loss : 0.843151
2021-12-28 14:58:40,762 |	  model_v_in_main test loss : 0.884142
2021-12-28 14:58:40,766 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4366, -4.4366, -4.4366,  ..., -4.4366, -4.4366, -4.4366],
       device='cuda:0', requires_grad=True))
2021-12-28 14:58:40,768 |	  Step count: 86
2021-12-28 14:58:40,773 |	  attentionweight:tensor([4.9677e-06, 4.9677e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:53,212 |	  attentionweight:tensor([4.9677e-06, 4.9677e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:53,499 |	  attentionweight:tensor([4.9677e-06, 4.9677e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:55,964 |	  attentionweight:tensor([4.9677e-06, 4.9677e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:56,150 |	  attentionweight:tensor([4.9677e-06, 4.9677e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:56,532 |	  attentionweight:tensor([1.1786e-07, 1.5742e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:58:56,581 |	  loss_w (train):2.9118285560514323e-09
2021-12-28 14:58:58,521 |	  v_loss (train):34.77616500854492
2021-12-28 14:58:59,068 |	  model_w_in_main test loss : 0.843222
2021-12-28 14:58:59,281 |	  model_v_in_main test loss : 0.883454
2021-12-28 14:58:59,320 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4365, -4.4365, -4.4365,  ..., -4.4365, -4.4365, -4.4365],
       device='cuda:0', requires_grad=True))
2021-12-28 14:58:59,323 |	  Step count: 87
2021-12-28 14:58:59,334 |	  attentionweight:tensor([3.5336e-06, 3.5336e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:13,344 |	  attentionweight:tensor([3.5336e-06, 3.5336e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:13,439 |	  attentionweight:tensor([3.5336e-06, 3.5336e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:16,998 |	  attentionweight:tensor([3.5336e-06, 3.5336e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:17,078 |	  attentionweight:tensor([3.5336e-06, 3.5336e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:17,465 |	  attentionweight:tensor([3.6793e-06, 5.2063e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:17,566 |	  loss_w (train):9.943926215782994e-07
2021-12-28 14:59:20,330 |	  v_loss (train):65.73844909667969
2021-12-28 14:59:20,925 |	  model_w_in_main test loss : 0.843156
2021-12-28 14:59:21,005 |	  model_v_in_main test loss : 0.880372
2021-12-28 14:59:21,044 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4364, -4.4364, -4.4364,  ..., -4.4364, -4.4364, -4.4364],
       device='cuda:0', requires_grad=True))
2021-12-28 14:59:21,046 |	  Step count: 88
2021-12-28 14:59:21,063 |	  attentionweight:tensor([2.7345e-06, 2.7345e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:34,075 |	  attentionweight:tensor([2.7345e-06, 2.7345e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:34,156 |	  attentionweight:tensor([2.7345e-06, 2.7345e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:37,120 |	  attentionweight:tensor([2.7345e-06, 2.7345e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:37,253 |	  attentionweight:tensor([2.7345e-06, 2.7345e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:37,604 |	  attentionweight:tensor([3.9597e-08, 1.2226e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:37,665 |	  loss_w (train):4.515383693615149e-07
2021-12-28 14:59:40,177 |	  v_loss (train):62.2418098449707
2021-12-28 14:59:40,788 |	  model_w_in_main test loss : 0.843195
2021-12-28 14:59:40,939 |	  model_v_in_main test loss : 0.876338
2021-12-28 14:59:40,945 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4364, -4.4364, -4.4364,  ..., -4.4364, -4.4364, -4.4364],
       device='cuda:0', requires_grad=True))
2021-12-28 14:59:40,947 |	  Step count: 89
2021-12-28 14:59:40,951 |	  attentionweight:tensor([1.6615e-06, 1.6615e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:50,917 |	  attentionweight:tensor([1.6615e-06, 1.6615e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:51,056 |	  attentionweight:tensor([1.6615e-06, 1.6615e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:52,277 |	  attentionweight:tensor([1.6615e-06, 1.6615e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:52,420 |	  attentionweight:tensor([1.6615e-06, 1.6615e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:52,821 |	  attentionweight:tensor([6.5807e-08, 5.4916e-09], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 14:59:52,871 |	  loss_w (train):1.2178967834231003e-09
2021-12-28 14:59:53,918 |	  v_loss (train):22.49173927307129
2021-12-28 14:59:54,312 |	  model_w_in_main test loss : 0.843245
2021-12-28 14:59:54,580 |	  model_v_in_main test loss : 0.878916
2021-12-28 14:59:54,584 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 14:59:54,586 |	  Step count: 90
2021-12-28 14:59:54,588 |	  attentionweight:tensor([3.7941e-07, 3.7941e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:04,375 |	  attentionweight:tensor([3.7941e-07, 3.7941e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:04,457 |	  attentionweight:tensor([3.7941e-07, 3.7941e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:06,917 |	  attentionweight:tensor([3.7941e-07, 3.7941e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:07,089 |	  attentionweight:tensor([3.7941e-07, 3.7941e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:07,514 |	  attentionweight:tensor([4.3518e-08, 7.1258e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:07,563 |	  loss_w (train):2.644672747464938e-08
2021-12-28 15:00:09,580 |	  v_loss (train):28.74805450439453
2021-12-28 15:00:10,160 |	  model_w_in_main test loss : 0.843244
2021-12-28 15:00:10,408 |	  model_v_in_main test loss : 0.872523
2021-12-28 15:00:10,412 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:00:10,415 |	  Step count: 91
2021-12-28 15:00:10,417 |	  attentionweight:tensor([1.1248e-07, 1.1248e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:21,799 |	  attentionweight:tensor([1.1248e-07, 1.1248e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:21,976 |	  attentionweight:tensor([1.1248e-07, 1.1248e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:23,003 |	  attentionweight:tensor([1.1248e-07, 1.1248e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:23,126 |	  attentionweight:tensor([1.1248e-07, 1.1248e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:23,455 |	  attentionweight:tensor([6.1669e-08, 5.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:23,557 |	  loss_w (train):4.513175388964896e-10
2021-12-28 15:00:25,489 |	  v_loss (train):26.52947998046875
2021-12-28 15:00:26,116 |	  model_w_in_main test loss : 0.843212
2021-12-28 15:00:26,174 |	  model_v_in_main test loss : 0.873060
2021-12-28 15:00:26,178 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:00:26,180 |	  Step count: 92
2021-12-28 15:00:26,183 |	  attentionweight:tensor([5.8789e-08, 5.8789e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:41,647 |	  attentionweight:tensor([5.8789e-08, 5.8789e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:41,833 |	  attentionweight:tensor([5.8789e-08, 5.8789e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:45,808 |	  attentionweight:tensor([5.8789e-08, 5.8789e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:45,893 |	  attentionweight:tensor([5.8789e-08, 5.8789e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:46,285 |	  attentionweight:tensor([4.2530e-08, 4.2119e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:00:46,391 |	  loss_w (train):2.708223367786644e-10
2021-12-28 15:00:49,749 |	  v_loss (train):127.67243957519531
2021-12-28 15:00:50,310 |	  model_w_in_main test loss : 0.843195
2021-12-28 15:00:50,566 |	  model_v_in_main test loss : 0.873079
2021-12-28 15:00:50,573 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:00:50,575 |	  Step count: 93
2021-12-28 15:00:50,577 |	  attentionweight:tensor([4.2192e-08, 4.2192e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:00,499 |	  attentionweight:tensor([4.2192e-08, 4.2192e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:00,571 |	  attentionweight:tensor([4.2192e-08, 4.2192e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:01,767 |	  attentionweight:tensor([4.2192e-08, 4.2192e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:01,925 |	  attentionweight:tensor([4.2192e-08, 4.2192e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:02,322 |	  attentionweight:tensor([1.8720e-08, 2.8199e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:02,366 |	  loss_w (train):2.8914035610227984e-09
2021-12-28 15:01:03,159 |	  v_loss (train):21.795068740844727
2021-12-28 15:01:03,712 |	  model_w_in_main test loss : 0.843163
2021-12-28 15:01:03,784 |	  model_v_in_main test loss : 0.877912
2021-12-28 15:01:03,788 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:01:03,789 |	  Step count: 94
2021-12-28 15:01:03,831 |	  attentionweight:tensor([3.0056e-08, 3.0056e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:13,886 |	  attentionweight:tensor([3.0056e-08, 3.0056e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:13,976 |	  attentionweight:tensor([3.0056e-08, 3.0056e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:15,248 |	  attentionweight:tensor([3.0056e-08, 3.0056e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:15,341 |	  attentionweight:tensor([3.0056e-08, 3.0056e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:15,547 |	  attentionweight:tensor([2.6410e-08, 2.9180e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:15,830 |	  loss_w (train):3.613395360702043e-09
2021-12-28 15:01:16,671 |	  v_loss (train):30.608238220214844
2021-12-28 15:01:17,320 |	  model_w_in_main test loss : 0.843220
2021-12-28 15:01:17,378 |	  model_v_in_main test loss : 0.884575
2021-12-28 15:01:17,382 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:01:17,384 |	  Step count: 95
2021-12-28 15:01:17,386 |	  attentionweight:tensor([2.6243e-08, 2.6243e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:27,243 |	  attentionweight:tensor([2.6243e-08, 2.6243e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:27,393 |	  attentionweight:tensor([2.6243e-08, 2.6243e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:28,469 |	  attentionweight:tensor([2.6243e-08, 2.6243e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:28,637 |	  attentionweight:tensor([2.6243e-08, 2.6243e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:29,042 |	  attentionweight:tensor([2.4941e-08, 2.4714e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:29,091 |	  loss_w (train):2.555983202867651e-09
2021-12-28 15:01:29,946 |	  v_loss (train):17.416011810302734
2021-12-28 15:01:30,528 |	  model_w_in_main test loss : 0.843153
2021-12-28 15:01:30,659 |	  model_v_in_main test loss : 0.888068
2021-12-28 15:01:30,665 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:01:30,667 |	  Step count: 96
2021-12-28 15:01:30,671 |	  attentionweight:tensor([2.4634e-08, 2.4634e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:50,541 |	  attentionweight:tensor([2.4634e-08, 2.4634e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:50,621 |	  attentionweight:tensor([2.4634e-08, 2.4634e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:57,042 |	  attentionweight:tensor([2.4634e-08, 2.4634e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:57,220 |	  attentionweight:tensor([2.4634e-08, 2.4634e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:57,632 |	  attentionweight:tensor([2.4246e-08, 2.4137e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:01:57,680 |	  loss_w (train):1.134472099595385e-10
2021-12-28 15:02:03,118 |	  v_loss (train):125.52816772460938
2021-12-28 15:02:03,713 |	  model_w_in_main test loss : 0.843232
2021-12-28 15:02:03,788 |	  model_v_in_main test loss : 0.887542
2021-12-28 15:02:03,792 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:02:03,794 |	  Step count: 97
2021-12-28 15:02:03,816 |	  attentionweight:tensor([2.3993e-08, 2.3993e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:13,637 |	  attentionweight:tensor([2.3993e-08, 2.3993e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:13,706 |	  attentionweight:tensor([2.3993e-08, 2.3993e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:14,867 |	  attentionweight:tensor([2.3993e-08, 2.3993e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:14,995 |	  attentionweight:tensor([2.3993e-08, 2.3993e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:15,427 |	  attentionweight:tensor([2.2809e-08, 2.4314e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:15,475 |	  loss_w (train):1.1503004770929692e-08
2021-12-28 15:02:16,486 |	  v_loss (train):33.09707260131836
2021-12-28 15:02:17,097 |	  model_w_in_main test loss : 0.843237
2021-12-28 15:02:17,233 |	  model_v_in_main test loss : 0.893805
2021-12-28 15:02:17,238 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:02:17,240 |	  Step count: 98
2021-12-28 15:02:17,243 |	  attentionweight:tensor([2.3626e-08, 2.3626e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:35,197 |	  attentionweight:tensor([2.3626e-08, 2.3626e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:35,352 |	  attentionweight:tensor([2.3626e-08, 2.3626e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:40,906 |	  attentionweight:tensor([2.3626e-08, 2.3626e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:40,984 |	  attentionweight:tensor([2.3626e-08, 2.3626e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:41,380 |	  attentionweight:tensor([2.3695e-08, 2.1941e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:02:41,496 |	  loss_w (train):9.06785402321475e-09
2021-12-28 15:02:46,070 |	  v_loss (train):93.38248443603516
2021-12-28 15:02:46,714 |	  model_w_in_main test loss : 0.843230
2021-12-28 15:02:46,769 |	  model_v_in_main test loss : 0.887956
2021-12-28 15:02:46,773 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:02:46,774 |	  Step count: 99
2021-12-28 15:02:46,776 |	  attentionweight:tensor([2.3184e-08, 2.3184e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:08,026 |	  attentionweight:tensor([2.3184e-08, 2.3184e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:08,334 |	  attentionweight:tensor([2.3184e-08, 2.3184e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:15,026 |	  attentionweight:tensor([2.3184e-08, 2.3184e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:15,315 |	  attentionweight:tensor([2.3184e-08, 2.3184e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:15,534 |	  attentionweight:tensor([2.3826e-08, 2.6468e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:15,614 |	  loss_w (train):3.2302585606380774e-10
2021-12-28 15:03:21,735 |	  v_loss (train):191.373046875
2021-12-28 15:03:22,301 |	  model_w_in_main test loss : 0.843234
2021-12-28 15:03:22,367 |	  model_v_in_main test loss : 0.896007
2021-12-28 15:03:22,386 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:03:22,388 |	  Step count: 100
2021-12-28 15:03:22,413 |	  attentionweight:tensor([2.3803e-08, 2.3803e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:41,570 |	  attentionweight:tensor([2.3803e-08, 2.3803e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:41,755 |	  attentionweight:tensor([2.3803e-08, 2.3803e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:47,424 |	  attentionweight:tensor([2.3803e-08, 2.3803e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:47,608 |	  attentionweight:tensor([2.3803e-08, 2.3803e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:47,996 |	  attentionweight:tensor([2.4681e-08, 2.3495e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:03:48,049 |	  loss_w (train):9.80316050558372e-10
2021-12-28 15:03:53,383 |	  v_loss (train):158.93638610839844
2021-12-28 15:03:53,990 |	  model_w_in_main test loss : 0.843197
2021-12-28 15:03:54,043 |	  model_v_in_main test loss : 0.894551
2021-12-28 15:03:54,047 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:03:54,049 |	  Step count: 101
2021-12-28 15:03:54,060 |	  attentionweight:tensor([2.4104e-08, 2.4104e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:05,305 |	  attentionweight:tensor([2.4104e-08, 2.4104e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:05,397 |	  attentionweight:tensor([2.4104e-08, 2.4104e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:07,153 |	  attentionweight:tensor([2.4104e-08, 2.4104e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:07,245 |	  attentionweight:tensor([2.4104e-08, 2.4104e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:07,628 |	  attentionweight:tensor([2.4273e-08, 2.4251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:07,730 |	  loss_w (train):1.2429274276470892e-10
2021-12-28 15:04:09,093 |	  v_loss (train):34.066646575927734
2021-12-28 15:04:09,721 |	  model_w_in_main test loss : 0.843182
2021-12-28 15:04:09,779 |	  model_v_in_main test loss : 0.899807
2021-12-28 15:04:09,788 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:04:09,790 |	  Step count: 102
2021-12-28 15:04:09,800 |	  attentionweight:tensor([2.4259e-08, 2.4259e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:26,020 |	  attentionweight:tensor([2.4259e-08, 2.4259e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:26,322 |	  attentionweight:tensor([2.4259e-08, 2.4259e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:31,082 |	  attentionweight:tensor([2.4259e-08, 2.4259e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:31,154 |	  attentionweight:tensor([2.4259e-08, 2.4259e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:31,320 |	  attentionweight:tensor([2.3720e-08, 2.2405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:31,421 |	  loss_w (train):1.8917289956732475e-09
2021-12-28 15:04:35,595 |	  v_loss (train):160.55325317382812
2021-12-28 15:04:36,214 |	  model_w_in_main test loss : 0.843209
2021-12-28 15:04:36,268 |	  model_v_in_main test loss : 0.900722
2021-12-28 15:04:36,277 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:04:36,279 |	  Step count: 103
2021-12-28 15:04:36,287 |	  attentionweight:tensor([2.3815e-08, 2.3815e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:50,586 |	  attentionweight:tensor([2.3815e-08, 2.3815e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:50,668 |	  attentionweight:tensor([2.3815e-08, 2.3815e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:53,742 |	  attentionweight:tensor([2.3815e-08, 2.3815e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:53,819 |	  attentionweight:tensor([2.3815e-08, 2.3815e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:54,012 |	  attentionweight:tensor([2.2989e-08, 2.1916e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:04:54,107 |	  loss_w (train):1.106874591982887e-09
2021-12-28 15:04:56,703 |	  v_loss (train):72.56710815429688
2021-12-28 15:04:57,238 |	  model_w_in_main test loss : 0.843220
2021-12-28 15:04:57,366 |	  model_v_in_main test loss : 0.899849
2021-12-28 15:04:57,370 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:04:57,371 |	  Step count: 104
2021-12-28 15:04:57,375 |	  attentionweight:tensor([2.3128e-08, 2.3128e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:12,378 |	  attentionweight:tensor([2.3128e-08, 2.3128e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:12,661 |	  attentionweight:tensor([2.3128e-08, 2.3128e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:16,544 |	  attentionweight:tensor([2.3128e-08, 2.3128e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:16,635 |	  attentionweight:tensor([2.3128e-08, 2.3128e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:17,051 |	  attentionweight:tensor([2.3006e-08, 2.3189e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:17,175 |	  loss_w (train):3.0619573543333445e-09
2021-12-28 15:05:20,423 |	  v_loss (train):71.45806121826172
2021-12-28 15:05:21,048 |	  model_w_in_main test loss : 0.843192
2021-12-28 15:05:21,095 |	  model_v_in_main test loss : 0.896376
2021-12-28 15:05:21,099 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:05:21,100 |	  Step count: 105
2021-12-28 15:05:21,102 |	  attentionweight:tensor([2.2913e-08, 2.2913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:31,379 |	  attentionweight:tensor([2.2913e-08, 2.2913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:31,507 |	  attentionweight:tensor([2.2913e-08, 2.2913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:32,939 |	  attentionweight:tensor([2.2913e-08, 2.2913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:33,256 |	  attentionweight:tensor([2.2913e-08, 2.2913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:33,424 |	  attentionweight:tensor([2.2409e-08, 2.3007e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:33,545 |	  loss_w (train):3.818318994319725e-09
2021-12-28 15:05:34,686 |	  v_loss (train):21.307024002075195
2021-12-28 15:05:35,335 |	  model_w_in_main test loss : 0.843224
2021-12-28 15:05:35,393 |	  model_v_in_main test loss : 0.901073
2021-12-28 15:05:35,397 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:05:35,399 |	  Step count: 106
2021-12-28 15:05:35,401 |	  attentionweight:tensor([2.2765e-08, 2.2765e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:50,984 |	  attentionweight:tensor([2.2765e-08, 2.2765e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:51,264 |	  attentionweight:tensor([2.2765e-08, 2.2765e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:55,058 |	  attentionweight:tensor([2.2765e-08, 2.2765e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:55,263 |	  attentionweight:tensor([2.2765e-08, 2.2765e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:55,436 |	  attentionweight:tensor([2.1299e-08, 2.1931e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:05:55,482 |	  loss_w (train):6.056498635942376e-10
2021-12-28 15:05:57,831 |	  v_loss (train):93.9518814086914
2021-12-28 15:05:58,359 |	  model_w_in_main test loss : 0.843159
2021-12-28 15:05:58,609 |	  model_v_in_main test loss : 0.906396
2021-12-28 15:05:58,612 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:05:58,614 |	  Step count: 107
2021-12-28 15:05:58,617 |	  attentionweight:tensor([2.2251e-08, 2.2251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:08,957 |	  attentionweight:tensor([2.2251e-08, 2.2251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:09,113 |	  attentionweight:tensor([2.2251e-08, 2.2251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:10,692 |	  attentionweight:tensor([2.2251e-08, 2.2251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:10,855 |	  attentionweight:tensor([2.2251e-08, 2.2251e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:11,275 |	  attentionweight:tensor([2.1818e-08, 2.1108e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:11,324 |	  loss_w (train):4.377482376582975e-09
2021-12-28 15:06:12,575 |	  v_loss (train):37.50190734863281
2021-12-28 15:06:13,178 |	  model_w_in_main test loss : 0.843182
2021-12-28 15:06:13,231 |	  model_v_in_main test loss : 0.904509
2021-12-28 15:06:13,235 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:06:13,237 |	  Step count: 108
2021-12-28 15:06:13,240 |	  attentionweight:tensor([2.1780e-08, 2.1780e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:26,044 |	  attentionweight:tensor([2.1780e-08, 2.1780e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:26,337 |	  attentionweight:tensor([2.1780e-08, 2.1780e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:29,047 |	  attentionweight:tensor([2.1780e-08, 2.1780e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:29,328 |	  attentionweight:tensor([2.1780e-08, 2.1780e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:29,482 |	  attentionweight:tensor([2.1374e-08, 2.1490e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:29,533 |	  loss_w (train):6.294943233342565e-09
2021-12-28 15:06:32,017 |	  v_loss (train):22.108448028564453
2021-12-28 15:06:32,554 |	  model_w_in_main test loss : 0.843187
2021-12-28 15:06:32,707 |	  model_v_in_main test loss : 0.907575
2021-12-28 15:06:32,711 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:06:32,714 |	  Step count: 109
2021-12-28 15:06:32,717 |	  attentionweight:tensor([2.1500e-08, 2.1500e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:44,441 |	  attentionweight:tensor([2.1500e-08, 2.1500e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:44,524 |	  attentionweight:tensor([2.1500e-08, 2.1500e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:46,341 |	  attentionweight:tensor([2.1500e-08, 2.1500e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:46,501 |	  attentionweight:tensor([2.1500e-08, 2.1500e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:46,914 |	  attentionweight:tensor([2.0425e-08, 1.7873e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:06:46,983 |	  loss_w (train):2.1395770488652488e-08
2021-12-28 15:06:48,760 |	  v_loss (train):34.06354904174805
2021-12-28 15:06:49,364 |	  model_w_in_main test loss : 0.843175
2021-12-28 15:06:49,411 |	  model_v_in_main test loss : 0.912589
2021-12-28 15:06:49,415 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:06:49,417 |	  Step count: 110
2021-12-28 15:06:49,419 |	  attentionweight:tensor([2.0420e-08, 2.0420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:00,114 |	  attentionweight:tensor([2.0420e-08, 2.0420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:00,194 |	  attentionweight:tensor([2.0420e-08, 2.0420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:01,946 |	  attentionweight:tensor([2.0420e-08, 2.0420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:02,026 |	  attentionweight:tensor([2.0420e-08, 2.0420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:02,208 |	  attentionweight:tensor([1.9998e-08, 1.9874e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:02,327 |	  loss_w (train):7.389680534153342e-11
2021-12-28 15:07:03,564 |	  v_loss (train):15.50284194946289
2021-12-28 15:07:04,093 |	  model_w_in_main test loss : 0.843219
2021-12-28 15:07:04,187 |	  model_v_in_main test loss : 0.917531
2021-12-28 15:07:04,191 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:07:04,193 |	  Step count: 111
2021-12-28 15:07:04,197 |	  attentionweight:tensor([1.9913e-08, 1.9913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:18,337 |	  attentionweight:tensor([1.9913e-08, 1.9913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:18,628 |	  attentionweight:tensor([1.9913e-08, 1.9913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:22,472 |	  attentionweight:tensor([1.9913e-08, 1.9913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:22,576 |	  attentionweight:tensor([1.9913e-08, 1.9913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:22,919 |	  attentionweight:tensor([1.9345e-08, 1.9271e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:23,039 |	  loss_w (train):3.6029745853483064e-10
2021-12-28 15:07:26,256 |	  v_loss (train):100.2447738647461
2021-12-28 15:07:26,867 |	  model_w_in_main test loss : 0.843198
2021-12-28 15:07:26,923 |	  model_v_in_main test loss : 0.910905
2021-12-28 15:07:26,927 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:07:26,929 |	  Step count: 112
2021-12-28 15:07:26,931 |	  attentionweight:tensor([1.9518e-08, 1.9518e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:43,127 |	  attentionweight:tensor([1.9518e-08, 1.9518e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:43,395 |	  attentionweight:tensor([1.9518e-08, 1.9518e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:48,026 |	  attentionweight:tensor([1.9518e-08, 1.9518e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:48,312 |	  attentionweight:tensor([1.9518e-08, 1.9518e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:48,458 |	  attentionweight:tensor([1.9302e-08, 1.9528e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:07:48,609 |	  loss_w (train):8.325898304128998e-11
2021-12-28 15:07:52,277 |	  v_loss (train):129.62301635742188
2021-12-28 15:07:52,740 |	  model_w_in_main test loss : 0.843157
2021-12-28 15:07:52,864 |	  model_v_in_main test loss : 0.910709
2021-12-28 15:07:52,868 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:07:52,870 |	  Step count: 113
2021-12-28 15:07:52,876 |	  attentionweight:tensor([1.9360e-08, 1.9360e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:07,575 |	  attentionweight:tensor([1.9360e-08, 1.9360e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:07,757 |	  attentionweight:tensor([1.9360e-08, 1.9360e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:10,871 |	  attentionweight:tensor([1.9360e-08, 1.9360e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:11,164 |	  attentionweight:tensor([1.9360e-08, 1.9360e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:11,352 |	  attentionweight:tensor([1.8341e-08, 1.8994e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:11,470 |	  loss_w (train):5.669158475996028e-09
2021-12-28 15:08:14,233 |	  v_loss (train):42.411231994628906
2021-12-28 15:08:14,856 |	  model_w_in_main test loss : 0.843195
2021-12-28 15:08:14,904 |	  model_v_in_main test loss : 0.899517
2021-12-28 15:08:14,908 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:08:14,909 |	  Step count: 114
2021-12-28 15:08:14,912 |	  attentionweight:tensor([1.9028e-08, 1.9028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:33,405 |	  attentionweight:tensor([1.9028e-08, 1.9028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:33,679 |	  attentionweight:tensor([1.9028e-08, 1.9028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:38,550 |	  attentionweight:tensor([1.9028e-08, 1.9028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:38,656 |	  attentionweight:tensor([1.9028e-08, 1.9028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:38,996 |	  attentionweight:tensor([1.9898e-08, 2.2645e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:39,107 |	  loss_w (train):7.2252968053021505e-09
2021-12-28 15:08:43,484 |	  v_loss (train):148.2244415283203
2021-12-28 15:08:44,104 |	  model_w_in_main test loss : 0.843254
2021-12-28 15:08:44,151 |	  model_v_in_main test loss : 0.893446
2021-12-28 15:08:44,155 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:08:44,157 |	  Step count: 115
2021-12-28 15:08:44,159 |	  attentionweight:tensor([1.9791e-08, 1.9791e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:56,034 |	  attentionweight:tensor([1.9791e-08, 1.9791e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:56,126 |	  attentionweight:tensor([1.9791e-08, 1.9791e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:58,534 |	  attentionweight:tensor([1.9791e-08, 1.9791e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:58,626 |	  attentionweight:tensor([1.9791e-08, 1.9791e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:59,028 |	  attentionweight:tensor([2.0184e-08, 2.0118e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:08:59,155 |	  loss_w (train):1.2313366992700026e-09
2021-12-28 15:09:01,071 |	  v_loss (train):16.53329086303711
2021-12-28 15:09:01,648 |	  model_w_in_main test loss : 0.843141
2021-12-28 15:09:01,698 |	  model_v_in_main test loss : 0.896045
2021-12-28 15:09:01,702 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:09:01,704 |	  Step count: 116
2021-12-28 15:09:01,706 |	  attentionweight:tensor([2.0171e-08, 2.0171e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:13,151 |	  attentionweight:tensor([2.0171e-08, 2.0171e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:13,334 |	  attentionweight:tensor([2.0171e-08, 2.0171e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:14,560 |	  attentionweight:tensor([2.0171e-08, 2.0171e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:14,859 |	  attentionweight:tensor([2.0171e-08, 2.0171e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:15,057 |	  attentionweight:tensor([2.4394e-08, 2.5914e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:15,170 |	  loss_w (train):8.948703111855139e-09
2021-12-28 15:09:16,160 |	  v_loss (train):15.228129386901855
2021-12-28 15:09:16,770 |	  model_w_in_main test loss : 0.843223
2021-12-28 15:09:16,823 |	  model_v_in_main test loss : 0.905505
2021-12-28 15:09:16,827 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:09:16,829 |	  Step count: 117
2021-12-28 15:09:16,831 |	  attentionweight:tensor([2.2177e-08, 2.2177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:28,421 |	  attentionweight:tensor([2.2177e-08, 2.2177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:28,514 |	  attentionweight:tensor([2.2177e-08, 2.2177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:30,531 |	  attentionweight:tensor([2.2177e-08, 2.2177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:30,832 |	  attentionweight:tensor([2.2177e-08, 2.2177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:31,048 |	  attentionweight:tensor([2.4499e-08, 2.3462e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:31,167 |	  loss_w (train):2.9073197183038246e-09
2021-12-28 15:09:32,740 |	  v_loss (train):33.711280822753906
2021-12-28 15:09:33,364 |	  model_w_in_main test loss : 0.843236
2021-12-28 15:09:33,422 |	  model_v_in_main test loss : 0.901749
2021-12-28 15:09:33,426 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:09:33,428 |	  Step count: 118
2021-12-28 15:09:33,431 |	  attentionweight:tensor([2.3541e-08, 2.3541e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:49,885 |	  attentionweight:tensor([2.3541e-08, 2.3541e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:50,008 |	  attentionweight:tensor([2.3541e-08, 2.3541e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:54,448 |	  attentionweight:tensor([2.3541e-08, 2.3541e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:54,638 |	  attentionweight:tensor([2.3541e-08, 2.3541e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:55,060 |	  attentionweight:tensor([2.2958e-08, 2.3751e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:09:55,110 |	  loss_w (train):1.9071271228909836e-09
2021-12-28 15:09:59,006 |	  v_loss (train):118.1911392211914
2021-12-28 15:09:59,630 |	  model_w_in_main test loss : 0.843221
2021-12-28 15:09:59,684 |	  model_v_in_main test loss : 0.901696
2021-12-28 15:09:59,688 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:09:59,690 |	  Step count: 119
2021-12-28 15:09:59,699 |	  attentionweight:tensor([2.3889e-08, 2.3889e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:12,570 |	  attentionweight:tensor([2.3889e-08, 2.3889e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:12,730 |	  attentionweight:tensor([2.3889e-08, 2.3889e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:15,413 |	  attentionweight:tensor([2.3889e-08, 2.3889e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:15,732 |	  attentionweight:tensor([2.3889e-08, 2.3889e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:15,904 |	  attentionweight:tensor([2.4241e-08, 2.4881e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:16,044 |	  loss_w (train):2.056973480435431e-09
2021-12-28 15:10:18,362 |	  v_loss (train):42.369590759277344
2021-12-28 15:10:18,993 |	  model_w_in_main test loss : 0.843177
2021-12-28 15:10:19,042 |	  model_v_in_main test loss : 0.915231
2021-12-28 15:10:19,046 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:10:19,048 |	  Step count: 120
2021-12-28 15:10:19,050 |	  attentionweight:tensor([2.4262e-08, 2.4262e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:29,353 |	  attentionweight:tensor([2.4262e-08, 2.4262e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:29,436 |	  attentionweight:tensor([2.4262e-08, 2.4262e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:30,720 |	  attentionweight:tensor([2.4262e-08, 2.4262e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:30,892 |	  attentionweight:tensor([2.4262e-08, 2.4262e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:31,302 |	  attentionweight:tensor([2.2262e-08, 2.2969e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:31,356 |	  loss_w (train):7.33674399011619e-10
2021-12-28 15:10:32,498 |	  v_loss (train):63.848663330078125
2021-12-28 15:10:33,123 |	  model_w_in_main test loss : 0.843220
2021-12-28 15:10:33,175 |	  model_v_in_main test loss : 0.913858
2021-12-28 15:10:33,179 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:10:33,181 |	  Step count: 121
2021-12-28 15:10:33,183 |	  attentionweight:tensor([2.3699e-08, 2.3699e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:52,981 |	  attentionweight:tensor([2.3699e-08, 2.3699e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:53,059 |	  attentionweight:tensor([2.3699e-08, 2.3699e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:58,606 |	  attentionweight:tensor([2.3699e-08, 2.3699e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:58,908 |	  attentionweight:tensor([2.3699e-08, 2.3699e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:59,130 |	  attentionweight:tensor([2.1792e-08, 1.9820e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:10:59,250 |	  loss_w (train):1.4204124099137516e-08
2021-12-28 15:11:04,367 |	  v_loss (train):158.17892456054688
2021-12-28 15:11:04,959 |	  model_w_in_main test loss : 0.843274
2021-12-28 15:11:05,006 |	  model_v_in_main test loss : 0.912611
2021-12-28 15:11:05,010 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:11:05,011 |	  Step count: 122
2021-12-28 15:11:05,014 |	  attentionweight:tensor([2.2325e-08, 2.2325e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:21,694 |	  attentionweight:tensor([2.2325e-08, 2.2325e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:21,809 |	  attentionweight:tensor([2.2325e-08, 2.2325e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:26,507 |	  attentionweight:tensor([2.2325e-08, 2.2325e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:26,684 |	  attentionweight:tensor([2.2325e-08, 2.2325e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:27,086 |	  attentionweight:tensor([2.1617e-08, 2.1654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:27,135 |	  loss_w (train):3.973627649145328e-09
2021-12-28 15:11:31,237 |	  v_loss (train):67.32486724853516
2021-12-28 15:11:31,856 |	  model_w_in_main test loss : 0.843223
2021-12-28 15:11:31,911 |	  model_v_in_main test loss : 0.912335
2021-12-28 15:11:31,915 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:11:31,917 |	  Step count: 123
2021-12-28 15:11:31,919 |	  attentionweight:tensor([2.1652e-08, 2.1652e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:48,046 |	  attentionweight:tensor([2.1652e-08, 2.1652e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:48,138 |	  attentionweight:tensor([2.1652e-08, 2.1652e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:52,890 |	  attentionweight:tensor([2.1652e-08, 2.1652e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:52,970 |	  attentionweight:tensor([2.1652e-08, 2.1652e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:53,343 |	  attentionweight:tensor([2.2166e-08, 2.3379e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:11:53,448 |	  loss_w (train):2.9888851393877758e-09
2021-12-28 15:11:57,086 |	  v_loss (train):119.29261779785156
2021-12-28 15:11:57,705 |	  model_w_in_main test loss : 0.843265
2021-12-28 15:11:57,757 |	  model_v_in_main test loss : 0.907148
2021-12-28 15:11:57,761 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:11:57,764 |	  Step count: 124
2021-12-28 15:11:57,766 |	  attentionweight:tensor([2.1892e-08, 2.1892e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:14,667 |	  attentionweight:tensor([2.1892e-08, 2.1892e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:14,749 |	  attentionweight:tensor([2.1892e-08, 2.1892e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:19,585 |	  attentionweight:tensor([2.1892e-08, 2.1892e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:19,763 |	  attentionweight:tensor([2.1892e-08, 2.1892e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:20,155 |	  attentionweight:tensor([2.4963e-08, 2.6286e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:20,201 |	  loss_w (train):5.794617763399401e-08
2021-12-28 15:12:24,966 |	  v_loss (train):71.8470458984375
2021-12-28 15:12:25,546 |	  model_w_in_main test loss : 0.843170
2021-12-28 15:12:25,592 |	  model_v_in_main test loss : 0.914555
2021-12-28 15:12:25,596 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:12:25,598 |	  Step count: 125
2021-12-28 15:12:25,600 |	  attentionweight:tensor([2.3397e-08, 2.3397e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:40,087 |	  attentionweight:tensor([2.3397e-08, 2.3397e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:40,169 |	  attentionweight:tensor([2.3397e-08, 2.3397e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:43,569 |	  attentionweight:tensor([2.3397e-08, 2.3397e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:43,890 |	  attentionweight:tensor([2.3397e-08, 2.3397e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:44,042 |	  attentionweight:tensor([2.3619e-08, 2.1016e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:12:44,143 |	  loss_w (train):1.0965164332077393e-09
2021-12-28 15:12:47,247 |	  v_loss (train):84.72988891601562
2021-12-28 15:12:47,893 |	  model_w_in_main test loss : 0.843162
2021-12-28 15:12:47,960 |	  model_v_in_main test loss : 0.917942
2021-12-28 15:12:47,968 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:12:47,970 |	  Step count: 126
2021-12-28 15:12:47,974 |	  attentionweight:tensor([2.3405e-08, 2.3405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:04,154 |	  attentionweight:tensor([2.3405e-08, 2.3405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:04,470 |	  attentionweight:tensor([2.3405e-08, 2.3405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:08,851 |	  attentionweight:tensor([2.3405e-08, 2.3405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:08,974 |	  attentionweight:tensor([2.3405e-08, 2.3405e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:09,152 |	  attentionweight:tensor([1.7965e-08, 2.1681e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:09,257 |	  loss_w (train):5.640591549394003e-09
2021-12-28 15:13:13,132 |	  v_loss (train):83.21463012695312
2021-12-28 15:13:13,733 |	  model_w_in_main test loss : 0.843211
2021-12-28 15:13:13,781 |	  model_v_in_main test loss : 0.914443
2021-12-28 15:13:13,784 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:13:13,786 |	  Step count: 127
2021-12-28 15:13:13,788 |	  attentionweight:tensor([2.1859e-08, 2.1859e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:32,768 |	  attentionweight:tensor([2.1859e-08, 2.1859e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:33,094 |	  attentionweight:tensor([2.1859e-08, 2.1859e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:38,035 |	  attentionweight:tensor([2.1859e-08, 2.1859e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:38,174 |	  attentionweight:tensor([2.1859e-08, 2.1859e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:38,602 |	  attentionweight:tensor([2.0152e-08, 1.9354e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:38,647 |	  loss_w (train):6.763479554194873e-09
2021-12-28 15:13:43,794 |	  v_loss (train):127.06906127929688
2021-12-28 15:13:44,412 |	  model_w_in_main test loss : 0.843238
2021-12-28 15:13:44,465 |	  model_v_in_main test loss : 0.907520
2021-12-28 15:13:44,469 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:13:44,471 |	  Step count: 128
2021-12-28 15:13:44,473 |	  attentionweight:tensor([2.0556e-08, 2.0556e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:55,740 |	  attentionweight:tensor([2.0556e-08, 2.0556e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:55,818 |	  attentionweight:tensor([2.0556e-08, 2.0556e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:57,035 |	  attentionweight:tensor([2.0556e-08, 2.0556e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:57,217 |	  attentionweight:tensor([2.0556e-08, 2.0556e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:57,580 |	  attentionweight:tensor([1.9085e-08, 1.5195e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:13:57,639 |	  loss_w (train):4.9882702413128754e-09
2021-12-28 15:13:58,693 |	  v_loss (train):51.981346130371094
2021-12-28 15:13:59,305 |	  model_w_in_main test loss : 0.843234
2021-12-28 15:13:59,355 |	  model_v_in_main test loss : 0.912449
2021-12-28 15:13:59,359 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:13:59,361 |	  Step count: 129
2021-12-28 15:13:59,363 |	  attentionweight:tensor([1.8698e-08, 1.8698e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:11,052 |	  attentionweight:tensor([1.8698e-08, 1.8698e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:11,131 |	  attentionweight:tensor([1.8698e-08, 1.8698e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:12,476 |	  attentionweight:tensor([1.8698e-08, 1.8698e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:12,791 |	  attentionweight:tensor([1.8698e-08, 1.8698e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:13,051 |	  attentionweight:tensor([1.9219e-08, 1.6564e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:13,355 |	  loss_w (train):2.7179400952093147e-09
2021-12-28 15:14:15,183 |	  v_loss (train):95.03121185302734
2021-12-28 15:14:15,842 |	  model_w_in_main test loss : 0.843261
2021-12-28 15:14:15,967 |	  model_v_in_main test loss : 0.910840
2021-12-28 15:14:15,972 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:14:15,974 |	  Step count: 130
2021-12-28 15:14:15,978 |	  attentionweight:tensor([1.7833e-08, 1.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:26,729 |	  attentionweight:tensor([1.7833e-08, 1.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:26,826 |	  attentionweight:tensor([1.7833e-08, 1.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:28,060 |	  attentionweight:tensor([1.7833e-08, 1.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:28,155 |	  attentionweight:tensor([1.7833e-08, 1.7833e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:28,480 |	  attentionweight:tensor([1.7426e-08, 1.7354e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:28,644 |	  loss_w (train):2.7427895510356848e-09
2021-12-28 15:14:29,742 |	  v_loss (train):6.383997440338135
2021-12-28 15:14:30,231 |	  model_w_in_main test loss : 0.843273
2021-12-28 15:14:30,401 |	  model_v_in_main test loss : 0.906173
2021-12-28 15:14:30,405 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:14:30,407 |	  Step count: 131
2021-12-28 15:14:30,410 |	  attentionweight:tensor([1.7404e-08, 1.7404e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:46,466 |	  attentionweight:tensor([1.7404e-08, 1.7404e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:46,563 |	  attentionweight:tensor([1.7404e-08, 1.7404e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:50,855 |	  attentionweight:tensor([1.7404e-08, 1.7404e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:50,947 |	  attentionweight:tensor([1.7404e-08, 1.7404e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:51,359 |	  attentionweight:tensor([1.9784e-08, 1.7852e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:14:51,477 |	  loss_w (train):7.73582886637314e-09
2021-12-28 15:14:54,880 |	  v_loss (train):81.23147583007812
2021-12-28 15:14:55,484 |	  model_w_in_main test loss : 0.843270
2021-12-28 15:14:55,565 |	  model_v_in_main test loss : 0.903230
2021-12-28 15:14:55,569 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:14:55,571 |	  Step count: 132
2021-12-28 15:14:55,574 |	  attentionweight:tensor([1.7829e-08, 1.7829e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:06,163 |	  attentionweight:tensor([1.7829e-08, 1.7829e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:06,278 |	  attentionweight:tensor([1.7829e-08, 1.7829e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:08,044 |	  attentionweight:tensor([1.7829e-08, 1.7829e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:08,122 |	  attentionweight:tensor([1.7829e-08, 1.7829e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:08,503 |	  attentionweight:tensor([1.8202e-08, 1.7794e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:08,609 |	  loss_w (train):3.5574483359113174e-09
2021-12-28 15:15:10,925 |	  v_loss (train):14.735003471374512
2021-12-28 15:15:11,490 |	  model_w_in_main test loss : 0.843250
2021-12-28 15:15:11,612 |	  model_v_in_main test loss : 0.893936
2021-12-28 15:15:11,616 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:15:11,619 |	  Step count: 133
2021-12-28 15:15:11,622 |	  attentionweight:tensor([1.8026e-08, 1.8026e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:21,897 |	  attentionweight:tensor([1.8026e-08, 1.8026e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:22,021 |	  attentionweight:tensor([1.8026e-08, 1.8026e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:23,689 |	  attentionweight:tensor([1.8026e-08, 1.8026e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:23,755 |	  attentionweight:tensor([1.8026e-08, 1.8026e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:23,948 |	  attentionweight:tensor([1.8115e-08, 1.8061e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:24,024 |	  loss_w (train):1.8996280104488505e-09
2021-12-28 15:15:25,143 |	  v_loss (train):14.586963653564453
2021-12-28 15:15:25,697 |	  model_w_in_main test loss : 0.843219
2021-12-28 15:15:25,843 |	  model_v_in_main test loss : 0.898338
2021-12-28 15:15:25,849 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:15:25,851 |	  Step count: 134
2021-12-28 15:15:25,854 |	  attentionweight:tensor([1.8110e-08, 1.8110e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:43,051 |	  attentionweight:tensor([1.8110e-08, 1.8110e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:43,324 |	  attentionweight:tensor([1.8110e-08, 1.8110e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:47,639 |	  attentionweight:tensor([1.8110e-08, 1.8110e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:47,733 |	  attentionweight:tensor([1.8110e-08, 1.8110e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:48,120 |	  attentionweight:tensor([2.4446e-08, 2.8286e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:15:48,225 |	  loss_w (train):3.758891864436009e-09
2021-12-28 15:15:52,495 |	  v_loss (train):82.04068756103516
2021-12-28 15:15:53,101 |	  model_w_in_main test loss : 0.843204
2021-12-28 15:15:53,154 |	  model_v_in_main test loss : 0.897581
2021-12-28 15:15:53,158 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:15:53,160 |	  Step count: 135
2021-12-28 15:15:53,162 |	  attentionweight:tensor([2.1107e-08, 2.1107e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:06,963 |	  attentionweight:tensor([2.1107e-08, 2.1107e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:07,137 |	  attentionweight:tensor([2.1107e-08, 2.1107e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:10,083 |	  attentionweight:tensor([2.1107e-08, 2.1107e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:10,232 |	  attentionweight:tensor([2.1107e-08, 2.1107e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:10,604 |	  attentionweight:tensor([2.5354e-08, 2.3337e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:10,649 |	  loss_w (train):7.406283919486611e-10
2021-12-28 15:16:13,209 |	  v_loss (train):85.75871276855469
2021-12-28 15:16:13,772 |	  model_w_in_main test loss : 0.843153
2021-12-28 15:16:14,026 |	  model_v_in_main test loss : 0.904212
2021-12-28 15:16:14,034 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:16:14,036 |	  Step count: 136
2021-12-28 15:16:14,038 |	  attentionweight:tensor([2.3390e-08, 2.3390e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:26,474 |	  attentionweight:tensor([2.3390e-08, 2.3390e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:26,556 |	  attentionweight:tensor([2.3390e-08, 2.3390e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:29,531 |	  attentionweight:tensor([2.3390e-08, 2.3390e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:29,600 |	  attentionweight:tensor([2.3390e-08, 2.3390e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:29,788 |	  attentionweight:tensor([2.3980e-08, 2.4206e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:29,861 |	  loss_w (train):1.6402997848885548e-09
2021-12-28 15:16:32,119 |	  v_loss (train):35.925865173339844
2021-12-28 15:16:32,674 |	  model_w_in_main test loss : 0.843224
2021-12-28 15:16:32,802 |	  model_v_in_main test loss : 0.898495
2021-12-28 15:16:32,806 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:16:32,808 |	  Step count: 137
2021-12-28 15:16:32,811 |	  attentionweight:tensor([2.4409e-08, 2.4409e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:45,982 |	  attentionweight:tensor([2.4409e-08, 2.4409e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:46,064 |	  attentionweight:tensor([2.4409e-08, 2.4409e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:49,001 |	  attentionweight:tensor([2.4409e-08, 2.4409e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:49,079 |	  attentionweight:tensor([2.4409e-08, 2.4409e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:49,258 |	  attentionweight:tensor([2.2751e-08, 2.4612e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:16:49,331 |	  loss_w (train):2.4438773227331012e-09
2021-12-28 15:16:52,296 |	  v_loss (train):71.36325073242188
2021-12-28 15:16:52,821 |	  model_w_in_main test loss : 0.843195
2021-12-28 15:16:52,925 |	  model_v_in_main test loss : 0.900025
2021-12-28 15:16:52,929 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:16:52,930 |	  Step count: 138
2021-12-28 15:16:52,934 |	  attentionweight:tensor([2.4420e-08, 2.4420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:04,485 |	  attentionweight:tensor([2.4420e-08, 2.4420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:04,676 |	  attentionweight:tensor([2.4420e-08, 2.4420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:06,835 |	  attentionweight:tensor([2.4420e-08, 2.4420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:07,108 |	  attentionweight:tensor([2.4420e-08, 2.4420e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:07,279 |	  attentionweight:tensor([2.5670e-08, 2.9925e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:07,380 |	  loss_w (train):3.0396876127269934e-09
2021-12-28 15:17:09,411 |	  v_loss (train):50.914588928222656
2021-12-28 15:17:10,009 |	  model_w_in_main test loss : 0.843142
2021-12-28 15:17:10,093 |	  model_v_in_main test loss : 0.911718
2021-12-28 15:17:10,097 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:17:10,099 |	  Step count: 139
2021-12-28 15:17:10,102 |	  attentionweight:tensor([2.5688e-08, 2.5688e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:21,404 |	  attentionweight:tensor([2.5688e-08, 2.5688e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:21,542 |	  attentionweight:tensor([2.5688e-08, 2.5688e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:23,485 |	  attentionweight:tensor([2.5688e-08, 2.5688e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:23,793 |	  attentionweight:tensor([2.5688e-08, 2.5688e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:23,991 |	  attentionweight:tensor([2.6610e-08, 2.6295e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:24,118 |	  loss_w (train):1.593558063284206e-10
2021-12-28 15:17:25,842 |	  v_loss (train):37.67402267456055
2021-12-28 15:17:26,450 |	  model_w_in_main test loss : 0.843181
2021-12-28 15:17:26,500 |	  model_v_in_main test loss : 0.921538
2021-12-28 15:17:26,504 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:17:26,505 |	  Step count: 140
2021-12-28 15:17:26,508 |	  attentionweight:tensor([2.6389e-08, 2.6389e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:42,688 |	  attentionweight:tensor([2.6389e-08, 2.6389e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:42,992 |	  attentionweight:tensor([2.6389e-08, 2.6389e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:46,768 |	  attentionweight:tensor([2.6389e-08, 2.6389e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:46,912 |	  attentionweight:tensor([2.6389e-08, 2.6389e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:47,315 |	  attentionweight:tensor([2.6197e-08, 2.4378e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:17:47,361 |	  loss_w (train):2.538043997191153e-09
2021-12-28 15:17:50,561 |	  v_loss (train):107.20228576660156
2021-12-28 15:17:51,109 |	  model_w_in_main test loss : 0.843295
2021-12-28 15:17:51,261 |	  model_v_in_main test loss : 0.923297
2021-12-28 15:17:51,265 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:17:51,267 |	  Step count: 141
2021-12-28 15:17:51,269 |	  attentionweight:tensor([2.6152e-08, 2.6152e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:11,253 |	  attentionweight:tensor([2.6152e-08, 2.6152e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:11,344 |	  attentionweight:tensor([2.6152e-08, 2.6152e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:18,300 |	  attentionweight:tensor([2.6152e-08, 2.6152e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:18,391 |	  attentionweight:tensor([2.6152e-08, 2.6152e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:18,780 |	  attentionweight:tensor([2.6503e-08, 2.6456e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:18,914 |	  loss_w (train):6.527516027698965e-11
2021-12-28 15:18:24,870 |	  v_loss (train):1923.116455078125
2021-12-28 15:18:25,477 |	  model_w_in_main test loss : 0.843173
2021-12-28 15:18:25,605 |	  model_v_in_main test loss : 0.930723
2021-12-28 15:18:25,609 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:18:25,611 |	  Step count: 142
2021-12-28 15:18:25,614 |	  attentionweight:tensor([2.6210e-08, 2.6210e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:37,444 |	  attentionweight:tensor([2.6210e-08, 2.6210e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:37,518 |	  attentionweight:tensor([2.6210e-08, 2.6210e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:38,473 |	  attentionweight:tensor([2.6210e-08, 2.6210e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:38,804 |	  attentionweight:tensor([2.6210e-08, 2.6210e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:38,984 |	  attentionweight:tensor([2.3986e-08, 2.4496e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:39,139 |	  loss_w (train):2.3815442951047316e-09
2021-12-28 15:18:41,147 |	  v_loss (train):20.616910934448242
2021-12-28 15:18:41,711 |	  model_w_in_main test loss : 0.843146
2021-12-28 15:18:41,798 |	  model_v_in_main test loss : 0.930568
2021-12-28 15:18:41,802 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:18:41,804 |	  Step count: 143
2021-12-28 15:18:41,806 |	  attentionweight:tensor([2.5426e-08, 2.5426e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:53,720 |	  attentionweight:tensor([2.5426e-08, 2.5426e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:53,811 |	  attentionweight:tensor([2.5426e-08, 2.5426e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:55,094 |	  attentionweight:tensor([2.5426e-08, 2.5426e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:55,256 |	  attentionweight:tensor([2.5426e-08, 2.5426e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:55,659 |	  attentionweight:tensor([2.5171e-08, 2.6228e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:18:55,709 |	  loss_w (train):1.064947241502523e-09
2021-12-28 15:18:56,639 |	  v_loss (train):2.9516351222991943
2021-12-28 15:18:57,209 |	  model_w_in_main test loss : 0.843156
2021-12-28 15:18:57,322 |	  model_v_in_main test loss : 0.925810
2021-12-28 15:18:57,326 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:18:57,328 |	  Step count: 144
2021-12-28 15:18:57,330 |	  attentionweight:tensor([2.5299e-08, 2.5299e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:14,350 |	  attentionweight:tensor([2.5299e-08, 2.5299e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:14,517 |	  attentionweight:tensor([2.5299e-08, 2.5299e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:19,042 |	  attentionweight:tensor([2.5299e-08, 2.5299e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:19,354 |	  attentionweight:tensor([2.5299e-08, 2.5299e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:19,501 |	  attentionweight:tensor([2.2112e-08, 2.4400e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:19,638 |	  loss_w (train):2.0878789808165266e-09
2021-12-28 15:19:24,016 |	  v_loss (train):104.69986724853516
2021-12-28 15:19:24,615 |	  model_w_in_main test loss : 0.843226
2021-12-28 15:19:24,755 |	  model_v_in_main test loss : 0.924882
2021-12-28 15:19:24,761 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:19:24,763 |	  Step count: 145
2021-12-28 15:19:24,766 |	  attentionweight:tensor([2.4415e-08, 2.4415e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:35,482 |	  attentionweight:tensor([2.4415e-08, 2.4415e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:35,659 |	  attentionweight:tensor([2.4415e-08, 2.4415e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:37,402 |	  attentionweight:tensor([2.4415e-08, 2.4415e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:37,492 |	  attentionweight:tensor([2.4415e-08, 2.4415e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:37,883 |	  attentionweight:tensor([2.3376e-08, 2.1453e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:38,002 |	  loss_w (train):9.553481339352743e-10
2021-12-28 15:19:40,672 |	  v_loss (train):22.258832931518555
2021-12-28 15:19:41,314 |	  model_w_in_main test loss : 0.843226
2021-12-28 15:19:41,413 |	  model_v_in_main test loss : 0.925494
2021-12-28 15:19:41,418 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:19:41,421 |	  Step count: 146
2021-12-28 15:19:41,424 |	  attentionweight:tensor([2.3335e-08, 2.3335e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:59,864 |	  attentionweight:tensor([2.3335e-08, 2.3335e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:19:59,944 |	  attentionweight:tensor([2.3335e-08, 2.3335e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:06,449 |	  attentionweight:tensor([2.3335e-08, 2.3335e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:06,764 |	  attentionweight:tensor([2.3335e-08, 2.3335e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:06,921 |	  attentionweight:tensor([2.0836e-08, 2.0580e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:07,024 |	  loss_w (train):1.9894816460919174e-08
2021-12-28 15:20:12,674 |	  v_loss (train):141.96820068359375
2021-12-28 15:20:13,326 |	  model_w_in_main test loss : 0.843166
2021-12-28 15:20:13,382 |	  model_v_in_main test loss : 0.925917
2021-12-28 15:20:13,388 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:20:13,390 |	  Step count: 147
2021-12-28 15:20:13,393 |	  attentionweight:tensor([2.1942e-08, 2.1942e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:27,156 |	  attentionweight:tensor([2.1942e-08, 2.1942e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:27,440 |	  attentionweight:tensor([2.1942e-08, 2.1942e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:30,620 |	  attentionweight:tensor([2.1942e-08, 2.1942e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:30,760 |	  attentionweight:tensor([2.1942e-08, 2.1942e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:31,085 |	  attentionweight:tensor([2.1462e-08, 2.2607e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:31,200 |	  loss_w (train):4.390549701582813e-09
2021-12-28 15:20:33,819 |	  v_loss (train):100.58499908447266
2021-12-28 15:20:34,464 |	  model_w_in_main test loss : 0.843143
2021-12-28 15:20:34,517 |	  model_v_in_main test loss : 0.932521
2021-12-28 15:20:34,521 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:20:34,523 |	  Step count: 148
2021-12-28 15:20:34,525 |	  attentionweight:tensor([2.1573e-08, 2.1573e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:55,618 |	  attentionweight:tensor([2.1573e-08, 2.1573e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:20:55,739 |	  attentionweight:tensor([2.1573e-08, 2.1573e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:02,657 |	  attentionweight:tensor([2.1573e-08, 2.1573e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:02,737 |	  attentionweight:tensor([2.1573e-08, 2.1573e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:03,136 |	  attentionweight:tensor([3.1528e-08, 2.3923e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:03,273 |	  loss_w (train):7.726625561588207e-09
2021-12-28 15:21:09,308 |	  v_loss (train):128.7924346923828
2021-12-28 15:21:09,884 |	  model_w_in_main test loss : 0.843227
2021-12-28 15:21:09,963 |	  model_v_in_main test loss : 0.931664
2021-12-28 15:21:09,967 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:21:09,969 |	  Step count: 149
2021-12-28 15:21:09,978 |	  attentionweight:tensor([2.3654e-08, 2.3654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:21,882 |	  attentionweight:tensor([2.3654e-08, 2.3654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:21,975 |	  attentionweight:tensor([2.3654e-08, 2.3654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:24,438 |	  attentionweight:tensor([2.3654e-08, 2.3654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:24,624 |	  attentionweight:tensor([2.3654e-08, 2.3654e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:25,018 |	  attentionweight:tensor([2.4796e-08, 2.4953e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:25,079 |	  loss_w (train):1.7376756700215878e-09
2021-12-28 15:21:27,016 |	  v_loss (train):24.082103729248047
2021-12-28 15:21:27,675 |	  model_w_in_main test loss : 0.843206
2021-12-28 15:21:27,733 |	  model_v_in_main test loss : 0.912538
2021-12-28 15:21:27,737 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:21:27,739 |	  Step count: 150
2021-12-28 15:21:27,742 |	  attentionweight:tensor([2.4810e-08, 2.4810e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:48,936 |	  attentionweight:tensor([2.4810e-08, 2.4810e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:49,250 |	  attentionweight:tensor([2.4810e-08, 2.4810e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:55,933 |	  attentionweight:tensor([2.4810e-08, 2.4810e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:56,248 |	  attentionweight:tensor([2.4810e-08, 2.4810e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:56,446 |	  attentionweight:tensor([2.3743e-08, 5.2619e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:21:56,579 |	  loss_w (train):2.1587061027616983e-08
2021-12-28 15:22:02,779 |	  v_loss (train):691.3910522460938
2021-12-28 15:22:03,416 |	  model_w_in_main test loss : 0.843189
2021-12-28 15:22:03,467 |	  model_v_in_main test loss : 0.917841
2021-12-28 15:22:03,471 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:22:03,473 |	  Step count: 151
2021-12-28 15:22:03,476 |	  attentionweight:tensor([2.8983e-08, 2.8983e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:14,756 |	  attentionweight:tensor([2.8983e-08, 2.8983e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:15,045 |	  attentionweight:tensor([2.8983e-08, 2.8983e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:16,944 |	  attentionweight:tensor([2.8983e-08, 2.8983e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:17,021 |	  attentionweight:tensor([2.8983e-08, 2.8983e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:17,391 |	  attentionweight:tensor([3.1861e-08, 3.2464e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:17,496 |	  loss_w (train):1.685015327268502e-08
2021-12-28 15:22:18,980 |	  v_loss (train):43.53717041015625
2021-12-28 15:22:19,566 |	  model_w_in_main test loss : 0.843154
2021-12-28 15:22:19,670 |	  model_v_in_main test loss : 0.908932
2021-12-28 15:22:19,674 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:22:19,676 |	  Step count: 152
2021-12-28 15:22:19,679 |	  attentionweight:tensor([3.1642e-08, 3.1642e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:34,663 |	  attentionweight:tensor([3.1642e-08, 3.1642e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:34,743 |	  attentionweight:tensor([3.1642e-08, 3.1642e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:38,457 |	  attentionweight:tensor([3.1642e-08, 3.1642e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:38,534 |	  attentionweight:tensor([3.1642e-08, 3.1642e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:38,919 |	  attentionweight:tensor([2.9053e-08, 2.8218e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:39,018 |	  loss_w (train):1.0477364753569418e-08
2021-12-28 15:22:41,197 |	  v_loss (train):87.71260833740234
2021-12-28 15:22:41,510 |	  model_w_in_main test loss : 0.843170
2021-12-28 15:22:41,616 |	  model_v_in_main test loss : 0.894736
2021-12-28 15:22:41,619 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:22:41,621 |	  Step count: 153
2021-12-28 15:22:41,623 |	  attentionweight:tensor([3.1260e-08, 3.1260e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:53,235 |	  attentionweight:tensor([3.1260e-08, 3.1260e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:53,330 |	  attentionweight:tensor([3.1260e-08, 3.1260e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:55,714 |	  attentionweight:tensor([3.1260e-08, 3.1260e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:55,810 |	  attentionweight:tensor([3.1260e-08, 3.1260e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:56,045 |	  attentionweight:tensor([3.0516e-08, 3.3162e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:22:56,308 |	  loss_w (train):2.4683448174300793e-08
2021-12-28 15:22:57,755 |	  v_loss (train):24.21639633178711
2021-12-28 15:22:58,352 |	  model_w_in_main test loss : 0.843208
2021-12-28 15:22:58,476 |	  model_v_in_main test loss : 0.902695
2021-12-28 15:22:58,482 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:22:58,484 |	  Step count: 154
2021-12-28 15:22:58,489 |	  attentionweight:tensor([3.1357e-08, 3.1357e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:09,540 |	  attentionweight:tensor([3.1357e-08, 3.1357e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:09,634 |	  attentionweight:tensor([3.1357e-08, 3.1357e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:10,806 |	  attentionweight:tensor([3.1357e-08, 3.1357e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:10,900 |	  attentionweight:tensor([3.1357e-08, 3.1357e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:11,291 |	  attentionweight:tensor([3.1359e-08, 3.1597e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:11,458 |	  loss_w (train):1.3902369533358439e-10
2021-12-28 15:23:13,354 |	  v_loss (train):23.577980041503906
2021-12-28 15:23:13,946 |	  model_w_in_main test loss : 0.843188
2021-12-28 15:23:13,994 |	  model_v_in_main test loss : 0.901399
2021-12-28 15:23:13,998 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:23:14,000 |	  Step count: 155
2021-12-28 15:23:14,003 |	  attentionweight:tensor([3.1434e-08, 3.1434e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:25,805 |	  attentionweight:tensor([3.1434e-08, 3.1434e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:25,940 |	  attentionweight:tensor([3.1434e-08, 3.1434e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:28,694 |	  attentionweight:tensor([3.1434e-08, 3.1434e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:28,779 |	  attentionweight:tensor([3.1434e-08, 3.1434e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:28,972 |	  attentionweight:tensor([3.2006e-08, 3.1746e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:29,099 |	  loss_w (train):5.746993991806448e-10
2021-12-28 15:23:30,948 |	  v_loss (train):37.851661682128906
2021-12-28 15:23:31,491 |	  model_w_in_main test loss : 0.843101
2021-12-28 15:23:31,603 |	  model_v_in_main test loss : 0.906066
2021-12-28 15:23:31,607 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:23:31,610 |	  Step count: 156
2021-12-28 15:23:31,612 |	  attentionweight:tensor([3.1629e-08, 3.1629e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:41,842 |	  attentionweight:tensor([3.1629e-08, 3.1629e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:42,148 |	  attentionweight:tensor([3.1629e-08, 3.1629e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:43,334 |	  attentionweight:tensor([3.1629e-08, 3.1629e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:43,447 |	  attentionweight:tensor([3.1629e-08, 3.1629e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:43,630 |	  attentionweight:tensor([3.2574e-08, 3.2764e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:43,716 |	  loss_w (train):6.628360971205893e-09
2021-12-28 15:23:44,655 |	  v_loss (train):34.192256927490234
2021-12-28 15:23:45,261 |	  model_w_in_main test loss : 0.843229
2021-12-28 15:23:45,314 |	  model_v_in_main test loss : 0.908042
2021-12-28 15:23:45,318 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:23:45,320 |	  Step count: 157
2021-12-28 15:23:45,322 |	  attentionweight:tensor([3.2091e-08, 3.2091e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:56,103 |	  attentionweight:tensor([3.2091e-08, 3.2091e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:56,282 |	  attentionweight:tensor([3.2091e-08, 3.2091e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:58,010 |	  attentionweight:tensor([3.2091e-08, 3.2091e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:58,166 |	  attentionweight:tensor([3.2091e-08, 3.2091e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:58,605 |	  attentionweight:tensor([3.2792e-08, 3.1607e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:23:58,653 |	  loss_w (train):5.027184446504407e-09
2021-12-28 15:24:00,034 |	  v_loss (train):8.767769813537598
2021-12-28 15:24:00,644 |	  model_w_in_main test loss : 0.843240
2021-12-28 15:24:00,741 |	  model_v_in_main test loss : 0.904017
2021-12-28 15:24:00,745 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:24:00,747 |	  Step count: 158
2021-12-28 15:24:00,811 |	  attentionweight:tensor([3.2274e-08, 3.2274e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:16,718 |	  attentionweight:tensor([3.2274e-08, 3.2274e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:17,000 |	  attentionweight:tensor([3.2274e-08, 3.2274e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:20,924 |	  attentionweight:tensor([3.2274e-08, 3.2274e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:21,157 |	  attentionweight:tensor([3.2274e-08, 3.2274e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:21,352 |	  attentionweight:tensor([2.8927e-08, 3.1266e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:21,396 |	  loss_w (train):1.3708113755228624e-08
2021-12-28 15:24:25,181 |	  v_loss (train):96.18209838867188
2021-12-28 15:24:25,762 |	  model_w_in_main test loss : 0.843172
2021-12-28 15:24:25,877 |	  model_v_in_main test loss : 0.904703
2021-12-28 15:24:25,893 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:24:25,895 |	  Step count: 159
2021-12-28 15:24:25,971 |	  attentionweight:tensor([3.1455e-08, 3.1455e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:42,803 |	  attentionweight:tensor([3.1455e-08, 3.1455e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:42,867 |	  attentionweight:tensor([3.1455e-08, 3.1455e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:48,057 |	  attentionweight:tensor([3.1455e-08, 3.1455e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:48,139 |	  attentionweight:tensor([3.1455e-08, 3.1455e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:48,372 |	  attentionweight:tensor([2.9953e-08, 3.6430e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:24:48,616 |	  loss_w (train):1.0679686468506588e-08
2021-12-28 15:24:52,646 |	  v_loss (train):173.66807556152344
2021-12-28 15:24:53,265 |	  model_w_in_main test loss : 0.843183
2021-12-28 15:24:53,317 |	  model_v_in_main test loss : 0.911879
2021-12-28 15:24:53,322 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:24:53,324 |	  Step count: 160
2021-12-28 15:24:53,326 |	  attentionweight:tensor([3.1809e-08, 3.1809e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:06,843 |	  attentionweight:tensor([3.1809e-08, 3.1809e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:06,921 |	  attentionweight:tensor([3.1809e-08, 3.1809e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:09,903 |	  attentionweight:tensor([3.1809e-08, 3.1809e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:10,085 |	  attentionweight:tensor([3.1809e-08, 3.1809e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:10,478 |	  attentionweight:tensor([3.2425e-08, 3.1177e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:10,527 |	  loss_w (train):7.784088040807546e-09
2021-12-28 15:25:12,995 |	  v_loss (train):35.729278564453125
2021-12-28 15:25:13,614 |	  model_w_in_main test loss : 0.843210
2021-12-28 15:25:13,714 |	  model_v_in_main test loss : 0.898334
2021-12-28 15:25:13,721 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:25:13,723 |	  Step count: 161
2021-12-28 15:25:13,728 |	  attentionweight:tensor([3.1913e-08, 3.1913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:25,951 |	  attentionweight:tensor([3.1913e-08, 3.1913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:26,030 |	  attentionweight:tensor([3.1913e-08, 3.1913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:28,433 |	  attentionweight:tensor([3.1913e-08, 3.1913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:28,545 |	  attentionweight:tensor([3.1913e-08, 3.1913e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:28,876 |	  attentionweight:tensor([3.2311e-08, 3.2124e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:28,979 |	  loss_w (train):2.1119728188523368e-10
2021-12-28 15:25:30,872 |	  v_loss (train):31.672115325927734
2021-12-28 15:25:31,500 |	  model_w_in_main test loss : 0.843181
2021-12-28 15:25:31,558 |	  model_v_in_main test loss : 0.896391
2021-12-28 15:25:31,562 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:25:31,564 |	  Step count: 162
2021-12-28 15:25:31,567 |	  attentionweight:tensor([3.2063e-08, 3.2063e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:47,016 |	  attentionweight:tensor([3.2063e-08, 3.2063e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:47,197 |	  attentionweight:tensor([3.2063e-08, 3.2063e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:51,697 |	  attentionweight:tensor([3.2063e-08, 3.2063e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:51,977 |	  attentionweight:tensor([3.2063e-08, 3.2063e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:52,124 |	  attentionweight:tensor([4.0267e-08, 3.4154e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:25:52,273 |	  loss_w (train):4.963980781980126e-09
2021-12-28 15:25:55,694 |	  v_loss (train):69.13455200195312
2021-12-28 15:25:56,228 |	  model_w_in_main test loss : 0.843247
2021-12-28 15:25:56,330 |	  model_v_in_main test loss : 0.898790
2021-12-28 15:25:56,333 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:25:56,335 |	  Step count: 163
2021-12-28 15:25:56,338 |	  attentionweight:tensor([3.3975e-08, 3.3975e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:09,944 |	  attentionweight:tensor([3.3975e-08, 3.3975e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:10,037 |	  attentionweight:tensor([3.3975e-08, 3.3975e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:12,857 |	  attentionweight:tensor([3.3975e-08, 3.3975e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:13,110 |	  attentionweight:tensor([3.3975e-08, 3.3975e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:13,343 |	  attentionweight:tensor([3.4883e-08, 3.4807e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:13,455 |	  loss_w (train):7.163498683127045e-10
2021-12-28 15:26:15,958 |	  v_loss (train):25.895870208740234
2021-12-28 15:26:16,558 |	  model_w_in_main test loss : 0.843212
2021-12-28 15:26:16,605 |	  model_v_in_main test loss : 0.892807
2021-12-28 15:26:16,608 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:26:16,609 |	  Step count: 164
2021-12-28 15:26:16,611 |	  attentionweight:tensor([3.4924e-08, 3.4924e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:34,321 |	  attentionweight:tensor([3.4924e-08, 3.4924e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:34,644 |	  attentionweight:tensor([3.4924e-08, 3.4924e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:37,732 |	  attentionweight:tensor([3.4924e-08, 3.4924e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:37,904 |	  attentionweight:tensor([3.4924e-08, 3.4924e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:38,084 |	  attentionweight:tensor([3.5596e-08, 3.6416e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:38,199 |	  loss_w (train):2.1773940428460037e-09
2021-12-28 15:26:41,099 |	  v_loss (train):52.925498962402344
2021-12-28 15:26:41,737 |	  model_w_in_main test loss : 0.843171
2021-12-28 15:26:41,794 |	  model_v_in_main test loss : 0.891635
2021-12-28 15:26:41,798 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:26:41,800 |	  Step count: 165
2021-12-28 15:26:41,803 |	  attentionweight:tensor([3.5636e-08, 3.5636e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:55,815 |	  attentionweight:tensor([3.5636e-08, 3.5636e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:55,995 |	  attentionweight:tensor([3.5636e-08, 3.5636e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:59,530 |	  attentionweight:tensor([3.5636e-08, 3.5636e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:26:59,714 |	  attentionweight:tensor([3.5636e-08, 3.5636e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:00,131 |	  attentionweight:tensor([3.5090e-08, 3.4462e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:00,183 |	  loss_w (train):5.620971688102827e-09
2021-12-28 15:27:03,196 |	  v_loss (train):62.30569076538086
2021-12-28 15:27:03,826 |	  model_w_in_main test loss : 0.843197
2021-12-28 15:27:03,876 |	  model_v_in_main test loss : 0.897780
2021-12-28 15:27:03,880 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:27:03,881 |	  Step count: 166
2021-12-28 15:27:03,884 |	  attentionweight:tensor([3.5523e-08, 3.5523e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:15,761 |	  attentionweight:tensor([3.5523e-08, 3.5523e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:15,853 |	  attentionweight:tensor([3.5523e-08, 3.5523e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:18,244 |	  attentionweight:tensor([3.5523e-08, 3.5523e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:18,330 |	  attentionweight:tensor([3.5523e-08, 3.5523e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:18,524 |	  attentionweight:tensor([3.5474e-08, 3.5505e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:18,603 |	  loss_w (train):2.7380122613607227e-09
2021-12-28 15:27:20,308 |	  v_loss (train):21.41415786743164
2021-12-28 15:27:20,947 |	  model_w_in_main test loss : 0.843303
2021-12-28 15:27:21,020 |	  model_v_in_main test loss : 0.906150
2021-12-28 15:27:21,024 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:27:21,026 |	  Step count: 167
2021-12-28 15:27:21,036 |	  attentionweight:tensor([3.5476e-08, 3.5476e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:40,387 |	  attentionweight:tensor([3.5476e-08, 3.5476e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:40,513 |	  attentionweight:tensor([3.5476e-08, 3.5476e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:45,343 |	  attentionweight:tensor([3.5476e-08, 3.5476e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:45,490 |	  attentionweight:tensor([3.5476e-08, 3.5476e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:45,805 |	  attentionweight:tensor([2.8854e-08, 3.3436e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:27:45,918 |	  loss_w (train):3.3548435141028676e-09
2021-12-28 15:27:51,104 |	  v_loss (train):119.33479309082031
2021-12-28 15:27:51,741 |	  model_w_in_main test loss : 0.843226
2021-12-28 15:27:51,795 |	  model_v_in_main test loss : 0.906998
2021-12-28 15:27:51,799 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:27:51,801 |	  Step count: 168
2021-12-28 15:27:51,804 |	  attentionweight:tensor([3.3692e-08, 3.3692e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:04,832 |	  attentionweight:tensor([3.3692e-08, 3.3692e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:04,912 |	  attentionweight:tensor([3.3692e-08, 3.3692e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:07,885 |	  attentionweight:tensor([3.3692e-08, 3.3692e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:07,989 |	  attentionweight:tensor([3.3692e-08, 3.3692e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:08,311 |	  attentionweight:tensor([2.8031e-08, 2.8841e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:08,421 |	  loss_w (train):4.424987043449846e-09
2021-12-28 15:28:10,789 |	  v_loss (train):48.04656982421875
2021-12-28 15:28:11,327 |	  model_w_in_main test loss : 0.843325
2021-12-28 15:28:11,425 |	  model_v_in_main test loss : 0.894558
2021-12-28 15:28:11,429 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:28:11,431 |	  Step count: 169
2021-12-28 15:28:11,433 |	  attentionweight:tensor([3.1048e-08, 3.1048e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:22,687 |	  attentionweight:tensor([3.1048e-08, 3.1048e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:22,861 |	  attentionweight:tensor([3.1048e-08, 3.1048e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:24,525 |	  attentionweight:tensor([3.1048e-08, 3.1048e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:24,692 |	  attentionweight:tensor([3.1048e-08, 3.1048e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:25,117 |	  attentionweight:tensor([2.7150e-08, 3.8349e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:25,166 |	  loss_w (train):1.3253772301879962e-08
2021-12-28 15:28:26,507 |	  v_loss (train):38.87269973754883
2021-12-28 15:28:27,064 |	  model_w_in_main test loss : 0.843200
2021-12-28 15:28:27,206 |	  model_v_in_main test loss : 0.899380
2021-12-28 15:28:27,212 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:28:27,214 |	  Step count: 170
2021-12-28 15:28:27,218 |	  attentionweight:tensor([3.0738e-08, 3.0738e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:40,463 |	  attentionweight:tensor([3.0738e-08, 3.0738e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:40,592 |	  attentionweight:tensor([3.0738e-08, 3.0738e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:43,183 |	  attentionweight:tensor([3.0738e-08, 3.0738e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:43,327 |	  attentionweight:tensor([3.0738e-08, 3.0738e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:43,721 |	  attentionweight:tensor([3.0913e-08, 3.2230e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:43,766 |	  loss_w (train):1.2392477322098472e-10
2021-12-28 15:28:45,946 |	  v_loss (train):71.87877655029297
2021-12-28 15:28:46,438 |	  model_w_in_main test loss : 0.843253
2021-12-28 15:28:46,590 |	  model_v_in_main test loss : 0.898036
2021-12-28 15:28:46,593 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:28:46,595 |	  Step count: 171
2021-12-28 15:28:46,598 |	  attentionweight:tensor([3.0963e-08, 3.0963e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:57,369 |	  attentionweight:tensor([3.0963e-08, 3.0963e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:57,512 |	  attentionweight:tensor([3.0963e-08, 3.0963e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:59,315 |	  attentionweight:tensor([3.0963e-08, 3.0963e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:59,392 |	  attentionweight:tensor([3.0963e-08, 3.0963e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:59,769 |	  attentionweight:tensor([3.7935e-08, 3.2475e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:28:59,882 |	  loss_w (train):6.56140297650154e-09
2021-12-28 15:29:01,181 |	  v_loss (train):16.28371238708496
2021-12-28 15:29:01,802 |	  model_w_in_main test loss : 0.843204
2021-12-28 15:29:01,849 |	  model_v_in_main test loss : 0.906603
2021-12-28 15:29:01,852 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:29:01,853 |	  Step count: 172
2021-12-28 15:29:01,856 |	  attentionweight:tensor([3.2586e-08, 3.2586e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:14,699 |	  attentionweight:tensor([3.2586e-08, 3.2586e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:14,794 |	  attentionweight:tensor([3.2586e-08, 3.2586e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:17,604 |	  attentionweight:tensor([3.2586e-08, 3.2586e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:17,695 |	  attentionweight:tensor([3.2586e-08, 3.2586e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:17,913 |	  attentionweight:tensor([3.3617e-08, 3.3685e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:18,175 |	  loss_w (train):1.4112450097414353e-10
2021-12-28 15:29:20,170 |	  v_loss (train):60.43046569824219
2021-12-28 15:29:20,781 |	  model_w_in_main test loss : 0.843235
2021-12-28 15:29:20,902 |	  model_v_in_main test loss : 0.912197
2021-12-28 15:29:20,907 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:29:20,909 |	  Step count: 173
2021-12-28 15:29:20,914 |	  attentionweight:tensor([3.3515e-08, 3.3515e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:34,010 |	  attentionweight:tensor([3.3515e-08, 3.3515e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:34,097 |	  attentionweight:tensor([3.3515e-08, 3.3515e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:36,776 |	  attentionweight:tensor([3.3515e-08, 3.3515e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:36,927 |	  attentionweight:tensor([3.3515e-08, 3.3515e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:37,255 |	  attentionweight:tensor([3.3371e-08, 3.4821e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:37,300 |	  loss_w (train):2.2717049574794146e-08
2021-12-28 15:29:39,699 |	  v_loss (train):49.56903839111328
2021-12-28 15:29:40,336 |	  model_w_in_main test loss : 0.843258
2021-12-28 15:29:40,393 |	  model_v_in_main test loss : 0.907117
2021-12-28 15:29:40,397 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:29:40,399 |	  Step count: 174
2021-12-28 15:29:40,402 |	  attentionweight:tensor([3.4028e-08, 3.4028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:55,282 |	  attentionweight:tensor([3.4028e-08, 3.4028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:29:55,366 |	  attentionweight:tensor([3.4028e-08, 3.4028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:00,018 |	  attentionweight:tensor([3.4028e-08, 3.4028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:00,330 |	  attentionweight:tensor([3.4028e-08, 3.4028e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:00,484 |	  attentionweight:tensor([3.7968e-08, 3.6407e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:00,590 |	  loss_w (train):2.212514615962391e-08
2021-12-28 15:30:03,978 |	  v_loss (train):103.48918914794922
2021-12-28 15:30:04,503 |	  model_w_in_main test loss : 0.843171
2021-12-28 15:30:04,575 |	  model_v_in_main test loss : 0.898964
2021-12-28 15:30:04,579 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:30:04,581 |	  Step count: 175
2021-12-28 15:30:04,583 |	  attentionweight:tensor([3.5375e-08, 3.5375e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:15,383 |	  attentionweight:tensor([3.5375e-08, 3.5375e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:15,474 |	  attentionweight:tensor([3.5375e-08, 3.5375e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:16,728 |	  attentionweight:tensor([3.5375e-08, 3.5375e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:16,881 |	  attentionweight:tensor([3.5375e-08, 3.5375e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:17,265 |	  attentionweight:tensor([3.6017e-08, 3.6030e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:17,319 |	  loss_w (train):4.191766933558938e-10
2021-12-28 15:30:18,184 |	  v_loss (train):3.6543421745300293
2021-12-28 15:30:18,784 |	  model_w_in_main test loss : 0.843177
2021-12-28 15:30:18,890 |	  model_v_in_main test loss : 0.903385
2021-12-28 15:30:18,894 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:30:18,896 |	  Step count: 176
2021-12-28 15:30:18,901 |	  attentionweight:tensor([3.6052e-08, 3.6052e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:31,153 |	  attentionweight:tensor([3.6052e-08, 3.6052e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:31,259 |	  attentionweight:tensor([3.6052e-08, 3.6052e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:33,735 |	  attentionweight:tensor([3.6052e-08, 3.6052e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:33,827 |	  attentionweight:tensor([3.6052e-08, 3.6052e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:34,240 |	  attentionweight:tensor([3.5952e-08, 3.6939e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:34,376 |	  loss_w (train):3.5619462934732837e-09
2021-12-28 15:30:36,332 |	  v_loss (train):44.419647216796875
2021-12-28 15:30:36,959 |	  model_w_in_main test loss : 0.843246
2021-12-28 15:30:37,005 |	  model_v_in_main test loss : 0.909542
2021-12-28 15:30:37,008 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:30:37,010 |	  Step count: 177
2021-12-28 15:30:37,012 |	  attentionweight:tensor([3.6413e-08, 3.6413e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:48,066 |	  attentionweight:tensor([3.6413e-08, 3.6413e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:48,190 |	  attentionweight:tensor([3.6413e-08, 3.6413e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:50,143 |	  attentionweight:tensor([3.6413e-08, 3.6413e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:50,295 |	  attentionweight:tensor([3.6413e-08, 3.6413e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:50,693 |	  attentionweight:tensor([9.5051e-07, 3.1654e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:30:50,738 |	  loss_w (train):2.6358925708791503e-08
2021-12-28 15:30:52,466 |	  v_loss (train):54.41361999511719
2021-12-28 15:30:52,953 |	  model_w_in_main test loss : 0.843213
2021-12-28 15:30:53,133 |	  model_v_in_main test loss : 0.908387
2021-12-28 15:30:53,137 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:30:53,138 |	  Step count: 178
2021-12-28 15:30:53,140 |	  attentionweight:tensor([1.5041e-07, 1.5041e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:03,822 |	  attentionweight:tensor([1.5041e-07, 1.5041e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:03,901 |	  attentionweight:tensor([1.5041e-07, 1.5041e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:04,785 |	  attentionweight:tensor([1.5041e-07, 1.5041e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:04,856 |	  attentionweight:tensor([1.5041e-07, 1.5041e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:05,024 |	  attentionweight:tensor([3.5525e-07, 6.3370e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:05,076 |	  loss_w (train):2.3041307883886475e-07
2021-12-28 15:31:06,190 |	  v_loss (train):22.965965270996094
2021-12-28 15:31:06,691 |	  model_w_in_main test loss : 0.843181
2021-12-28 15:31:06,840 |	  model_v_in_main test loss : 0.907885
2021-12-28 15:31:06,844 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:31:06,846 |	  Step count: 179
2021-12-28 15:31:06,849 |	  attentionweight:tensor([2.8151e-07, 2.8151e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:21,529 |	  attentionweight:tensor([2.8151e-07, 2.8151e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:21,700 |	  attentionweight:tensor([2.8151e-07, 2.8151e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:25,374 |	  attentionweight:tensor([2.8151e-07, 2.8151e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:25,649 |	  attentionweight:tensor([2.8151e-07, 2.8151e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:25,829 |	  attentionweight:tensor([3.2126e-07, 3.1964e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:25,931 |	  loss_w (train):5.93832405471062e-09
2021-12-28 15:31:29,165 |	  v_loss (train):103.29803466796875
2021-12-28 15:31:29,758 |	  model_w_in_main test loss : 0.843142
2021-12-28 15:31:29,806 |	  model_v_in_main test loss : 0.910880
2021-12-28 15:31:29,809 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:31:29,811 |	  Step count: 180
2021-12-28 15:31:29,814 |	  attentionweight:tensor([3.4529e-07, 3.4529e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:41,253 |	  attentionweight:tensor([3.4529e-07, 3.4529e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:41,319 |	  attentionweight:tensor([3.4529e-07, 3.4529e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:42,509 |	  attentionweight:tensor([3.4529e-07, 3.4529e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:42,664 |	  attentionweight:tensor([3.4529e-07, 3.4529e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:43,080 |	  attentionweight:tensor([2.7396e-07, 1.9650e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:43,128 |	  loss_w (train):3.6787294988016583e-08
2021-12-28 15:31:43,905 |	  v_loss (train):6.216277599334717
2021-12-28 15:31:44,468 |	  model_w_in_main test loss : 0.843165
2021-12-28 15:31:44,730 |	  model_v_in_main test loss : 0.906237
2021-12-28 15:31:44,735 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:31:44,737 |	  Step count: 181
2021-12-28 15:31:44,739 |	  attentionweight:tensor([3.4191e-07, 3.4191e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:57,281 |	  attentionweight:tensor([3.4191e-07, 3.4191e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:57,372 |	  attentionweight:tensor([3.4191e-07, 3.4191e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:59,758 |	  attentionweight:tensor([3.4191e-07, 3.4191e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:31:59,824 |	  attentionweight:tensor([3.4191e-07, 3.4191e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:00,192 |	  attentionweight:tensor([1.4896e-07, 4.6032e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:00,291 |	  loss_w (train):8.974994614163734e-08
2021-12-28 15:32:02,750 |	  v_loss (train):24.671199798583984
2021-12-28 15:32:03,359 |	  model_w_in_main test loss : 0.843152
2021-12-28 15:32:03,414 |	  model_v_in_main test loss : 0.906061
2021-12-28 15:32:03,418 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:32:03,420 |	  Step count: 182
2021-12-28 15:32:03,430 |	  attentionweight:tensor([3.2134e-07, 3.2134e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:19,679 |	  attentionweight:tensor([3.2134e-07, 3.2134e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:19,746 |	  attentionweight:tensor([3.2134e-07, 3.2134e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:24,102 |	  attentionweight:tensor([3.2134e-07, 3.2134e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:24,363 |	  attentionweight:tensor([3.2134e-07, 3.2134e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:24,644 |	  attentionweight:tensor([6.1022e-08, 2.5294e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:24,764 |	  loss_w (train):3.36674830236916e-08
2021-12-28 15:32:28,271 |	  v_loss (train):93.47564697265625
2021-12-28 15:32:28,835 |	  model_w_in_main test loss : 0.843178
2021-12-28 15:32:28,984 |	  model_v_in_main test loss : 0.914102
2021-12-28 15:32:28,988 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:32:28,990 |	  Step count: 183
2021-12-28 15:32:28,994 |	  attentionweight:tensor([2.5577e-07, 2.5577e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:47,050 |	  attentionweight:tensor([2.5577e-07, 2.5577e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:47,128 |	  attentionweight:tensor([2.5577e-07, 2.5577e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:52,518 |	  attentionweight:tensor([2.5577e-07, 2.5577e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:52,611 |	  attentionweight:tensor([2.5577e-07, 2.5577e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:53,028 |	  attentionweight:tensor([1.2125e-07, 1.7522e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:32:53,139 |	  loss_w (train):1.0769343639083218e-08
2021-12-28 15:32:57,646 |	  v_loss (train):121.16407012939453
2021-12-28 15:32:58,243 |	  model_w_in_main test loss : 0.843236
2021-12-28 15:32:58,400 |	  model_v_in_main test loss : 0.905841
2021-12-28 15:32:58,404 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:32:58,406 |	  Step count: 184
2021-12-28 15:32:58,410 |	  attentionweight:tensor([2.0595e-07, 2.0595e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:11,256 |	  attentionweight:tensor([2.0595e-07, 2.0595e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:11,348 |	  attentionweight:tensor([2.0595e-07, 2.0595e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:14,314 |	  attentionweight:tensor([2.0595e-07, 2.0595e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:14,460 |	  attentionweight:tensor([2.0595e-07, 2.0595e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:14,864 |	  attentionweight:tensor([1.7072e-07, 2.3578e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:14,908 |	  loss_w (train):9.007205648003946e-09
2021-12-28 15:33:17,258 |	  v_loss (train):81.47311401367188
2021-12-28 15:33:17,861 |	  model_w_in_main test loss : 0.843279
2021-12-28 15:33:17,908 |	  model_v_in_main test loss : 0.900409
2021-12-28 15:33:17,912 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:33:17,913 |	  Step count: 185
2021-12-28 15:33:17,916 |	  attentionweight:tensor([1.8793e-07, 1.8793e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:28,680 |	  attentionweight:tensor([1.8793e-07, 1.8793e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:28,774 |	  attentionweight:tensor([1.8793e-07, 1.8793e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:30,765 |	  attentionweight:tensor([1.8793e-07, 1.8793e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:31,052 |	  attentionweight:tensor([1.8793e-07, 1.8793e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:31,244 |	  attentionweight:tensor([1.7083e-07, 1.7563e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:31,362 |	  loss_w (train):7.74438957407142e-10
2021-12-28 15:33:32,957 |	  v_loss (train):22.501096725463867
2021-12-28 15:33:33,584 |	  model_w_in_main test loss : 0.843221
2021-12-28 15:33:33,643 |	  model_v_in_main test loss : 0.896836
2021-12-28 15:33:33,647 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:33:33,649 |	  Step count: 186
2021-12-28 15:33:33,652 |	  attentionweight:tensor([1.7797e-07, 1.7797e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:43,957 |	  attentionweight:tensor([1.7797e-07, 1.7797e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:44,039 |	  attentionweight:tensor([1.7797e-07, 1.7797e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:45,268 |	  attentionweight:tensor([1.7797e-07, 1.7797e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:45,359 |	  attentionweight:tensor([1.7797e-07, 1.7797e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:45,731 |	  attentionweight:tensor([1.6089e-07, 1.7939e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:45,830 |	  loss_w (train):3.0789784943863197e-08
2021-12-28 15:33:46,664 |	  v_loss (train):4.183403968811035
2021-12-28 15:33:47,216 |	  model_w_in_main test loss : 0.843169
2021-12-28 15:33:47,366 |	  model_v_in_main test loss : 0.906294
2021-12-28 15:33:47,371 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:33:47,373 |	  Step count: 187
2021-12-28 15:33:47,375 |	  attentionweight:tensor([1.7237e-07, 1.7237e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:59,115 |	  attentionweight:tensor([1.7237e-07, 1.7237e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:33:59,195 |	  attentionweight:tensor([1.7237e-07, 1.7237e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:00,472 |	  attentionweight:tensor([1.7237e-07, 1.7237e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:00,638 |	  attentionweight:tensor([1.7237e-07, 1.7237e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:00,933 |	  attentionweight:tensor([2.6492e-07, 3.4243e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:01,049 |	  loss_w (train):3.651977749541402e-06
2021-12-28 15:34:03,784 |	  v_loss (train):11.481486320495605
2021-12-28 15:34:04,425 |	  model_w_in_main test loss : 0.843151
2021-12-28 15:34:04,518 |	  model_v_in_main test loss : 0.912092
2021-12-28 15:34:04,524 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:34:04,526 |	  Step count: 188
2021-12-28 15:34:04,530 |	  attentionweight:tensor([2.5911e-07, 2.5911e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:14,311 |	  attentionweight:tensor([2.5911e-07, 2.5911e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:14,418 |	  attentionweight:tensor([2.5911e-07, 2.5911e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:15,610 |	  attentionweight:tensor([2.5911e-07, 2.5911e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:15,757 |	  attentionweight:tensor([2.5911e-07, 2.5911e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:16,106 |	  attentionweight:tensor([2.3140e-07, 2.2398e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:16,174 |	  loss_w (train):8.117818595110293e-08
2021-12-28 15:34:16,952 |	  v_loss (train):3.2158355712890625
2021-12-28 15:34:17,485 |	  model_w_in_main test loss : 0.843261
2021-12-28 15:34:17,607 |	  model_v_in_main test loss : 0.898424
2021-12-28 15:34:17,611 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:34:17,614 |	  Step count: 189
2021-12-28 15:34:17,619 |	  attentionweight:tensor([2.9394e-07, 2.9394e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:31,499 |	  attentionweight:tensor([2.9394e-07, 2.9394e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:31,579 |	  attentionweight:tensor([2.9394e-07, 2.9394e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:34,681 |	  attentionweight:tensor([2.9394e-07, 2.9394e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:34,763 |	  attentionweight:tensor([2.9394e-07, 2.9394e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:35,178 |	  attentionweight:tensor([1.8708e-07, 8.4963e-09], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:35,302 |	  loss_w (train):4.182603596802892e-09
2021-12-28 15:34:37,570 |	  v_loss (train):87.92745971679688
2021-12-28 15:34:38,169 |	  model_w_in_main test loss : 0.843212
2021-12-28 15:34:38,236 |	  model_v_in_main test loss : 0.907003
2021-12-28 15:34:38,255 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:34:38,257 |	  Step count: 190
2021-12-28 15:34:38,283 |	  attentionweight:tensor([1.9348e-07, 1.9348e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:55,762 |	  attentionweight:tensor([1.9348e-07, 1.9348e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:34:55,855 |	  attentionweight:tensor([1.9348e-07, 1.9348e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:01,407 |	  attentionweight:tensor([1.9348e-07, 1.9348e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:01,575 |	  attentionweight:tensor([1.9348e-07, 1.9348e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:01,804 |	  attentionweight:tensor([1.1178e-06, 1.6220e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:01,870 |	  loss_w (train):1.973305430169603e-08
2021-12-28 15:35:06,203 |	  v_loss (train):92.4212646484375
2021-12-28 15:35:06,884 |	  model_w_in_main test loss : 0.843206
2021-12-28 15:35:07,137 |	  model_v_in_main test loss : 0.905732
2021-12-28 15:35:07,141 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:35:07,143 |	  Step count: 191
2021-12-28 15:35:07,146 |	  attentionweight:tensor([1.9707e-07, 1.9707e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:16,755 |	  attentionweight:tensor([1.9707e-07, 1.9707e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:16,849 |	  attentionweight:tensor([1.9707e-07, 1.9707e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:18,111 |	  attentionweight:tensor([1.9707e-07, 1.9707e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:18,204 |	  attentionweight:tensor([1.9707e-07, 1.9707e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:18,602 |	  attentionweight:tensor([1.9796e-07, 2.0034e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:18,718 |	  loss_w (train):4.8954786890931246e-09
2021-12-28 15:35:19,497 |	  v_loss (train):1.1031372547149658
2021-12-28 15:35:20,149 |	  model_w_in_main test loss : 0.843207
2021-12-28 15:35:20,207 |	  model_v_in_main test loss : 0.903455
2021-12-28 15:35:20,211 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:35:20,215 |	  Step count: 192
2021-12-28 15:35:20,218 |	  attentionweight:tensor([1.9892e-07, 1.9892e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:32,296 |	  attentionweight:tensor([1.9892e-07, 1.9892e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:32,398 |	  attentionweight:tensor([1.9892e-07, 1.9892e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:35,084 |	  attentionweight:tensor([1.9892e-07, 1.9892e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:35,364 |	  attentionweight:tensor([1.9892e-07, 1.9892e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:35,547 |	  attentionweight:tensor([2.0634e-07, 2.1178e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:35,665 |	  loss_w (train):5.06926234322691e-08
2021-12-28 15:35:37,501 |	  v_loss (train):11.657328605651855
2021-12-28 15:35:38,040 |	  model_w_in_main test loss : 0.843161
2021-12-28 15:35:38,134 |	  model_v_in_main test loss : 0.909546
2021-12-28 15:35:38,137 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:35:38,139 |	  Step count: 193
2021-12-28 15:35:38,141 |	  attentionweight:tensor([2.0185e-07, 2.0185e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:47,967 |	  attentionweight:tensor([2.0185e-07, 2.0185e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:48,061 |	  attentionweight:tensor([2.0185e-07, 2.0185e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:49,284 |	  attentionweight:tensor([2.0185e-07, 2.0185e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:49,441 |	  attentionweight:tensor([2.0185e-07, 2.0185e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:49,865 |	  attentionweight:tensor([2.0711e-07, 5.4650e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:35:49,913 |	  loss_w (train):1.0368314917741372e-07
2021-12-28 15:35:51,909 |	  v_loss (train):2.523195266723633
2021-12-28 15:35:52,430 |	  model_w_in_main test loss : 0.843198
2021-12-28 15:35:52,538 |	  model_v_in_main test loss : 0.911931
2021-12-28 15:35:52,541 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:35:52,542 |	  Step count: 194
2021-12-28 15:35:52,544 |	  attentionweight:tensor([2.2725e-07, 2.2725e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:06,647 |	  attentionweight:tensor([2.2725e-07, 2.2725e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:06,819 |	  attentionweight:tensor([2.2725e-07, 2.2725e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:10,254 |	  attentionweight:tensor([2.2725e-07, 2.2725e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:10,373 |	  attentionweight:tensor([2.2725e-07, 2.2725e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:10,700 |	  attentionweight:tensor([2.2964e-07, 2.2463e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:10,804 |	  loss_w (train):1.5161033317667716e-08
2021-12-28 15:36:13,502 |	  v_loss (train):34.30622863769531
2021-12-28 15:36:14,044 |	  model_w_in_main test loss : 0.843232
2021-12-28 15:36:14,116 |	  model_v_in_main test loss : 0.929831
2021-12-28 15:36:14,159 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:36:14,162 |	  Step count: 195
2021-12-28 15:36:14,183 |	  attentionweight:tensor([2.3795e-07, 2.3795e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:25,864 |	  attentionweight:tensor([2.3795e-07, 2.3795e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:25,945 |	  attentionweight:tensor([2.3795e-07, 2.3795e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:28,206 |	  attentionweight:tensor([2.3795e-07, 2.3795e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:28,289 |	  attentionweight:tensor([2.3795e-07, 2.3795e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:28,677 |	  attentionweight:tensor([4.2237e-08, 2.1367e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:28,779 |	  loss_w (train):1.3457560399388058e-08
2021-12-28 15:36:30,513 |	  v_loss (train):59.8642463684082
2021-12-28 15:36:31,106 |	  model_w_in_main test loss : 0.843222
2021-12-28 15:36:31,153 |	  model_v_in_main test loss : 0.921393
2021-12-28 15:36:31,156 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:36:31,158 |	  Step count: 196
2021-12-28 15:36:31,160 |	  attentionweight:tensor([1.9802e-07, 1.9802e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:43,745 |	  attentionweight:tensor([1.9802e-07, 1.9802e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:43,931 |	  attentionweight:tensor([1.9802e-07, 1.9802e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:46,684 |	  attentionweight:tensor([1.9802e-07, 1.9802e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:46,816 |	  attentionweight:tensor([1.9802e-07, 1.9802e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:47,190 |	  attentionweight:tensor([3.4169e-07, 1.9281e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:36:47,243 |	  loss_w (train):1.0416469820029306e-07
2021-12-28 15:36:49,570 |	  v_loss (train):35.2814826965332
2021-12-28 15:36:50,176 |	  model_w_in_main test loss : 0.843209
2021-12-28 15:36:50,334 |	  model_v_in_main test loss : 0.926362
2021-12-28 15:36:50,338 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:36:50,340 |	  Step count: 197
2021-12-28 15:36:50,345 |	  attentionweight:tensor([1.9505e-07, 1.9505e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:03,372 |	  attentionweight:tensor([1.9505e-07, 1.9505e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:03,460 |	  attentionweight:tensor([1.9505e-07, 1.9505e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:05,902 |	  attentionweight:tensor([1.9505e-07, 1.9505e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:06,088 |	  attentionweight:tensor([1.9505e-07, 1.9505e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:06,469 |	  attentionweight:tensor([1.7009e-07, 1.6075e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:06,517 |	  loss_w (train):5.373873013247987e-10
2021-12-28 15:37:08,882 |	  v_loss (train):22.872394561767578
2021-12-28 15:37:09,493 |	  model_w_in_main test loss : 0.843243
2021-12-28 15:37:09,544 |	  model_v_in_main test loss : 0.923094
2021-12-28 15:37:09,548 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:37:09,550 |	  Step count: 198
2021-12-28 15:37:09,552 |	  attentionweight:tensor([1.8694e-07, 1.8694e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:24,791 |	  attentionweight:tensor([1.8694e-07, 1.8694e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:24,909 |	  attentionweight:tensor([1.8694e-07, 1.8694e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:29,053 |	  attentionweight:tensor([1.8694e-07, 1.8694e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:29,129 |	  attentionweight:tensor([1.8694e-07, 1.8694e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:29,308 |	  attentionweight:tensor([1.9577e-07, 1.8864e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:29,421 |	  loss_w (train):8.616318325493921e-09
2021-12-28 15:37:32,874 |	  v_loss (train):45.04125213623047
2021-12-28 15:37:33,472 |	  model_w_in_main test loss : 0.843260
2021-12-28 15:37:33,519 |	  model_v_in_main test loss : 0.920224
2021-12-28 15:37:33,522 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:37:33,524 |	  Step count: 199
2021-12-28 15:37:33,526 |	  attentionweight:tensor([1.8500e-07, 1.8500e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:46,151 |	  attentionweight:tensor([1.8500e-07, 1.8500e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:46,291 |	  attentionweight:tensor([1.8500e-07, 1.8500e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:49,163 |	  attentionweight:tensor([1.8500e-07, 1.8500e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:49,253 |	  attentionweight:tensor([1.8500e-07, 1.8500e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:49,624 |	  attentionweight:tensor([1.6929e-07, 1.8973e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:37:49,725 |	  loss_w (train):5.6023790051540345e-08
2021-12-28 15:37:52,136 |	  v_loss (train):41.911338806152344
2021-12-28 15:37:52,725 |	  model_w_in_main test loss : 0.843184
2021-12-28 15:37:52,773 |	  model_v_in_main test loss : 0.911949
2021-12-28 15:37:52,776 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:37:52,778 |	  Step count: 200
2021-12-28 15:37:52,780 |	  attentionweight:tensor([1.8295e-07, 1.8295e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:04,572 |	  attentionweight:tensor([1.8295e-07, 1.8295e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:04,664 |	  attentionweight:tensor([1.8295e-07, 1.8295e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:07,009 |	  attentionweight:tensor([1.8295e-07, 1.8295e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:07,089 |	  attentionweight:tensor([1.8295e-07, 1.8295e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:07,309 |	  attentionweight:tensor([1.6396e-07, 1.5446e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:07,580 |	  loss_w (train):2.7478325170804396e-10
2021-12-28 15:38:09,366 |	  v_loss (train):25.166263580322266
2021-12-28 15:38:09,764 |	  model_w_in_main test loss : 0.843227
2021-12-28 15:38:10,029 |	  model_v_in_main test loss : 0.904738
2021-12-28 15:38:10,033 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:38:10,035 |	  Step count: 201
2021-12-28 15:38:10,037 |	  attentionweight:tensor([1.7656e-07, 1.7656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:29,588 |	  attentionweight:tensor([1.7656e-07, 1.7656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:29,856 |	  attentionweight:tensor([1.7656e-07, 1.7656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:36,129 |	  attentionweight:tensor([1.7656e-07, 1.7656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:36,207 |	  attentionweight:tensor([1.7656e-07, 1.7656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:36,382 |	  attentionweight:tensor([1.1588e-05, 3.2806e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:36,611 |	  loss_w (train):4.325479494582396e-06
2021-12-28 15:38:41,399 |	  v_loss (train):142.77581787109375
2021-12-28 15:38:41,950 |	  model_w_in_main test loss : 0.843184
2021-12-28 15:38:42,054 |	  model_v_in_main test loss : 0.902229
2021-12-28 15:38:42,057 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:38:42,059 |	  Step count: 202
2021-12-28 15:38:42,062 |	  attentionweight:tensor([3.1483e-07, 3.1483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:58,603 |	  attentionweight:tensor([3.1483e-07, 3.1483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:38:58,694 |	  attentionweight:tensor([3.1483e-07, 3.1483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:02,917 |	  attentionweight:tensor([3.1483e-07, 3.1483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:03,102 |	  attentionweight:tensor([3.1483e-07, 3.1483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:03,492 |	  attentionweight:tensor([5.4786e-08, 1.3477e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:03,541 |	  loss_w (train):5.080215004227284e-08
2021-12-28 15:39:06,986 |	  v_loss (train):102.98907470703125
2021-12-28 15:39:07,585 |	  model_w_in_main test loss : 0.843161
2021-12-28 15:39:07,634 |	  model_v_in_main test loss : 0.908594
2021-12-28 15:39:07,638 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:39:07,639 |	  Step count: 203
2021-12-28 15:39:07,641 |	  attentionweight:tensor([3.0224e-07, 3.0224e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:22,389 |	  attentionweight:tensor([3.0224e-07, 3.0224e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:22,479 |	  attentionweight:tensor([3.0224e-07, 3.0224e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:26,635 |	  attentionweight:tensor([3.0224e-07, 3.0224e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:26,818 |	  attentionweight:tensor([3.0224e-07, 3.0224e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:27,185 |	  attentionweight:tensor([1.8553e-06, 1.3168e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:27,230 |	  loss_w (train):8.309035592901637e-07
2021-12-28 15:39:30,809 |	  v_loss (train):74.4220199584961
2021-12-28 15:39:31,410 |	  model_w_in_main test loss : 0.843202
2021-12-28 15:39:31,462 |	  model_v_in_main test loss : 0.902987
2021-12-28 15:39:31,466 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:39:31,467 |	  Step count: 204
2021-12-28 15:39:31,477 |	  attentionweight:tensor([4.1006e-07, 4.1006e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:45,867 |	  attentionweight:tensor([4.1006e-07, 4.1006e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:45,938 |	  attentionweight:tensor([4.1006e-07, 4.1006e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:49,181 |	  attentionweight:tensor([4.1006e-07, 4.1006e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:49,463 |	  attentionweight:tensor([4.1006e-07, 4.1006e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:49,651 |	  attentionweight:tensor([2.2766e-07, 2.3699e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:39:49,752 |	  loss_w (train):8.485897495802419e-08
2021-12-28 15:39:52,565 |	  v_loss (train):83.73438262939453
2021-12-28 15:39:53,166 |	  model_w_in_main test loss : 0.843192
2021-12-28 15:39:53,218 |	  model_v_in_main test loss : 0.899450
2021-12-28 15:39:53,222 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:39:53,223 |	  Step count: 205
2021-12-28 15:39:53,226 |	  attentionweight:tensor([4.1708e-07, 4.1708e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:05,316 |	  attentionweight:tensor([4.1708e-07, 4.1708e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:05,439 |	  attentionweight:tensor([4.1708e-07, 4.1708e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:07,740 |	  attentionweight:tensor([4.1708e-07, 4.1708e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:07,820 |	  attentionweight:tensor([4.1708e-07, 4.1708e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:08,217 |	  attentionweight:tensor([3.6317e-07, 3.8165e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:08,335 |	  loss_w (train):2.661872144926747e-08
2021-12-28 15:40:10,378 |	  v_loss (train):52.33558654785156
2021-12-28 15:40:10,982 |	  model_w_in_main test loss : 0.843189
2021-12-28 15:40:11,035 |	  model_v_in_main test loss : 0.904868
2021-12-28 15:40:11,039 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:40:11,041 |	  Step count: 206
2021-12-28 15:40:11,043 |	  attentionweight:tensor([4.1135e-07, 4.1135e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:25,732 |	  attentionweight:tensor([4.1135e-07, 4.1135e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:25,853 |	  attentionweight:tensor([4.1135e-07, 4.1135e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:29,691 |	  attentionweight:tensor([4.1135e-07, 4.1135e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:29,770 |	  attentionweight:tensor([4.1135e-07, 4.1135e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:30,171 |	  attentionweight:tensor([4.8708e-07, 8.4405e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:30,222 |	  loss_w (train):7.559387995570432e-06
2021-12-28 15:40:33,412 |	  v_loss (train):29.517822265625
2021-12-28 15:40:33,984 |	  model_w_in_main test loss : 0.843189
2021-12-28 15:40:34,196 |	  model_v_in_main test loss : 0.910730
2021-12-28 15:40:34,203 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:40:34,204 |	  Step count: 207
2021-12-28 15:40:34,245 |	  attentionweight:tensor([5.5562e-07, 5.5562e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:46,763 |	  attentionweight:tensor([5.5562e-07, 5.5562e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:46,856 |	  attentionweight:tensor([5.5562e-07, 5.5562e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:48,531 |	  attentionweight:tensor([5.5562e-07, 5.5562e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:48,604 |	  attentionweight:tensor([5.5562e-07, 5.5562e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:48,787 |	  attentionweight:tensor([6.0342e-07, 6.1701e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:40:48,884 |	  loss_w (train):2.3587714004236204e-09
2021-12-28 15:40:51,013 |	  v_loss (train):15.946331977844238
2021-12-28 15:40:51,620 |	  model_w_in_main test loss : 0.843055
2021-12-28 15:40:51,673 |	  model_v_in_main test loss : 0.899730
2021-12-28 15:40:51,677 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:40:51,679 |	  Step count: 208
2021-12-28 15:40:51,682 |	  attentionweight:tensor([6.3712e-07, 6.3712e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:01,575 |	  attentionweight:tensor([6.3712e-07, 6.3712e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:01,709 |	  attentionweight:tensor([6.3712e-07, 6.3712e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:03,258 |	  attentionweight:tensor([6.3712e-07, 6.3712e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:03,337 |	  attentionweight:tensor([6.3712e-07, 6.3712e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:03,523 |	  attentionweight:tensor([6.4127e-07, 9.8480e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:03,784 |	  loss_w (train):2.1679518624750926e-07
2021-12-28 15:41:04,707 |	  v_loss (train):0.908514142036438
2021-12-28 15:41:05,282 |	  model_w_in_main test loss : 0.843205
2021-12-28 15:41:05,547 |	  model_v_in_main test loss : 0.891772
2021-12-28 15:41:05,551 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:41:05,553 |	  Step count: 209
2021-12-28 15:41:05,558 |	  attentionweight:tensor([6.9858e-07, 6.9858e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:15,964 |	  attentionweight:tensor([6.9858e-07, 6.9858e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:16,045 |	  attentionweight:tensor([6.9858e-07, 6.9858e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:17,771 |	  attentionweight:tensor([6.9858e-07, 6.9858e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:17,958 |	  attentionweight:tensor([6.9858e-07, 6.9858e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:18,335 |	  attentionweight:tensor([5.4973e-06, 5.3450e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:18,404 |	  loss_w (train):2.6982243070960976e-06
2021-12-28 15:41:19,608 |	  v_loss (train):13.867574691772461
2021-12-28 15:41:20,187 |	  model_w_in_main test loss : 0.843169
2021-12-28 15:41:20,238 |	  model_v_in_main test loss : 0.894920
2021-12-28 15:41:20,242 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:41:20,243 |	  Step count: 210
2021-12-28 15:41:20,246 |	  attentionweight:tensor([8.3952e-07, 8.3952e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:31,332 |	  attentionweight:tensor([8.3952e-07, 8.3952e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:31,492 |	  attentionweight:tensor([8.3952e-07, 8.3952e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:33,649 |	  attentionweight:tensor([8.3952e-07, 8.3952e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:33,736 |	  attentionweight:tensor([8.3952e-07, 8.3952e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:33,957 |	  attentionweight:tensor([9.9881e-07, 1.0658e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:34,221 |	  loss_w (train):4.552547849812072e-08
2021-12-28 15:41:35,523 |	  v_loss (train):29.351360321044922
2021-12-28 15:41:36,116 |	  model_w_in_main test loss : 0.843098
2021-12-28 15:41:36,169 |	  model_v_in_main test loss : 0.894272
2021-12-28 15:41:36,173 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:41:36,175 |	  Step count: 211
2021-12-28 15:41:36,177 |	  attentionweight:tensor([9.3378e-07, 9.3378e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:47,153 |	  attentionweight:tensor([9.3378e-07, 9.3378e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:47,231 |	  attentionweight:tensor([9.3378e-07, 9.3378e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:49,050 |	  attentionweight:tensor([9.3378e-07, 9.3378e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:49,142 |	  attentionweight:tensor([9.3378e-07, 9.3378e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:49,554 |	  attentionweight:tensor([6.1926e-07, 9.4656e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:41:49,667 |	  loss_w (train):9.204445206023593e-08
2021-12-28 15:41:52,356 |	  v_loss (train):36.546234130859375
2021-12-28 15:41:52,957 |	  model_w_in_main test loss : 0.843125
2021-12-28 15:41:53,073 |	  model_v_in_main test loss : 0.893942
2021-12-28 15:41:53,078 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:41:53,080 |	  Step count: 212
2021-12-28 15:41:53,087 |	  attentionweight:tensor([9.4954e-07, 9.4954e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:05,883 |	  attentionweight:tensor([9.4954e-07, 9.4954e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:05,966 |	  attentionweight:tensor([9.4954e-07, 9.4954e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:08,483 |	  attentionweight:tensor([9.4954e-07, 9.4954e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:08,585 |	  attentionweight:tensor([9.4954e-07, 9.4954e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:08,939 |	  attentionweight:tensor([9.9470e-07, 1.7937e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:09,047 |	  loss_w (train):7.51338760096587e-08
2021-12-28 15:42:10,948 |	  v_loss (train):28.127670288085938
2021-12-28 15:42:11,571 |	  model_w_in_main test loss : 0.843233
2021-12-28 15:42:11,630 |	  model_v_in_main test loss : 0.879110
2021-12-28 15:42:11,634 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:42:11,636 |	  Step count: 213
2021-12-28 15:42:11,644 |	  attentionweight:tensor([1.0035e-06, 1.0035e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:22,882 |	  attentionweight:tensor([1.0035e-06, 1.0035e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:22,955 |	  attentionweight:tensor([1.0035e-06, 1.0035e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:24,735 |	  attentionweight:tensor([1.0035e-06, 1.0035e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:24,817 |	  attentionweight:tensor([1.0035e-06, 1.0035e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:25,199 |	  attentionweight:tensor([1.0807e-06, 9.1647e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:25,299 |	  loss_w (train):2.6125233532070524e-08
2021-12-28 15:42:26,607 |	  v_loss (train):23.494108200073242
2021-12-28 15:42:27,177 |	  model_w_in_main test loss : 0.843168
2021-12-28 15:42:27,224 |	  model_v_in_main test loss : 0.880904
2021-12-28 15:42:27,227 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:42:27,229 |	  Step count: 214
2021-12-28 15:42:27,231 |	  attentionweight:tensor([1.0263e-06, 1.0263e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:39,704 |	  attentionweight:tensor([1.0263e-06, 1.0263e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:39,832 |	  attentionweight:tensor([1.0263e-06, 1.0263e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:42,206 |	  attentionweight:tensor([1.0263e-06, 1.0263e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:42,483 |	  attentionweight:tensor([1.0263e-06, 1.0263e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:42,632 |	  attentionweight:tensor([9.5617e-07, 2.2120e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:42,780 |	  loss_w (train):3.713736873578455e-07
2021-12-28 15:42:44,649 |	  v_loss (train):24.88282585144043
2021-12-28 15:42:45,159 |	  model_w_in_main test loss : 0.843164
2021-12-28 15:42:45,239 |	  model_v_in_main test loss : 0.877475
2021-12-28 15:42:45,242 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:42:45,244 |	  Step count: 215
2021-12-28 15:42:45,246 |	  attentionweight:tensor([1.0866e-06, 1.0866e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:59,748 |	  attentionweight:tensor([1.0866e-06, 1.0866e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:42:59,844 |	  attentionweight:tensor([1.0866e-06, 1.0866e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:03,408 |	  attentionweight:tensor([1.0866e-06, 1.0866e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:03,504 |	  attentionweight:tensor([1.0866e-06, 1.0866e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:03,831 |	  attentionweight:tensor([1.5724e-06, 4.5491e-04], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:03,928 |	  loss_w (train):7.760702828818467e-06
2021-12-28 15:43:06,594 |	  v_loss (train):76.82865905761719
2021-12-28 15:43:07,103 |	  model_w_in_main test loss : 0.844026
2021-12-28 15:43:07,222 |	  model_v_in_main test loss : 0.893060
2021-12-28 15:43:07,227 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:43:07,229 |	  Step count: 216
2021-12-28 15:43:07,232 |	  attentionweight:tensor([1.9731e-06, 1.9731e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:19,338 |	  attentionweight:tensor([1.9731e-06, 1.9731e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:19,593 |	  attentionweight:tensor([1.9731e-06, 1.9731e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:21,943 |	  attentionweight:tensor([1.9731e-06, 1.9731e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:22,014 |	  attentionweight:tensor([1.9731e-06, 1.9731e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:22,173 |	  attentionweight:tensor([1.5838e-06, 2.1320e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:22,296 |	  loss_w (train):1.6769131860883135e-08
2021-12-28 15:43:24,166 |	  v_loss (train):38.26872634887695
2021-12-28 15:43:24,736 |	  model_w_in_main test loss : 0.844026
2021-12-28 15:43:24,806 |	  model_v_in_main test loss : 0.887472
2021-12-28 15:43:24,810 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4363, -4.4363, -4.4363,  ..., -4.4363, -4.4363, -4.4363],
       device='cuda:0', requires_grad=True))
2021-12-28 15:43:24,812 |	  Step count: 217
2021-12-28 15:43:24,864 |	  attentionweight:tensor([1.9239e-06, 1.9239e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:37,026 |	  attentionweight:tensor([1.9239e-06, 1.9239e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:37,125 |	  attentionweight:tensor([1.9239e-06, 1.9239e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:38,349 |	  attentionweight:tensor([1.9239e-06, 1.9239e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:38,479 |	  attentionweight:tensor([1.9239e-06, 1.9239e-06], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:38,827 |	  attentionweight:tensor([2.2907e-09, 2.3727e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:38,944 |	  loss_w (train):1.2076381672443404e-08
2021-12-28 15:43:40,800 |	  v_loss (train):2.561976671218872
2021-12-28 15:43:41,347 |	  model_w_in_main test loss : 0.844118
2021-12-28 15:43:41,559 |	  model_v_in_main test loss : 0.886934
2021-12-28 15:43:41,564 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:43:41,565 |	  Step count: 218
2021-12-28 15:43:41,584 |	  attentionweight:tensor([9.5572e-07, 9.5572e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:53,456 |	  attentionweight:tensor([9.5572e-07, 9.5572e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:53,548 |	  attentionweight:tensor([9.5572e-07, 9.5572e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:56,111 |	  attentionweight:tensor([9.5572e-07, 9.5572e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:56,411 |	  attentionweight:tensor([9.5572e-07, 9.5572e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:56,566 |	  attentionweight:tensor([6.1516e-07, 1.6721e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:43:56,670 |	  loss_w (train):2.4947891574811365e-07
2021-12-28 15:43:58,949 |	  v_loss (train):49.42739486694336
2021-12-28 15:43:59,549 |	  model_w_in_main test loss : 0.844016
2021-12-28 15:43:59,600 |	  model_v_in_main test loss : 0.895642
2021-12-28 15:43:59,604 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:43:59,605 |	  Step count: 219
2021-12-28 15:43:59,608 |	  attentionweight:tensor([4.7468e-07, 4.7468e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:10,827 |	  attentionweight:tensor([4.7468e-07, 4.7468e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:10,919 |	  attentionweight:tensor([4.7468e-07, 4.7468e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:13,239 |	  attentionweight:tensor([4.7468e-07, 4.7468e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:13,335 |	  attentionweight:tensor([4.7468e-07, 4.7468e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:13,739 |	  attentionweight:tensor([2.9653e-07, 2.9934e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:13,842 |	  loss_w (train):1.0850741460899371e-07
2021-12-28 15:44:15,410 |	  v_loss (train):15.583110809326172
2021-12-28 15:44:15,965 |	  model_w_in_main test loss : 0.844102
2021-12-28 15:44:16,215 |	  model_v_in_main test loss : 0.894019
2021-12-28 15:44:16,225 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:44:16,227 |	  Step count: 220
2021-12-28 15:44:16,230 |	  attentionweight:tensor([3.2058e-07, 3.2058e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:26,427 |	  attentionweight:tensor([3.2058e-07, 3.2058e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:26,529 |	  attentionweight:tensor([3.2058e-07, 3.2058e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:28,231 |	  attentionweight:tensor([3.2058e-07, 3.2058e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:28,322 |	  attentionweight:tensor([3.2058e-07, 3.2058e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:28,542 |	  attentionweight:tensor([2.6611e-07, 2.0391e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:28,806 |	  loss_w (train):2.11853237175319e-08
2021-12-28 15:44:29,804 |	  v_loss (train):9.918322563171387
2021-12-28 15:44:30,358 |	  model_w_in_main test loss : 0.844062
2021-12-28 15:44:30,430 |	  model_v_in_main test loss : 0.890619
2021-12-28 15:44:30,434 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:44:30,435 |	  Step count: 221
2021-12-28 15:44:30,490 |	  attentionweight:tensor([2.5661e-07, 2.5661e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:42,218 |	  attentionweight:tensor([2.5661e-07, 2.5661e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:42,373 |	  attentionweight:tensor([2.5661e-07, 2.5661e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:44,596 |	  attentionweight:tensor([2.5661e-07, 2.5661e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:44,686 |	  attentionweight:tensor([2.5661e-07, 2.5661e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:45,076 |	  attentionweight:tensor([2.0171e-07, 2.4751e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:44:45,181 |	  loss_w (train):5.56658363848328e-09
2021-12-28 15:44:47,038 |	  v_loss (train):14.428153038024902
2021-12-28 15:44:47,643 |	  model_w_in_main test loss : 0.844039
2021-12-28 15:44:47,691 |	  model_v_in_main test loss : 0.896069
2021-12-28 15:44:47,694 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:44:47,696 |	  Step count: 222
2021-12-28 15:44:47,698 |	  attentionweight:tensor([2.2817e-07, 2.2817e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:01,292 |	  attentionweight:tensor([2.2817e-07, 2.2817e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:01,370 |	  attentionweight:tensor([2.2817e-07, 2.2817e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:04,414 |	  attentionweight:tensor([2.2817e-07, 2.2817e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:04,533 |	  attentionweight:tensor([2.2817e-07, 2.2817e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:04,946 |	  attentionweight:tensor([1.9224e-07, 2.4401e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:05,006 |	  loss_w (train):1.9786224214612957e-08
2021-12-28 15:45:07,678 |	  v_loss (train):48.055084228515625
2021-12-28 15:45:08,224 |	  model_w_in_main test loss : 0.844066
2021-12-28 15:45:08,309 |	  model_v_in_main test loss : 0.910000
2021-12-28 15:45:08,312 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:45:08,314 |	  Step count: 223
2021-12-28 15:45:08,317 |	  attentionweight:tensor([2.1534e-07, 2.1534e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:20,000 |	  attentionweight:tensor([2.1534e-07, 2.1534e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:20,079 |	  attentionweight:tensor([2.1534e-07, 2.1534e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:23,746 |	  attentionweight:tensor([2.1534e-07, 2.1534e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:23,840 |	  attentionweight:tensor([2.1534e-07, 2.1534e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:24,222 |	  attentionweight:tensor([3.3715e-08, 1.9227e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:24,328 |	  loss_w (train):8.478959756530458e-09
2021-12-28 15:45:26,265 |	  v_loss (train):26.94965362548828
2021-12-28 15:45:26,876 |	  model_w_in_main test loss : 0.844041
2021-12-28 15:45:26,929 |	  model_v_in_main test loss : 0.916659
2021-12-28 15:45:26,932 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:45:26,934 |	  Step count: 224
2021-12-28 15:45:26,937 |	  attentionweight:tensor([1.7484e-07, 1.7484e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:43,713 |	  attentionweight:tensor([1.7484e-07, 1.7484e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:44,000 |	  attentionweight:tensor([1.7484e-07, 1.7484e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:48,878 |	  attentionweight:tensor([1.7484e-07, 1.7484e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:49,156 |	  attentionweight:tensor([1.7484e-07, 1.7484e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:49,339 |	  attentionweight:tensor([3.6346e-07, 1.6073e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:45:49,465 |	  loss_w (train):1.4995715957866196e-07
2021-12-28 15:45:53,435 |	  v_loss (train):111.32193756103516
2021-12-28 15:45:54,050 |	  model_w_in_main test loss : 0.844130
2021-12-28 15:45:54,143 |	  model_v_in_main test loss : 0.912764
2021-12-28 15:45:54,148 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:45:54,150 |	  Step count: 225
2021-12-28 15:45:54,156 |	  attentionweight:tensor([1.7056e-07, 1.7056e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:04,516 |	  attentionweight:tensor([1.7056e-07, 1.7056e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:04,607 |	  attentionweight:tensor([1.7056e-07, 1.7056e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:05,813 |	  attentionweight:tensor([1.7056e-07, 1.7056e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:05,875 |	  attentionweight:tensor([1.7056e-07, 1.7056e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:06,231 |	  attentionweight:tensor([2.0981e-07, 2.0119e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:06,327 |	  loss_w (train):6.340083302802668e-08
2021-12-28 15:46:07,075 |	  v_loss (train):13.337209701538086
2021-12-28 15:46:07,678 |	  model_w_in_main test loss : 0.844072
2021-12-28 15:46:07,748 |	  model_v_in_main test loss : 0.900086
2021-12-28 15:46:07,756 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:46:07,758 |	  Step count: 226
2021-12-28 15:46:07,762 |	  attentionweight:tensor([1.7482e-07, 1.7482e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:19,629 |	  attentionweight:tensor([1.7482e-07, 1.7482e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:19,807 |	  attentionweight:tensor([1.7482e-07, 1.7482e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:23,463 |	  attentionweight:tensor([1.7482e-07, 1.7482e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:23,754 |	  attentionweight:tensor([1.7482e-07, 1.7482e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:23,923 |	  attentionweight:tensor([1.8424e-07, 1.8296e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:24,061 |	  loss_w (train):2.6512703144021543e-09
2021-12-28 15:46:26,247 |	  v_loss (train):38.19778060913086
2021-12-28 15:46:26,857 |	  model_w_in_main test loss : 0.844024
2021-12-28 15:46:26,910 |	  model_v_in_main test loss : 0.899002
2021-12-28 15:46:26,913 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:46:26,915 |	  Step count: 227
2021-12-28 15:46:26,917 |	  attentionweight:tensor([1.7820e-07, 1.7820e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:42,232 |	  attentionweight:tensor([1.7820e-07, 1.7820e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:42,319 |	  attentionweight:tensor([1.7820e-07, 1.7820e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:46,440 |	  attentionweight:tensor([1.7820e-07, 1.7820e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:46,621 |	  attentionweight:tensor([1.7820e-07, 1.7820e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:47,005 |	  attentionweight:tensor([8.5912e-07, 1.7024e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:46:47,053 |	  loss_w (train):3.8165049431881926e-07
2021-12-28 15:46:50,850 |	  v_loss (train):43.458778381347656
2021-12-28 15:46:51,435 |	  model_w_in_main test loss : 0.843986
2021-12-28 15:46:51,510 |	  model_v_in_main test loss : 0.905347
2021-12-28 15:46:51,514 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:46:51,516 |	  Step count: 228
2021-12-28 15:46:51,518 |	  attentionweight:tensor([2.0741e-07, 2.0741e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:03,260 |	  attentionweight:tensor([2.0741e-07, 2.0741e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:03,353 |	  attentionweight:tensor([2.0741e-07, 2.0741e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:05,156 |	  attentionweight:tensor([2.0741e-07, 2.0741e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:05,334 |	  attentionweight:tensor([2.0741e-07, 2.0741e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:05,720 |	  attentionweight:tensor([1.9199e-07, 2.2007e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:05,768 |	  loss_w (train):1.3451562530519823e-08
2021-12-28 15:47:07,771 |	  v_loss (train):22.966707229614258
2021-12-28 15:47:08,294 |	  model_w_in_main test loss : 0.844061
2021-12-28 15:47:08,433 |	  model_v_in_main test loss : 0.906449
2021-12-28 15:47:08,437 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:47:08,439 |	  Step count: 229
2021-12-28 15:47:08,442 |	  attentionweight:tensor([2.2025e-07, 2.2025e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:23,412 |	  attentionweight:tensor([2.2025e-07, 2.2025e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:23,490 |	  attentionweight:tensor([2.2025e-07, 2.2025e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:27,657 |	  attentionweight:tensor([2.2025e-07, 2.2025e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:27,821 |	  attentionweight:tensor([2.2025e-07, 2.2025e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:28,209 |	  attentionweight:tensor([5.5055e-08, 3.0407e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:28,253 |	  loss_w (train):1.0751882051351913e-08
2021-12-28 15:47:31,973 |	  v_loss (train):51.771644592285156
2021-12-28 15:47:32,548 |	  model_w_in_main test loss : 0.844080
2021-12-28 15:47:32,814 |	  model_v_in_main test loss : 0.910637
2021-12-28 15:47:32,818 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:47:32,820 |	  Step count: 230
2021-12-28 15:47:32,822 |	  attentionweight:tensor([2.0456e-07, 2.0456e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:43,605 |	  attentionweight:tensor([2.0456e-07, 2.0456e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:43,761 |	  attentionweight:tensor([2.0456e-07, 2.0456e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:45,811 |	  attentionweight:tensor([2.0456e-07, 2.0456e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:45,999 |	  attentionweight:tensor([2.0456e-07, 2.0456e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:46,372 |	  attentionweight:tensor([1.9287e-07, 1.7763e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:47:46,420 |	  loss_w (train):1.4406899140340101e-07
2021-12-28 15:47:47,999 |	  v_loss (train):51.23164367675781
2021-12-28 15:47:48,415 |	  model_w_in_main test loss : 0.844158
2021-12-28 15:47:48,669 |	  model_v_in_main test loss : 0.915172
2021-12-28 15:47:48,673 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:47:48,675 |	  Step count: 231
2021-12-28 15:47:48,677 |	  attentionweight:tensor([1.9483e-07, 1.9483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:03,311 |	  attentionweight:tensor([1.9483e-07, 1.9483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:03,473 |	  attentionweight:tensor([1.9483e-07, 1.9483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:07,104 |	  attentionweight:tensor([1.9483e-07, 1.9483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:07,178 |	  attentionweight:tensor([1.9483e-07, 1.9483e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:07,361 |	  attentionweight:tensor([7.4945e-08, 5.2184e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:07,453 |	  loss_w (train):8.115383565154843e-08
2021-12-28 15:48:10,466 |	  v_loss (train):42.39569854736328
2021-12-28 15:48:11,044 |	  model_w_in_main test loss : 0.844110
2021-12-28 15:48:11,132 |	  model_v_in_main test loss : 0.914929
2021-12-28 15:48:11,136 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:48:11,138 |	  Step count: 232
2021-12-28 15:48:11,140 |	  attentionweight:tensor([1.9153e-07, 1.9153e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:22,870 |	  attentionweight:tensor([1.9153e-07, 1.9153e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:23,164 |	  attentionweight:tensor([1.9153e-07, 1.9153e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:25,112 |	  attentionweight:tensor([1.9153e-07, 1.9153e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:25,236 |	  attentionweight:tensor([1.9153e-07, 1.9153e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:25,547 |	  attentionweight:tensor([1.7408e-07, 5.6648e-08], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:25,641 |	  loss_w (train):3.272130300047138e-08
2021-12-28 15:48:27,185 |	  v_loss (train):23.49958610534668
2021-12-28 15:48:27,729 |	  model_w_in_main test loss : 0.844057
2021-12-28 15:48:27,808 |	  model_v_in_main test loss : 0.922990
2021-12-28 15:48:27,812 |	  ('Attention Weights A : ', Parameter containing:
tensor([-4.4362, -4.4362, -4.4362,  ..., -4.4362, -4.4362, -4.4362],
       device='cuda:0', requires_grad=True))
2021-12-28 15:48:27,814 |	  Step count: 233
2021-12-28 15:48:27,817 |	  attentionweight:tensor([1.6824e-07, 1.6824e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:44,863 |	  attentionweight:tensor([1.6824e-07, 1.6824e-07], device='cuda:0', grad_fn=<IndexBackward>)
2021-12-28 15:48:45,022 |	  attentionweight:tensor([1.6824e-07, 1.6824e-07], device='cuda:0', grad_fn=<IndexBackward>)
