{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reload each module run each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\kevin\\\\OneDrive\\\\桌面\\\\paper\\\\mycode\\\\1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T5 import *\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus_euconst (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\opus_euconst\\en-fr\\1.0.0\\d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 10104\n",
      "    })\n",
      "})\n",
      "{'translation': {'en': 'Celex Test  ', 'fr': 'Celex Test  '}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('opus_euconst','en-fr')\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(0)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\opus_euconst\\en-fr\\1.0.0\\d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908\\cache-ed40eb159454adf9.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id, reduction='none')\n",
    "L = len(dataset['train'])\n",
    "L_t = L//4*3\n",
    "L_v = L//8\n",
    "L_test = L//8\n",
    "dataset = dataset.shuffle(seed=seed_)\n",
    "\n",
    "\n",
    "\n",
    "train = dataset['train']['translation'][:L_t]\n",
    "valid = dataset['train']['translation'][L_t:L_t+L_v]\n",
    "test = dataset['train']['translation'][-L_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = 'translate English to French:' + t['en']\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 7578\n",
      "valid len: 1263\n",
      "test len: 1263\n",
      "{'en': 'translate English to French:(a)  ', 'fr': 'a)  '}\n"
     ]
    }
   ],
   "source": [
    "print(\"train len:\",len(train))\n",
    "print(\"valid len:\",len(valid))\n",
    "print(\"test len:\" ,len(test))\n",
    "print(train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: \n",
      "torch.Size([7578, 232]) torch.Size([7578, 232])\n",
      "Target shape: \n",
      "torch.Size([7578, 100]) torch.Size([7578, 100])\n",
      "Input shape: \n",
      "torch.Size([7578, 232]) torch.Size([7578, 232])\n"
     ]
    }
   ],
   "source": [
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), \n",
    "                        batch_size=5, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the attention parameters\n",
    "A = attention_params(len(train) - int(len(train) * ux_ratio))\n",
    "# attention_weights.load_state_dict(torch.load(os.path.join(args.save, 'A.pt')))\n",
    "A = A.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: \n",
      "torch.Size([1263, 407]) torch.Size([1263, 407])\n",
      "Target shape: \n",
      "torch.Size([1263, 100]) torch.Size([1263, 100])\n"
     ]
    }
   ],
   "source": [
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=5, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: \n",
      "torch.Size([1263, 136]) torch.Size([1263, 136])\n",
      "Target shape: \n",
      "torch.Size([1263, 100]) torch.Size([1263, 100])\n"
     ]
    }
   ],
   "source": [
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=5, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from MT_hyperparams import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = T5(criterion=criterion, tokenizer= tokenizer)\n",
    "# model.load_state_dict(torch.load(os.path.join(args.save, 'gpt_weights.pt')))\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.SGD(model_w.parameters(),lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_w  = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v = T5(criterion=criterion, tokenizer= tokenizer)\n",
    "# model.load_state_dict(torch.load(os.path.join(args.save, 'gpt_weights.pt')))\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.SGD(model_v.parameters(),lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_v  = torch.optim.lr_scheduler.CosineAnnealingLR(v_optimizer, float(epochs), eta_min=learning_rate_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  =next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['my name is kevin','it is my name']\n",
    "for index,i in enumerate(x) :\n",
    "    x[index] = 'translate English to French:' + x[index]\n",
    "y= tokenize(x, tokenizer, max_length = summary_length)\n",
    "input = y[0].cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = model_v.generate(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> mon nom est kevin</s>', \"<pad> c'est mon nom</s>\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_dataloader, valid_dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        for t in batch:\n",
    "            print(t.shape,end=' ')\n",
    "        batch_loss_w, batch_loss_v,  batch_count = 0, 0, 0, 0\n",
    "        input_w = Variable(batch[0], requires_grad=False).cuda()\n",
    "        input_w_attn = Variable(batch[1], requires_grad=False).cuda()        \n",
    "        # Number of datapoints\n",
    "        n = input_w.size(0)      \n",
    "        output_w = Variable(batch[2], requires_grad=False).cuda()\n",
    "        output_w_attn = Variable(batch[3], requires_grad=False).cuda()        \n",
    "        input_v = Variable(batch[4], requires_grad=False).cuda()\n",
    "        input_v_attn = Variable(batch[5], requires_grad=False).cuda()        \n",
    "        # attention indices for CTG loss\n",
    "        attn_idx = Variable(batch[6], requires_grad=False).cuda()\n",
    "        \n",
    "        \n",
    "        #####################################################################################\n",
    "        # valid \n",
    "\n",
    "        # valid input_valid, target_valid, valid_attn_classifier\n",
    "        \n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        valid_batch = next(iter(valid_dataloader))#???\n",
    "\n",
    "        valid_input_v      = Variable(valid_batch[0], requires_grad=False).cuda()\n",
    "        valid_input_v_attn = Variable(valid_batch[1], requires_grad=False).cuda()\n",
    "        valid_out_v      = Variable(valid_batch[2], requires_grad=False).cuda()\n",
    "        valid_out_v_attn = Variable(valid_batch[3], requires_grad=False).cuda()\n",
    "\n",
    "\n",
    "        if begin_epoch <= epoch <= stop_epoch:\n",
    "            \n",
    "            architect.step(input_w, input_w_attn, output_w, output_w_attn, input_v, input_v_attn,valid_input_v, valid_input_v_attn, valid_out_v, \n",
    "                valid_out_v_attn, attn_idx, lr_w, lr_v, v_optimizer, w_optimizer)\n",
    "\n",
    "        # end the framework training and just train on the classifier task after the stop epoch\n",
    "        if epoch <=stop_epoch:\n",
    "            ######################################################################\n",
    "            # Update the W model\n",
    "            w_optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            # W\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            \n",
    "            # store the batch loss\n",
    "            batch_loss_w += loss_w.item()\n",
    "\n",
    "            loss_w.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm(w_model.parameters(), grad_clip)\n",
    "            \n",
    "            w_optimizer.step()\n",
    "            \n",
    "            ######################################################################\n",
    "            # Update the V model\n",
    "            v_optimizer.zero_grad()\n",
    "        \n",
    "            # the training loss\n",
    "            # logits, loss_tr = w_model.loss(article_DS, article_DS_attn, summary_DS, summary_DS_attn)\n",
    "\n",
    "            # Loss on augmented dataset\n",
    "            \n",
    "            loss_aug = calc_loss_aug(input_v, input_v_attn, w_model, v_model)\n",
    "        \n",
    "            v_loss =  (loss_aug)\n",
    "            \n",
    "            # store for printing\n",
    "            batch_loss_v += v_loss.item()\n",
    "            \n",
    "            v_loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm(v_model.parameters(), grad_clip)\n",
    "            \n",
    "            # update the classifier model\n",
    "            v_optimizer.step()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(begin_epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
