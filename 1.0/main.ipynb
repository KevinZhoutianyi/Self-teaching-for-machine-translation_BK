{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/28 02:11:30 PM |\t  Reusing dataset opus_euconst (/home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52540bc8b1c24e55a7ca3e32ab5e31b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/28 02:11:30 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 10104\n",
      "    })\n",
      "})\n",
      "12/28 02:11:30 PM |\t  {'translation': {'en': 'CONSIDERING that Article IV-437(2)(e) of the Constitution provides that the Treaty of 16 April 2003 concerning the accessions referred to above shall be repealed;  ', 'fr': \"CONSIDÉRANT que l'article\\xa0IV-437, paragraphe\\xa02, point\\xa0e), de la Constitution prévoit l'abrogation du traité du 16\\xa0avril 2003 relatif aux adhésions visées ci-dessus;  \"}}\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('opus_euconst','en-fr')\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(0)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/28 02:11:32 PM |\t  Loading cached shuffled indices for dataset at /home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908/cache-774986f0005795ce.arrow\n",
      "12/28 02:11:33 PM |\t  train len: 7578\n",
      "12/28 02:11:33 PM |\t  valid len: 1263\n",
      "12/28 02:11:33 PM |\t  test len: 1263\n",
      "12/28 02:11:33 PM |\t  {'en': 'translate English to French:, on the basis of Article\\xa02, and shall report thereon at least once a year.  ', 'fr': \"L'Agence européenne de défense contribue à l'évaluation régulière des contributions des États membres participants en matière de capacités, en particulier des contributions fournies suivant les critères qui seront établis, entre autres, sur la base de l'article\\xa02, et en fait rapport au moins une fois par an.  \"}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id, reduction='none')\n",
    "L = len(dataset['train'])\n",
    "L_t = L//4*3\n",
    "L_v = L//8\n",
    "L_test = L//8\n",
    "dataset = dataset.shuffle(seed=seed_)\n",
    "\n",
    "\n",
    "train = dataset['train']['translation'][:L_t]\n",
    "valid = dataset['train']['translation'][L_t:L_t+L_v]\n",
    "test = dataset['train']['translation'][-L_test:]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = 'translate English to French:' + t['en']\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), \n",
    "                        batch_size=2, pin_memory=True, num_workers=0)\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=2, pin_memory=True, num_workers=0)\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                        batch_size=5, pin_memory=True, num_workers=0)#, sampler=RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(len(train))\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.SGD(model_w.parameters(),w_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_w  = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.SGD(model_v.parameters(),v_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_v  = torch.optim.lr_scheduler.CosineAnnealingLR(v_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> mon nom est kevin</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
       " \"<pad> c'est mon nomci est ma dénomination 321312</s>\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['my name is kevin','it is my nameit is my nameit is my name 321312']\n",
    "for index,i in enumerate(x) :\n",
    "    x[index] = 'translate English to French:' + x[index]\n",
    "y= tokenize(x, tokenizer, max_length = summary_length)\n",
    "input = y[0].cuda()\n",
    "output  = model_v.generate(input)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (3463554573.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_301017/3463554573.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(test_dataloader):\n",
    "    test_dataloaderx = Variable(batch[0], requires_grad=False).cuda()\n",
    "    test_dataloaderx_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "    test_dataloadery = Variable(batch[2], requires_grad=False).cuda()\n",
    "    test_dataloadery_attn = Variable(batch[3], requires_grad=False).cuda()\n",
    "    break\n",
    "def my_test(test_dataloader,model):\n",
    "    ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "    logging.info('%s test loss : %f',model.name,ls)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, train_dataloader, valid_dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        logging.info(\"Step count: %d\",step)\n",
    "        \n",
    "        batch_loss_w, batch_loss_v,  batch_count = 0, 0, 0\n",
    "        input_w = Variable(batch[0], requires_grad=False).cuda()\n",
    "        input_w_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        output_w = Variable(batch[2], requires_grad=False).cuda()\n",
    "        output_w_attn = Variable(batch[3], requires_grad=False).cuda()        \n",
    "        input_v = Variable(batch[4], requires_grad=False).cuda()\n",
    "        input_v_attn = Variable(batch[5], requires_grad=False).cuda()      \n",
    "        attn_idx = Variable(batch[6], requires_grad=False).cuda()\n",
    "        \n",
    "        valid_batch = next(iter(valid_dataloader))\n",
    "        valid_input_v      = Variable(valid_batch[0], requires_grad=False).cuda()\n",
    "        valid_input_v_attn = Variable(valid_batch[1], requires_grad=False).cuda()\n",
    "        valid_out_v      = Variable(valid_batch[2], requires_grad=False).cuda()\n",
    "        valid_out_v_attn = Variable(valid_batch[3], requires_grad=False).cuda()\n",
    "        \n",
    "\n",
    "        if epoch <= stop_epoch:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_v, input_v_attn,valid_input_v, valid_input_v_attn, valid_out_v, \n",
    "                valid_out_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "\n",
    "        if epoch <=stop_epoch:\n",
    "            \n",
    "            w_optimizer.zero_grad()\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            logging.info(f\"loss_w (train):{loss_w}\")\n",
    "            batch_loss_w += loss_w.item()\n",
    "            loss_w.backward()\n",
    "            nn.utils.clip_grad_norm(w_model.parameters(), grad_clip)\n",
    "            w_optimizer.step()\n",
    "\n",
    "\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(input_v, input_v_attn, w_model, v_model)\n",
    "            v_loss =  (loss_aug)\n",
    "            logging.info(f\"v_loss (train):{v_loss}\")\n",
    "            batch_loss_v += v_loss.item()\n",
    "            v_loss.backward()\n",
    "            nn.utils.clip_grad_norm(v_model.parameters(), grad_clip)\n",
    "            v_optimizer.step()     \n",
    "            \n",
    "            my_test(test_dataloader,w_model) \n",
    "            my_test(test_dataloader,v_model)      \n",
    "        if step % 1  == 0:\n",
    "            AA = A\n",
    "            logging.info(str((\"Attention Weights A : \", A.alpha)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/28 02:11:51 PM |\t  Step count: 0\n",
      "12/28 02:11:51 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:20 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:20 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "12/28 02:12:20 PM |\t  attentionweight:tensor([0.0010, 0.0010], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:21 PM |\t  loss_w (train):0.00025195657508447766\n",
      "12/28 02:12:26 PM |\t  v_loss (train):333.8587341308594\n",
      "12/28 02:12:27 PM |\t  model_w_in_main test loss : 0.836469\n",
      "12/28 02:12:27 PM |\t  model_v_in_main test loss : 0.832117\n",
      "12/28 02:12:27 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-0.9999, -0.9999, -0.9999,  ..., -0.9999, -0.9999, -0.9999],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:12:27 PM |\t  Step count: 1\n",
      "12/28 02:12:27 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:38 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:38 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:39 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:40 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-0.9999, -0.9999, -0.9999,  ..., -0.9999, -0.9999, -0.9999],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:12:40 PM |\t  attentionweight:tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:12:40 PM |\t  loss_w (train):0.0004649713810067624\n",
      "12/28 02:12:43 PM |\t  v_loss (train):127.73165893554688\n",
      "12/28 02:12:43 PM |\t  model_w_in_main test loss : 0.836375\n",
      "12/28 02:12:44 PM |\t  model_v_in_main test loss : 0.828571\n",
      "12/28 02:12:44 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.0114, -2.0114, -2.0114,  ..., -2.0114, -2.0114, -2.0114],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:12:44 PM |\t  Step count: 2\n",
      "12/28 02:12:44 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:03 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:04 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:10 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:10 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.0114, -2.0114, -2.0114,  ..., -2.0114, -2.0114, -2.0114],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:13:11 PM |\t  attentionweight:tensor([7.6562e-05, 5.4823e-04], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:11 PM |\t  loss_w (train):9.34405397856608e-06\n",
      "12/28 02:13:16 PM |\t  v_loss (train):331.6329650878906\n",
      "12/28 02:13:17 PM |\t  model_w_in_main test loss : 0.836378\n",
      "12/28 02:13:17 PM |\t  model_v_in_main test loss : 0.825166\n",
      "12/28 02:13:17 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.4790, -2.4790, -2.4790,  ..., -2.4790, -2.4790, -2.4790],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:13:17 PM |\t  Step count: 3\n",
      "12/28 02:13:17 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:27 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:27 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:28 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:28 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.4790, -2.4790, -2.4790,  ..., -2.4790, -2.4790, -2.4790],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:13:29 PM |\t  attentionweight:tensor([5.8997e-05, 5.0310e-04], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:29 PM |\t  loss_w (train):6.117967859609053e-05\n",
      "12/28 02:13:30 PM |\t  v_loss (train):131.08840942382812\n",
      "12/28 02:13:30 PM |\t  model_w_in_main test loss : 0.835889\n",
      "12/28 02:13:30 PM |\t  model_v_in_main test loss : 0.828687\n",
      "12/28 02:13:30 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7877, -2.7877, -2.7877,  ..., -2.7877, -2.7877, -2.7877],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:13:30 PM |\t  Step count: 4\n",
      "12/28 02:13:30 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:41 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:41 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7877, -2.7877, -2.7877,  ..., -2.7877, -2.7877, -2.7877],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:13:42 PM |\t  attentionweight:tensor([4.6750e-04, 5.0327e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:43 PM |\t  loss_w (train):6.5331846599292476e-06\n",
      "12/28 02:13:43 PM |\t  v_loss (train):129.38734436035156\n",
      "12/28 02:13:44 PM |\t  model_w_in_main test loss : 0.836019\n",
      "12/28 02:13:44 PM |\t  model_v_in_main test loss : 0.827725\n",
      "12/28 02:13:44 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.9572, -2.9572, -2.9572,  ..., -2.9572, -2.9572, -2.9572],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:13:44 PM |\t  Step count: 5\n",
      "12/28 02:13:44 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:56 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:57 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:59 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:13:59 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.9572, -2.9572, -2.9572,  ..., -2.9572, -2.9572, -2.9572],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:13:59 PM |\t  attentionweight:tensor([3.8221e-05, 4.5866e-04], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:00 PM |\t  loss_w (train):9.004250750876963e-05\n",
      "12/28 02:14:01 PM |\t  v_loss (train):223.82435607910156\n",
      "12/28 02:14:02 PM |\t  model_w_in_main test loss : 0.836003\n",
      "12/28 02:14:02 PM |\t  model_v_in_main test loss : 0.826271\n",
      "12/28 02:14:02 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.0147, -3.0147, -3.0147,  ..., -3.0147, -3.0147, -3.0147],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:14:02 PM |\t  Step count: 6\n",
      "12/28 02:14:02 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:12 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:12 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.0147, -3.0147, -3.0147,  ..., -3.0147, -3.0147, -3.0147],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:14:14 PM |\t  attentionweight:tensor([4.0404e-05, 5.7914e-04], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:14 PM |\t  loss_w (train):0.00024985458003357053\n",
      "12/28 02:14:15 PM |\t  v_loss (train):127.57524108886719\n",
      "12/28 02:14:15 PM |\t  model_w_in_main test loss : 0.834292\n",
      "12/28 02:14:15 PM |\t  model_v_in_main test loss : 0.820319\n",
      "12/28 02:14:15 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.2303, -3.2303, -3.2303,  ..., -3.2303, -3.2303, -3.2303],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:14:15 PM |\t  Step count: 7\n",
      "12/28 02:14:15 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:26 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:26 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:29 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:29 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.2303, -3.2303, -3.2303,  ..., -3.2303, -3.2303, -3.2303],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:14:29 PM |\t  attentionweight:tensor([3.2874e-05, 3.2900e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:30 PM |\t  loss_w (train):1.1048872465835302e-06\n",
      "12/28 02:14:31 PM |\t  v_loss (train):150.99072265625\n",
      "12/28 02:14:32 PM |\t  model_w_in_main test loss : 0.834295\n",
      "12/28 02:14:32 PM |\t  model_v_in_main test loss : 0.817002\n",
      "12/28 02:14:32 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3394, -3.3394, -3.3394,  ..., -3.3394, -3.3394, -3.3394],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:14:32 PM |\t  Step count: 8\n",
      "12/28 02:14:32 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:43 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:43 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3394, -3.3394, -3.3394,  ..., -3.3394, -3.3394, -3.3394],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:14:43 PM |\t  attentionweight:tensor([0.0006, 0.0006], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:14:44 PM |\t  loss_w (train):0.0001127277864725329\n",
      "12/28 02:14:45 PM |\t  v_loss (train):103.73046112060547\n",
      "12/28 02:14:45 PM |\t  model_w_in_main test loss : 0.834263\n",
      "12/28 02:14:45 PM |\t  model_v_in_main test loss : 0.818861\n",
      "12/28 02:14:45 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.4354, -3.4354, -3.4354,  ..., -3.4354, -3.4354, -3.4354],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:14:45 PM |\t  Step count: 9\n",
      "12/28 02:14:45 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:06 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:06 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:14 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.4354, -3.4354, -3.4354,  ..., -3.4354, -3.4354, -3.4354],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:15:14 PM |\t  attentionweight:tensor([0.0006, 0.0006], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:14 PM |\t  loss_w (train):4.038014594698325e-05\n",
      "12/28 02:15:19 PM |\t  v_loss (train):253.50064086914062\n",
      "12/28 02:15:20 PM |\t  model_w_in_main test loss : 0.834637\n",
      "12/28 02:15:20 PM |\t  model_v_in_main test loss : 0.812867\n",
      "12/28 02:15:20 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.5076, -3.5076, -3.5076,  ..., -3.5076, -3.5076, -3.5076],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:15:20 PM |\t  Step count: 10\n",
      "12/28 02:15:20 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:31 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:31 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:32 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:32 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.5076, -3.5076, -3.5076,  ..., -3.5076, -3.5076, -3.5076],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:15:32 PM |\t  attentionweight:tensor([5.7369e-04, 2.1210e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:32 PM |\t  loss_w (train):7.058461505948799e-06\n",
      "12/28 02:15:34 PM |\t  v_loss (train):123.72813415527344\n",
      "12/28 02:15:34 PM |\t  model_w_in_main test loss : 0.834728\n",
      "12/28 02:15:34 PM |\t  model_v_in_main test loss : 0.812148\n",
      "12/28 02:15:34 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.4284, -3.4284, -3.4284,  ..., -3.4284, -3.4284, -3.4284],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:15:34 PM |\t  Step count: 11\n",
      "12/28 02:15:34 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:50 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:50 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:54 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:55 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.4284, -3.4284, -3.4284,  ..., -3.4284, -3.4284, -3.4284],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:15:55 PM |\t  attentionweight:tensor([2.0251e-05, 2.0284e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:15:55 PM |\t  loss_w (train):2.7585406314756256e-06\n",
      "12/28 02:15:59 PM |\t  v_loss (train):205.99923706054688\n",
      "12/28 02:16:00 PM |\t  model_w_in_main test loss : 0.834745\n",
      "12/28 02:16:00 PM |\t  model_v_in_main test loss : 0.817200\n",
      "12/28 02:16:00 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3738, -3.3738, -3.3738,  ..., -3.3738, -3.3738, -3.3738],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:16:00 PM |\t  Step count: 12\n",
      "12/28 02:16:00 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:15 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:15 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3738, -3.3738, -3.3738,  ..., -3.3738, -3.3738, -3.3738],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:16:16 PM |\t  attentionweight:tensor([3.6655e-04, 2.2173e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:16 PM |\t  loss_w (train):2.0936546206939965e-05\n",
      "12/28 02:16:18 PM |\t  v_loss (train):152.2181854248047\n",
      "12/28 02:16:19 PM |\t  model_w_in_main test loss : 0.834685\n",
      "12/28 02:16:19 PM |\t  model_v_in_main test loss : 0.819349\n",
      "12/28 02:16:19 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3450, -3.3450, -3.3450,  ..., -3.3450, -3.3450, -3.3450],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:16:19 PM |\t  Step count: 13\n",
      "12/28 02:16:19 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:37 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:37 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:43 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:43 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3450, -3.3450, -3.3450,  ..., -3.3450, -3.3450, -3.3450],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:16:43 PM |\t  attentionweight:tensor([0.0008, 0.0008], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:16:44 PM |\t  loss_w (train):0.0006281952955760062\n",
      "12/28 02:16:49 PM |\t  v_loss (train):236.7562255859375\n",
      "12/28 02:16:49 PM |\t  model_w_in_main test loss : 0.833549\n",
      "12/28 02:16:49 PM |\t  model_v_in_main test loss : 0.818700\n",
      "12/28 02:16:49 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3492, -3.3492, -3.3492,  ..., -3.3492, -3.3492, -3.3492],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:16:49 PM |\t  Step count: 14\n",
      "12/28 02:16:49 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:00 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:01 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:02 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:02 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3492, -3.3492, -3.3492,  ..., -3.3492, -3.3492, -3.3492],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:17:03 PM |\t  attentionweight:tensor([1.7378e-05, 2.1926e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:03 PM |\t  loss_w (train):1.7733780168782687e-06\n",
      "12/28 02:17:04 PM |\t  v_loss (train):117.13170623779297\n",
      "12/28 02:17:05 PM |\t  model_w_in_main test loss : 0.833518\n",
      "12/28 02:17:05 PM |\t  model_v_in_main test loss : 0.818943\n",
      "12/28 02:17:05 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3478, -3.3478, -3.3478,  ..., -3.3478, -3.3478, -3.3478],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:17:05 PM |\t  Step count: 15\n",
      "12/28 02:17:05 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:17 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:17 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:18 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:18 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3478, -3.3478, -3.3478,  ..., -3.3478, -3.3478, -3.3478],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:17:19 PM |\t  attentionweight:tensor([8.5149e-04, 1.5833e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:19 PM |\t  loss_w (train):4.501064267969923e-06\n",
      "12/28 02:17:21 PM |\t  v_loss (train):88.42459869384766\n",
      "12/28 02:17:22 PM |\t  model_w_in_main test loss : 0.833550\n",
      "12/28 02:17:22 PM |\t  model_v_in_main test loss : 0.812589\n",
      "12/28 02:17:22 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3232, -3.3232, -3.3232,  ..., -3.3232, -3.3232, -3.3232],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:17:22 PM |\t  Step count: 16\n",
      "12/28 02:17:22 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:33 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:33 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:35 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:35 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3232, -3.3232, -3.3232,  ..., -3.3232, -3.3232, -3.3232],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:17:36 PM |\t  attentionweight:tensor([0.0009, 0.0009], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:36 PM |\t  loss_w (train):0.00024384900461882353\n",
      "12/28 02:17:37 PM |\t  v_loss (train):142.03839111328125\n",
      "12/28 02:17:38 PM |\t  model_w_in_main test loss : 0.834150\n",
      "12/28 02:17:38 PM |\t  model_v_in_main test loss : 0.817943\n",
      "12/28 02:17:38 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3204, -3.3204, -3.3204,  ..., -3.3204, -3.3204, -3.3204],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:17:38 PM |\t  Step count: 17\n",
      "12/28 02:17:38 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:55 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:55 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:59 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:17:59 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3204, -3.3204, -3.3204,  ..., -3.3204, -3.3204, -3.3204],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:18:00 PM |\t  attentionweight:tensor([1.4644e-05, 1.4252e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:00 PM |\t  loss_w (train):5.164769163457095e-07\n",
      "12/28 02:18:03 PM |\t  v_loss (train):254.19142150878906\n",
      "12/28 02:18:04 PM |\t  model_w_in_main test loss : 0.834149\n",
      "12/28 02:18:04 PM |\t  model_v_in_main test loss : 0.812366\n",
      "12/28 02:18:04 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3144, -3.3144, -3.3144,  ..., -3.3144, -3.3144, -3.3144],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:18:04 PM |\t  Step count: 18\n",
      "12/28 02:18:04 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:18 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:18 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:22 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:22 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3144, -3.3144, -3.3144,  ..., -3.3144, -3.3144, -3.3144],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:18:22 PM |\t  attentionweight:tensor([0.0010, 0.0010], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:22 PM |\t  loss_w (train):1.6008392776711844e-05\n",
      "12/28 02:18:25 PM |\t  v_loss (train):159.035888671875\n",
      "12/28 02:18:26 PM |\t  model_w_in_main test loss : 0.834468\n",
      "12/28 02:18:26 PM |\t  model_v_in_main test loss : 0.818547\n",
      "12/28 02:18:26 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.3137, -3.3137, -3.3137,  ..., -3.3137, -3.3137, -3.3137],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:18:26 PM |\t  Step count: 19\n",
      "12/28 02:18:26 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:41 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:41 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:45 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:46 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.3137, -3.3137, -3.3137,  ..., -3.3137, -3.3137, -3.3137],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:18:46 PM |\t  attentionweight:tensor([1.2216e-05, 1.2208e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:18:46 PM |\t  loss_w (train):1.467264951315883e-06\n",
      "12/28 02:18:49 PM |\t  v_loss (train):219.75535583496094\n",
      "12/28 02:18:50 PM |\t  model_w_in_main test loss : 0.834397\n",
      "12/28 02:18:50 PM |\t  model_v_in_main test loss : 0.822497\n",
      "12/28 02:18:50 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.2871, -3.2871, -3.2871,  ..., -3.2871, -3.2871, -3.2871],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:18:50 PM |\t  Step count: 20\n",
      "12/28 02:18:50 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:05 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:05 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:08 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:09 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.2871, -3.2871, -3.2871,  ..., -3.2871, -3.2871, -3.2871],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:19:09 PM |\t  attentionweight:tensor([1.1043e-05, 1.0550e-03], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:09 PM |\t  loss_w (train):1.807342778192833e-05\n",
      "12/28 02:19:12 PM |\t  v_loss (train):183.73048400878906\n",
      "12/28 02:19:12 PM |\t  model_w_in_main test loss : 0.834696\n",
      "12/28 02:19:13 PM |\t  model_v_in_main test loss : 0.827752\n",
      "12/28 02:19:13 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.2242, -3.2242, -3.2242,  ..., -3.2242, -3.2242, -3.2242],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:19:13 PM |\t  Step count: 21\n",
      "12/28 02:19:13 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:24 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:24 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:27 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:27 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.2242, -3.2242, -3.2242,  ..., -3.2242, -3.2242, -3.2242],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:19:28 PM |\t  attentionweight:tensor([4.7090e-04, 1.1711e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:28 PM |\t  loss_w (train):5.334149318514392e-05\n",
      "12/28 02:19:30 PM |\t  v_loss (train):81.77188110351562\n",
      "12/28 02:19:30 PM |\t  model_w_in_main test loss : 0.834376\n",
      "12/28 02:19:30 PM |\t  model_v_in_main test loss : 0.824954\n",
      "12/28 02:19:31 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.1913, -3.1913, -3.1913,  ..., -3.1913, -3.1913, -3.1913],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:19:31 PM |\t  Step count: 22\n",
      "12/28 02:19:31 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:40 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:41 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:42 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.1913, -3.1913, -3.1913,  ..., -3.1913, -3.1913, -3.1913],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:19:42 PM |\t  attentionweight:tensor([1.1802e-03, 1.0059e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:42 PM |\t  loss_w (train):6.152850801299792e-06\n",
      "12/28 02:19:43 PM |\t  v_loss (train):77.35529327392578\n",
      "12/28 02:19:44 PM |\t  model_w_in_main test loss : 0.834671\n",
      "12/28 02:19:44 PM |\t  model_v_in_main test loss : 0.825384\n",
      "12/28 02:19:44 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.1643, -3.1643, -3.1643,  ..., -3.1643, -3.1643, -3.1643],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:19:44 PM |\t  Step count: 23\n",
      "12/28 02:19:44 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:58 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:19:59 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:02 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:02 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.1643, -3.1643, -3.1643,  ..., -3.1643, -3.1643, -3.1643],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:20:03 PM |\t  attentionweight:tensor([9.4660e-06, 1.7023e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:03 PM |\t  loss_w (train):5.9139106269867625e-06\n",
      "12/28 02:20:06 PM |\t  v_loss (train):144.04739379882812\n",
      "12/28 02:20:06 PM |\t  model_w_in_main test loss : 0.834731\n",
      "12/28 02:20:06 PM |\t  model_v_in_main test loss : 0.820209\n",
      "12/28 02:20:06 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.1438, -3.1438, -3.1438,  ..., -3.1438, -3.1438, -3.1438],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:20:06 PM |\t  Step count: 24\n",
      "12/28 02:20:06 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:17 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:17 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:19 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:19 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.1438, -3.1438, -3.1438,  ..., -3.1438, -3.1438, -3.1438],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:20:19 PM |\t  attentionweight:tensor([9.9219e-06, 8.4922e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:19 PM |\t  loss_w (train):1.0341685197090555e-07\n",
      "12/28 02:20:21 PM |\t  v_loss (train):85.82913970947266\n",
      "12/28 02:20:21 PM |\t  model_w_in_main test loss : 0.834807\n",
      "12/28 02:20:21 PM |\t  model_v_in_main test loss : 0.823069\n",
      "12/28 02:20:21 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-3.1180, -3.1180, -3.1180,  ..., -3.1180, -3.1180, -3.1180],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:20:21 PM |\t  Step count: 25\n",
      "12/28 02:20:21 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:36 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:37 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:40 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:40 PM |\t  attentionweight:tensor([0.0001, 0.0001], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-3.1180, -3.1180, -3.1180,  ..., -3.1180, -3.1180, -3.1180],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:20:41 PM |\t  attentionweight:tensor([3.7586e-06, 3.7584e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:41 PM |\t  loss_w (train):2.4660735107318033e-06\n",
      "12/28 02:20:44 PM |\t  v_loss (train):155.37464904785156\n",
      "12/28 02:20:45 PM |\t  model_w_in_main test loss : 0.834759\n",
      "12/28 02:20:45 PM |\t  model_v_in_main test loss : 0.823592\n",
      "12/28 02:20:45 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.9604, -2.9604, -2.9604,  ..., -2.9604, -2.9604, -2.9604],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:20:45 PM |\t  Step count: 26\n",
      "12/28 02:20:45 PM |\t  attentionweight:tensor([5.5439e-05, 5.5439e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:56 PM |\t  attentionweight:tensor([5.5439e-05, 5.5439e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:56 PM |\t  attentionweight:tensor([5.5439e-05, 5.5439e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:58 PM |\t  attentionweight:tensor([5.5439e-05, 5.5439e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:59 PM |\t  attentionweight:tensor([5.5439e-05, 5.5439e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.9604, -2.9604, -2.9604,  ..., -2.9604, -2.9604, -2.9604],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:20:59 PM |\t  attentionweight:tensor([0.0003, 0.0003], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:20:59 PM |\t  loss_w (train):3.054485887332703e-06\n",
      "12/28 02:21:00 PM |\t  v_loss (train):95.86639404296875\n",
      "12/28 02:21:01 PM |\t  model_w_in_main test loss : 0.834879\n",
      "12/28 02:21:01 PM |\t  model_v_in_main test loss : 0.827108\n",
      "12/28 02:21:01 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.8812, -2.8812, -2.8812,  ..., -2.8812, -2.8812, -2.8812],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:21:01 PM |\t  Step count: 27\n",
      "12/28 02:21:01 PM |\t  attentionweight:tensor([2.9701e-05, 2.9701e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:11 PM |\t  attentionweight:tensor([2.9701e-05, 2.9701e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:11 PM |\t  attentionweight:tensor([2.9701e-05, 2.9701e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:12 PM |\t  attentionweight:tensor([2.9701e-05, 2.9701e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:13 PM |\t  attentionweight:tensor([2.9701e-05, 2.9701e-05], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.8812, -2.8812, -2.8812,  ..., -2.8812, -2.8812, -2.8812],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:21:13 PM |\t  attentionweight:tensor([3.9054e-05, 2.0667e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:13 PM |\t  loss_w (train):5.529810209736752e-07\n",
      "12/28 02:21:14 PM |\t  v_loss (train):86.2138671875\n",
      "12/28 02:21:15 PM |\t  model_w_in_main test loss : 0.834848\n",
      "12/28 02:21:15 PM |\t  model_v_in_main test loss : 0.838900\n",
      "12/28 02:21:15 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.8284, -2.8284, -2.8284,  ..., -2.8284, -2.8284, -2.8284],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:21:15 PM |\t  Step count: 28\n",
      "12/28 02:21:15 PM |\t  attentionweight:tensor([3.0158e-06, 3.0158e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:31 PM |\t  attentionweight:tensor([3.0158e-06, 3.0158e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:31 PM |\t  attentionweight:tensor([3.0158e-06, 3.0158e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:36 PM |\t  attentionweight:tensor([3.0158e-06, 3.0158e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:36 PM |\t  attentionweight:tensor([3.0158e-06, 3.0158e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.8284, -2.8284, -2.8284,  ..., -2.8284, -2.8284, -2.8284],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:21:36 PM |\t  attentionweight:tensor([1.5559e-07, 1.0325e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:36 PM |\t  loss_w (train):1.941672778116299e-08\n",
      "12/28 02:21:40 PM |\t  v_loss (train):166.51239013671875\n",
      "12/28 02:21:41 PM |\t  model_w_in_main test loss : 0.834817\n",
      "12/28 02:21:41 PM |\t  model_v_in_main test loss : 0.839411\n",
      "12/28 02:21:41 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.8015, -2.8015, -2.8015,  ..., -2.8015, -2.8015, -2.8015],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:21:41 PM |\t  Step count: 29\n",
      "12/28 02:21:41 PM |\t  attentionweight:tensor([7.9684e-07, 7.9684e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:51 PM |\t  attentionweight:tensor([7.9684e-07, 7.9684e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:51 PM |\t  attentionweight:tensor([7.9684e-07, 7.9684e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:52 PM |\t  attentionweight:tensor([7.9684e-07, 7.9684e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:52 PM |\t  attentionweight:tensor([7.9684e-07, 7.9684e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.8015, -2.8015, -2.8015,  ..., -2.8015, -2.8015, -2.8015],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:21:53 PM |\t  attentionweight:tensor([4.9484e-07, 2.5954e-08], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:21:53 PM |\t  loss_w (train):7.566184478946525e-08\n",
      "12/28 02:21:54 PM |\t  v_loss (train):77.20767974853516\n",
      "12/28 02:21:54 PM |\t  model_w_in_main test loss : 0.834799\n",
      "12/28 02:21:54 PM |\t  model_v_in_main test loss : 0.850523\n",
      "12/28 02:21:54 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7878, -2.7878, -2.7878,  ..., -2.7878, -2.7878, -2.7878],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:21:54 PM |\t  Step count: 30\n",
      "12/28 02:21:54 PM |\t  attentionweight:tensor([3.6883e-07, 3.6883e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:07 PM |\t  attentionweight:tensor([3.6883e-07, 3.6883e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:08 PM |\t  attentionweight:tensor([3.6883e-07, 3.6883e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:10 PM |\t  attentionweight:tensor([3.6883e-07, 3.6883e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:10 PM |\t  attentionweight:tensor([3.6883e-07, 3.6883e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7878, -2.7878, -2.7878,  ..., -2.7878, -2.7878, -2.7878],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:22:11 PM |\t  attentionweight:tensor([2.0033e-07, 4.3395e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:11 PM |\t  loss_w (train):3.2858270770930176e-08\n",
      "12/28 02:22:13 PM |\t  v_loss (train):176.02911376953125\n",
      "12/28 02:22:14 PM |\t  model_w_in_main test loss : 0.834827\n",
      "12/28 02:22:14 PM |\t  model_v_in_main test loss : 0.852005\n",
      "12/28 02:22:14 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7808, -2.7808, -2.7808,  ..., -2.7808, -2.7808, -2.7808],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:22:14 PM |\t  Step count: 31\n",
      "12/28 02:22:14 PM |\t  attentionweight:tensor([2.4973e-07, 2.4973e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:23 PM |\t  attentionweight:tensor([2.4973e-07, 2.4973e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:24 PM |\t  attentionweight:tensor([2.4973e-07, 2.4973e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:26 PM |\t  attentionweight:tensor([2.4973e-07, 2.4973e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:26 PM |\t  attentionweight:tensor([2.4973e-07, 2.4973e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7808, -2.7808, -2.7808,  ..., -2.7808, -2.7808, -2.7808],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:22:26 PM |\t  attentionweight:tensor([2.3423e-07, 2.0365e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:27 PM |\t  loss_w (train):4.6421121169260005e-08\n",
      "12/28 02:22:29 PM |\t  v_loss (train):89.32952880859375\n",
      "12/28 02:22:29 PM |\t  model_w_in_main test loss : 0.834762\n",
      "12/28 02:22:29 PM |\t  model_v_in_main test loss : 0.839566\n",
      "12/28 02:22:29 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7773, -2.7773, -2.7773,  ..., -2.7773, -2.7773, -2.7773],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:22:29 PM |\t  Step count: 32\n",
      "12/28 02:22:29 PM |\t  attentionweight:tensor([2.0502e-07, 2.0502e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:44 PM |\t  attentionweight:tensor([2.0502e-07, 2.0502e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:44 PM |\t  attentionweight:tensor([2.0502e-07, 2.0502e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:48 PM |\t  attentionweight:tensor([2.0502e-07, 2.0502e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:48 PM |\t  attentionweight:tensor([2.0502e-07, 2.0502e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7773, -2.7773, -2.7773,  ..., -2.7773, -2.7773, -2.7773],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:22:48 PM |\t  attentionweight:tensor([2.4250e-07, 1.7793e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:22:48 PM |\t  loss_w (train):3.712448659598522e-08\n",
      "12/28 02:22:51 PM |\t  v_loss (train):174.12521362304688\n",
      "12/28 02:22:52 PM |\t  model_w_in_main test loss : 0.834768\n",
      "12/28 02:22:52 PM |\t  model_v_in_main test loss : 0.839182\n",
      "12/28 02:22:52 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7755, -2.7755, -2.7755,  ..., -2.7755, -2.7755, -2.7755],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:22:52 PM |\t  Step count: 33\n",
      "12/28 02:22:52 PM |\t  attentionweight:tensor([1.8579e-07, 1.8579e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:05 PM |\t  attentionweight:tensor([1.8579e-07, 1.8579e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:05 PM |\t  attentionweight:tensor([1.8579e-07, 1.8579e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:07 PM |\t  attentionweight:tensor([1.8579e-07, 1.8579e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:07 PM |\t  attentionweight:tensor([1.8579e-07, 1.8579e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7755, -2.7755, -2.7755,  ..., -2.7755, -2.7755, -2.7755],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:23:07 PM |\t  attentionweight:tensor([4.4492e-08, 1.5162e-06], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:07 PM |\t  loss_w (train):5.484061489369196e-07\n",
      "12/28 02:23:09 PM |\t  v_loss (train):183.27032470703125\n",
      "12/28 02:23:10 PM |\t  model_w_in_main test loss : 0.834821\n",
      "12/28 02:23:10 PM |\t  model_v_in_main test loss : 0.823545\n",
      "12/28 02:23:10 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7745, -2.7745, -2.7745,  ..., -2.7745, -2.7745, -2.7745],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:23:10 PM |\t  Step count: 34\n",
      "12/28 02:23:10 PM |\t  attentionweight:tensor([1.7897e-07, 1.7897e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:20 PM |\t  attentionweight:tensor([1.7897e-07, 1.7897e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:20 PM |\t  attentionweight:tensor([1.7897e-07, 1.7897e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:21 PM |\t  attentionweight:tensor([1.7897e-07, 1.7897e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:21 PM |\t  attentionweight:tensor([1.7897e-07, 1.7897e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7745, -2.7745, -2.7745,  ..., -2.7745, -2.7745, -2.7745],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:23:22 PM |\t  attentionweight:tensor([2.4134e-08, 9.7236e-08], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:22 PM |\t  loss_w (train):3.495281575283116e-08\n",
      "12/28 02:23:23 PM |\t  v_loss (train):121.13042449951172\n",
      "12/28 02:23:23 PM |\t  model_w_in_main test loss : 0.834876\n",
      "12/28 02:23:24 PM |\t  model_v_in_main test loss : 0.834008\n",
      "12/28 02:23:24 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7741, -2.7741, -2.7741,  ..., -2.7741, -2.7741, -2.7741],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:23:24 PM |\t  Step count: 35\n",
      "12/28 02:23:24 PM |\t  attentionweight:tensor([1.7119e-07, 1.7119e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:40 PM |\t  attentionweight:tensor([1.7119e-07, 1.7119e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:40 PM |\t  attentionweight:tensor([1.7119e-07, 1.7119e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:45 PM |\t  attentionweight:tensor([1.7119e-07, 1.7119e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:45 PM |\t  attentionweight:tensor([1.7119e-07, 1.7119e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7741, -2.7741, -2.7741,  ..., -2.7741, -2.7741, -2.7741],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:23:45 PM |\t  attentionweight:tensor([1.5117e-07, 1.5654e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:45 PM |\t  loss_w (train):5.572045047586016e-10\n",
      "12/28 02:23:49 PM |\t  v_loss (train):126.62290954589844\n",
      "12/28 02:23:49 PM |\t  model_w_in_main test loss : 0.834799\n",
      "12/28 02:23:49 PM |\t  model_v_in_main test loss : 0.828292\n",
      "12/28 02:23:49 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7739, -2.7739, -2.7739,  ..., -2.7739, -2.7739, -2.7739],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:23:49 PM |\t  Step count: 36\n",
      "12/28 02:23:49 PM |\t  attentionweight:tensor([1.6715e-07, 1.6715e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:23:59 PM |\t  attentionweight:tensor([1.6715e-07, 1.6715e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:00 PM |\t  attentionweight:tensor([1.6715e-07, 1.6715e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:01 PM |\t  attentionweight:tensor([1.6715e-07, 1.6715e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:01 PM |\t  attentionweight:tensor([1.6715e-07, 1.6715e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7739, -2.7739, -2.7739,  ..., -2.7739, -2.7739, -2.7739],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:24:01 PM |\t  attentionweight:tensor([5.2390e-08, 1.6721e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:01 PM |\t  loss_w (train):1.6535679492335476e-08\n",
      "12/28 02:24:02 PM |\t  v_loss (train):140.16993713378906\n",
      "12/28 02:24:03 PM |\t  model_w_in_main test loss : 0.834673\n",
      "12/28 02:24:03 PM |\t  model_v_in_main test loss : 0.826394\n",
      "12/28 02:24:03 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7737, -2.7737, -2.7737,  ..., -2.7737, -2.7737, -2.7737],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:24:03 PM |\t  Step count: 37\n",
      "12/28 02:24:03 PM |\t  attentionweight:tensor([1.6357e-07, 1.6357e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:19 PM |\t  attentionweight:tensor([1.6357e-07, 1.6357e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:19 PM |\t  attentionweight:tensor([1.6357e-07, 1.6357e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:23 PM |\t  attentionweight:tensor([1.6357e-07, 1.6357e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:23 PM |\t  attentionweight:tensor([1.6357e-07, 1.6357e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7737, -2.7737, -2.7737,  ..., -2.7737, -2.7737, -2.7737],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:24:24 PM |\t  attentionweight:tensor([1.9552e-07, 1.9440e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:24 PM |\t  loss_w (train):2.2668114496582348e-08\n",
      "12/28 02:24:28 PM |\t  v_loss (train):152.0687255859375\n",
      "12/28 02:24:28 PM |\t  model_w_in_main test loss : 0.834820\n",
      "12/28 02:24:28 PM |\t  model_v_in_main test loss : 0.817447\n",
      "12/28 02:24:28 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7737, -2.7737, -2.7737,  ..., -2.7737, -2.7737, -2.7737],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:24:28 PM |\t  Step count: 38\n",
      "12/28 02:24:28 PM |\t  attentionweight:tensor([1.6226e-07, 1.6226e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:40 PM |\t  attentionweight:tensor([1.6226e-07, 1.6226e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:40 PM |\t  attentionweight:tensor([1.6226e-07, 1.6226e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:42 PM |\t  attentionweight:tensor([1.6226e-07, 1.6226e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:42 PM |\t  attentionweight:tensor([1.6226e-07, 1.6226e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7737, -2.7737, -2.7737,  ..., -2.7737, -2.7737, -2.7737],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:24:42 PM |\t  attentionweight:tensor([1.6426e-07, 1.5179e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:24:43 PM |\t  loss_w (train):5.4676924321483966e-08\n",
      "12/28 02:24:44 PM |\t  v_loss (train):83.51922607421875\n",
      "12/28 02:24:45 PM |\t  model_w_in_main test loss : 0.834778\n",
      "12/28 02:24:45 PM |\t  model_v_in_main test loss : 0.818858\n",
      "12/28 02:24:45 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:24:45 PM |\t  Step count: 39\n",
      "12/28 02:24:45 PM |\t  attentionweight:tensor([1.6154e-07, 1.6154e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:04 PM |\t  attentionweight:tensor([1.6154e-07, 1.6154e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:04 PM |\t  attentionweight:tensor([1.6154e-07, 1.6154e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:08 PM |\t  attentionweight:tensor([1.6154e-07, 1.6154e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:08 PM |\t  attentionweight:tensor([1.6154e-07, 1.6154e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:25:08 PM |\t  attentionweight:tensor([1.6009e-07, 1.7261e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:09 PM |\t  loss_w (train):3.064646136863303e-08\n",
      "12/28 02:25:13 PM |\t  v_loss (train):240.16064453125\n",
      "12/28 02:25:13 PM |\t  model_w_in_main test loss : 0.834767\n",
      "12/28 02:25:13 PM |\t  model_v_in_main test loss : 0.817410\n",
      "12/28 02:25:13 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:25:13 PM |\t  Step count: 40\n",
      "12/28 02:25:13 PM |\t  attentionweight:tensor([1.6125e-07, 1.6125e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:24 PM |\t  attentionweight:tensor([1.6125e-07, 1.6125e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:25 PM |\t  attentionweight:tensor([1.6125e-07, 1.6125e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:26 PM |\t  attentionweight:tensor([1.6125e-07, 1.6125e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:26 PM |\t  attentionweight:tensor([1.6125e-07, 1.6125e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:25:27 PM |\t  attentionweight:tensor([3.8994e-06, 3.1444e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:27 PM |\t  loss_w (train):3.553166607161984e-06\n",
      "12/28 02:25:29 PM |\t  v_loss (train):89.33331298828125\n",
      "12/28 02:25:30 PM |\t  model_w_in_main test loss : 0.834874\n",
      "12/28 02:25:30 PM |\t  model_v_in_main test loss : 0.820994\n",
      "12/28 02:25:30 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:25:30 PM |\t  Step count: 41\n",
      "12/28 02:25:30 PM |\t  attentionweight:tensor([1.7935e-07, 1.7935e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:47 PM |\t  attentionweight:tensor([1.7935e-07, 1.7935e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:47 PM |\t  attentionweight:tensor([1.7935e-07, 1.7935e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:52 PM |\t  attentionweight:tensor([1.7935e-07, 1.7935e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:52 PM |\t  attentionweight:tensor([1.7935e-07, 1.7935e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "12/28 02:25:53 PM |\t  attentionweight:tensor([5.0299e-08, 4.5923e-07], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "12/28 02:25:53 PM |\t  loss_w (train):6.9935211399752e-08\n",
      "12/28 02:25:56 PM |\t  v_loss (train):192.78182983398438\n",
      "12/28 02:25:57 PM |\t  model_w_in_main test loss : 0.834894\n",
      "12/28 02:25:57 PM |\t  model_v_in_main test loss : 0.818362\n",
      "12/28 02:25:57 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([-2.7736, -2.7736, -2.7736,  ..., -2.7736, -2.7736, -2.7736],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "12/28 02:25:57 PM |\t  Step count: 42\n",
      "12/28 02:25:57 PM |\t  attentionweight:tensor([1.8849e-07, 1.8849e-07], device='cuda:0', grad_fn=<IndexBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_301017/143646209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_301017/529434501.py\u001b[0m in \u001b[0;36mmy_train\u001b[0;34m(epoch, train_dataloader, valid_dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstop_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_v, input_v_attn,valid_input_v, valid_input_v_attn, valid_out_v, \n\u001b[0m\u001b[1;32m     23\u001b[0m                 valid_out_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tianyi/Self-teaching-for-machine-translation/1.0/architect.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, w_input, w_target, w_input_attn, w_target_attn, w_optimizer, v_input, v_input_attn, valid_input_v, valid_input_v_attn, valid_out_v, valid_out_v_attn, v_optimizer, attn_idx, eta_w, eta_v)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mimplicit_grads_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outer_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_s_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_input_attn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mw_target_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_input_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrolled_w_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tianyi/Self-teaching-for-machine-translation/1.0/architect.py\u001b[0m in \u001b[0;36m_outer_A\u001b[0;34m(self, vector_s_dash, w_input, w_target, w_input_attn, w_target_attn, input_v, input_v_attn, attn_idx, unrolled_w_model, eta_w, eta_v, r)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0munrolled_w_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mloss_aug_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_v_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrolled_w_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tianyi/Self-teaching-for-machine-translation/1.0/losses.py\u001b[0m in \u001b[0;36mcalc_loss_aug\u001b[0;34m(input_ids, input_attn, w_model, v_model)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# logging.info(f\"{input_ids}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# logging.info(f\"{output_ids}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tianyi/Self-teaching-for-machine-translation/1.0/T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# print(\"start of : generate\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m## sampling with top_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m             )\n\u001b[1;32m   1169\u001b[0m             \u001b[0;31m# 12. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1171\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1908\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 )\n\u001b[1;32m   1008\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1010\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    645\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    549\u001b[0m     ):\n\u001b[1;32m    550\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0mposition_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;31m# if key and values are already calculated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mcompute_bias\u001b[0;34m(self, query_length, key_length)\u001b[0m\n\u001b[1;32m    406\u001b[0m         )[None, :]\n\u001b[1;32m    407\u001b[0m         \u001b[0mrelative_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_position\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext_position\u001b[0m  \u001b[0;31m# shape (query_length, key_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         relative_position_bucket = self._relative_position_bucket(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mrelative_position\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# shape (query_length, key_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tianyi/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m_relative_position_bucket\u001b[0;34m(relative_position, bidirectional, num_buckets, max_distance)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;31m# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         relative_postion_if_large = max_exact + (\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_exact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_distance\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_exact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_buckets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_exact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_train(begin_epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, w_lr,v_lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,  1.7181,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681,  1.7170, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.8334,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -0.8313, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681,  0.4324, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681,  0.4373, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681, -2.2681,\n",
       "        -2.2681, -2.2681], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=10_000)\n",
    "A.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,  6206,  6667,    27,     1])\n",
    "tokenizer.decode([13959,  1566,    12,  2379,    10, 17608,   994,    27,     1,     0])\n",
    "logging.info(\"vocab size : %d\",model_v.vocab_size)\n",
    "logit = torch.load('logits.pt')\n",
    "target = torch.load('target_ids.pt')\n",
    "tokenizer.decode(target[0])\n",
    "logit.shape\n",
    "_,maxx = torch.max(logit,dim=-1,keepdim=True)\n",
    "maxx.shape\n",
    "tokenizer.decode(maxx[0].squeeze(-1))\n",
    "\n",
    "model_v.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate English to French:The criterion on participation in the exchange-rate mechanism of the European Monetary System referred to in Article III-198(1)(c) of the Constitution shall mean that the Member State concerned has respected the normal fluctuation margins provided for by the exchange-rate mechanism of the European Monetary System without severe tensions for at least the last two years before the examination.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([13959,  1566,    12,  2379,    10,   634,     3,  2685, 10140,    29,\n",
    "            30,  5178,    16,     8,  2509,    18,  2206,  8557,    13,     8,\n",
    "          1611,  1290,  1582,  1208,  2149,     3,  4822,    12,    16,  7491,\n",
    "          6289,    18, 24151, 14296,   599,    75,    61,    13,     8, 11378,\n",
    "          1522,  1243,    24,     8,  8541,  1015,  4376,    65, 13841,     8,\n",
    "          1389, 23460,   257,  6346,     7,   937,    21,    57,     8,  2509,\n",
    "            18,  2206,  8557,    13,     8,  1611,  1290,  1582,  1208,  2149,\n",
    "           406,  5274,  7012,     7,    21,    44,   709,     8,   336,   192,\n",
    "           203,   274,     8,  6498,     5,     1,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0])\n",
    "tokenizer.decode([    0,   312,     3, 12563,  2970,    20,  5178,   185,     3, 28874,\n",
    "            20,   483,   146,  4870,  1911,   154,  5785, 18779,  4642,   154,\n",
    "             3,    85,     3,    40,    31,  8372,  6289,    18, 24151,     6,\n",
    "          8986,    15,  1914,   500,     3,    75,   201,    20,    50, 11378,\n",
    "             6, 16558,   238,    90,  3277, 13924,  2410,   154,  1445,    15,\n",
    "           110,  3157,  2897,  1389,    15,     7,    20, 23460,   257,     3,\n",
    "         25796,     7,   260,    90, 17100,    20,  1112,   146,  5224,     7,\n",
    "            17,  5843,  2963,   154,  2046,  6274,  2430,  3890,    35,  1532,\n",
    "          7012,     7,     3,     7,   154,   208, 12449,  4530,   185,  3000,\n",
    "           110,  1763,  6062,     7,  5228,  2811,    75,   154,    26,   288,\n",
    "            90, 10418,     5,     1])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
