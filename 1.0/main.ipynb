{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/25 05:01:43 PM |\t  Reusing dataset opus_euconst (/home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d9c26ae1a74f14b0988510458e6a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/25 05:01:44 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 10104\n",
      "    })\n",
      "})\n",
      "12/25 05:01:44 PM |\t  {'translation': {'en': 'CONSIDERING that Article IV-437(2)(e) of the Constitution provides that the Treaty of 16 April 2003 concerning the accessions referred to above shall be repealed;  ', 'fr': \"CONSIDÉRANT que l'article\\xa0IV-437, paragraphe\\xa02, point\\xa0e), de la Constitution prévoit l'abrogation du traité du 16\\xa0avril 2003 relatif aux adhésions visées ci-dessus;  \"}}\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('opus_euconst','en-fr')\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(0)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/25 05:01:46 PM |\t  Loading cached shuffled indices for dataset at /home/li/.cache/huggingface/datasets/opus_euconst/en-fr/1.0.0/d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908/cache-774986f0005795ce.arrow\n",
      "12/25 05:01:47 PM |\t  train len: 7578\n",
      "12/25 05:01:47 PM |\t  valid len: 1263\n",
      "12/25 05:01:47 PM |\t  test len: 1263\n",
      "12/25 05:01:47 PM |\t  {'en': 'translate English to French:, on the basis of Article\\xa02, and shall report thereon at least once a year.  ', 'fr': \"L'Agence européenne de défense contribue à l'évaluation régulière des contributions des États membres participants en matière de capacités, en particulier des contributions fournies suivant les critères qui seront établis, entre autres, sur la base de l'article\\xa02, et en fait rapport au moins une fois par an.  \"}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id, reduction='none')\n",
    "L = len(dataset['train'])\n",
    "L_t = L//4*3\n",
    "L_v = L//8\n",
    "L_test = L//8\n",
    "dataset = dataset.shuffle(seed=seed_)\n",
    "\n",
    "\n",
    "train = dataset['train']['translation'][:L_t]\n",
    "valid = dataset['train']['translation'][L_t:L_t+L_v]\n",
    "test = dataset['train']['translation'][-L_test:]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = 'translate English to French:' + t['en']\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), \n",
    "                        batch_size=2, pin_memory=True, num_workers=0)\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=2, pin_memory=True, num_workers=0)\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                        batch_size=5, pin_memory=True, num_workers=0)#, sampler=RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(len(train))\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.SGD(model_w.parameters(),w_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_w  = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.SGD(model_v.parameters(),v_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_v  = torch.optim.lr_scheduler.CosineAnnealingLR(v_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> mon nom est kevin</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
       " \"<pad> c'est mon nomci est ma dénomination 321312</s>\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['my name is kevin','it is my nameit is my nameit is my name 321312']\n",
    "for index,i in enumerate(x) :\n",
    "    x[index] = 'translate English to French:' + x[index]\n",
    "y= tokenize(x, tokenizer, max_length = summary_length)\n",
    "input = y[0].cuda()\n",
    "output  = model_v.generate(input)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test(test_dataloader,model):\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        model.eval()\n",
    "        x = Variable(batch[0], requires_grad=False).cuda()\n",
    "        x_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        y = Variable(batch[2], requires_grad=False).cuda()\n",
    "        y_attn = Variable(batch[3], requires_grad=False).cuda()\n",
    "\n",
    "        ls = my_loss(x,x_attn,y,y_attn,model)\n",
    "        logging.info('%s test loss : %f',model.name,ls)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, train_dataloader, valid_dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        logging.info(\"Step count: %d\",step)\n",
    "        \n",
    "        batch_loss_w, batch_loss_v,  batch_count = 0, 0, 0\n",
    "        input_w = Variable(batch[0], requires_grad=False).cuda()\n",
    "        input_w_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        output_w = Variable(batch[2], requires_grad=False).cuda()\n",
    "        output_w_attn = Variable(batch[3], requires_grad=False).cuda()        \n",
    "        input_v = Variable(batch[4], requires_grad=False).cuda()\n",
    "        input_v_attn = Variable(batch[5], requires_grad=False).cuda()      \n",
    "        attn_idx = Variable(batch[6], requires_grad=False).cuda()\n",
    "        \n",
    "        valid_batch = next(iter(valid_dataloader))\n",
    "        valid_input_v      = Variable(valid_batch[0], requires_grad=False).cuda()\n",
    "        valid_input_v_attn = Variable(valid_batch[1], requires_grad=False).cuda()\n",
    "        valid_out_v      = Variable(valid_batch[2], requires_grad=False).cuda()\n",
    "        valid_out_v_attn = Variable(valid_batch[3], requires_grad=False).cuda()\n",
    "        \n",
    "\n",
    "        if epoch <= stop_epoch:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_v, input_v_attn,valid_input_v, valid_input_v_attn, valid_out_v, \n",
    "                valid_out_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "\n",
    "        if epoch <=stop_epoch:\n",
    "            \n",
    "            w_optimizer.zero_grad()\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            logging.info(f\"loss_w (train):{loss_w}\")\n",
    "            batch_loss_w += loss_w.item()\n",
    "            loss_w.backward()\n",
    "            nn.utils.clip_grad_norm(w_model.parameters(), grad_clip)\n",
    "            w_optimizer.step()\n",
    "\n",
    "\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(input_v, input_v_attn, w_model, v_model)\n",
    "            v_loss =  (loss_aug)\n",
    "            logging.info(f\"v_loss (train):{v_loss}\")\n",
    "            batch_loss_v += v_loss.item()\n",
    "            v_loss.backward()\n",
    "            nn.utils.clip_grad_norm(v_model.parameters(), grad_clip)\n",
    "            v_optimizer.step()     \n",
    "            \n",
    "            my_test(test_dataloader,w_model) \n",
    "            my_test(test_dataloader,v_model)      \n",
    "        if step % 1  == 0:\n",
    "            logging.info(str((\"Attention Weights A : \", A.alpha)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/25 05:03:52 PM |\t  Step count: 0\n",
      "12/25 05:04:11 PM |\t  loss_w:3.4845939808292314e-05\n",
      "12/25 05:04:14 PM |\t  v_loss:333.8587341308594\n",
      "12/25 05:04:14 PM |\t  model_w_in_main test loss : 0.837824\n",
      "12/25 05:04:14 PM |\t  model_v_in_main test loss : 0.831462\n",
      "12/25 05:04:14 PM |\t  Step count: 1\n",
      "12/25 05:04:24 PM |\t  loss_w:6.766241131117567e-05\n",
      "12/25 05:04:26 PM |\t  v_loss:111.9421157836914\n",
      "12/25 05:04:26 PM |\t  model_w_in_main test loss : 0.837778\n",
      "12/25 05:04:26 PM |\t  model_v_in_main test loss : 0.868355\n",
      "12/25 05:04:26 PM |\t  Step count: 2\n",
      "12/25 05:04:43 PM |\t  loss_w:1.554307709739078e-05\n",
      "12/25 05:04:46 PM |\t  v_loss:263.779052734375\n",
      "12/25 05:04:47 PM |\t  model_w_in_main test loss : 0.837844\n",
      "12/25 05:04:47 PM |\t  model_v_in_main test loss : 0.852040\n",
      "12/25 05:04:47 PM |\t  Step count: 3\n",
      "12/25 05:04:57 PM |\t  loss_w:1.6379937733290717e-05\n",
      "12/25 05:04:57 PM |\t  v_loss:90.01312255859375\n",
      "12/25 05:04:58 PM |\t  model_w_in_main test loss : 0.837775\n",
      "12/25 05:04:58 PM |\t  model_v_in_main test loss : 0.853961\n",
      "12/25 05:04:58 PM |\t  Step count: 4\n",
      "12/25 05:05:08 PM |\t  loss_w:2.1330436084099347e-06\n",
      "12/25 05:05:09 PM |\t  v_loss:107.91200256347656\n",
      "12/25 05:05:09 PM |\t  model_w_in_main test loss : 0.837734\n",
      "12/25 05:05:09 PM |\t  model_v_in_main test loss : 0.878200\n",
      "12/25 05:05:09 PM |\t  Step count: 5\n",
      "12/25 05:05:22 PM |\t  loss_w:7.858308526920155e-05\n",
      "12/25 05:05:23 PM |\t  v_loss:158.43104553222656\n",
      "12/25 05:05:23 PM |\t  model_w_in_main test loss : 0.837713\n",
      "12/25 05:05:23 PM |\t  model_v_in_main test loss : 0.876339\n",
      "12/25 05:05:23 PM |\t  Step count: 6\n",
      "12/25 05:05:33 PM |\t  loss_w:8.64094981807284e-05\n",
      "12/25 05:05:34 PM |\t  v_loss:82.34506225585938\n",
      "12/25 05:05:34 PM |\t  model_w_in_main test loss : 0.837737\n",
      "12/25 05:05:34 PM |\t  model_v_in_main test loss : 0.880960\n",
      "12/25 05:05:34 PM |\t  Step count: 7\n",
      "12/25 05:05:44 PM |\t  loss_w:4.451858785614604e-06\n",
      "12/25 05:05:45 PM |\t  v_loss:143.1834259033203\n",
      "12/25 05:05:45 PM |\t  model_w_in_main test loss : 0.837820\n",
      "12/25 05:05:45 PM |\t  model_v_in_main test loss : 0.887228\n",
      "12/25 05:05:45 PM |\t  Step count: 8\n",
      "12/25 05:05:55 PM |\t  loss_w:2.421941280772444e-05\n",
      "12/25 05:05:56 PM |\t  v_loss:76.80359649658203\n",
      "12/25 05:05:56 PM |\t  model_w_in_main test loss : 0.837707\n",
      "12/25 05:05:56 PM |\t  model_v_in_main test loss : 0.871066\n",
      "12/25 05:05:56 PM |\t  Step count: 9\n",
      "12/25 05:06:14 PM |\t  loss_w:8.365855137526523e-06\n",
      "12/25 05:06:17 PM |\t  v_loss:198.57867431640625\n",
      "12/25 05:06:17 PM |\t  model_w_in_main test loss : 0.837758\n",
      "12/25 05:06:17 PM |\t  model_v_in_main test loss : 0.880966\n",
      "12/25 05:06:17 PM |\t  Step count: 10\n",
      "12/25 05:06:27 PM |\t  loss_w:3.635210305219516e-05\n",
      "12/25 05:06:28 PM |\t  v_loss:103.07505798339844\n",
      "12/25 05:06:28 PM |\t  model_w_in_main test loss : 0.837781\n",
      "12/25 05:06:28 PM |\t  model_v_in_main test loss : 0.901313\n",
      "12/25 05:06:28 PM |\t  Step count: 11\n",
      "12/25 05:06:42 PM |\t  loss_w:1.7604561435291544e-05\n",
      "12/25 05:06:44 PM |\t  v_loss:205.65298461914062\n",
      "12/25 05:06:45 PM |\t  model_w_in_main test loss : 0.837805\n",
      "12/25 05:06:45 PM |\t  model_v_in_main test loss : 0.910425\n",
      "12/25 05:06:45 PM |\t  Step count: 12\n",
      "12/25 05:06:57 PM |\t  loss_w:1.0456403288117144e-05\n",
      "12/25 05:06:58 PM |\t  v_loss:153.5947265625\n",
      "12/25 05:06:59 PM |\t  model_w_in_main test loss : 0.837647\n",
      "12/25 05:06:59 PM |\t  model_v_in_main test loss : 0.903791\n",
      "12/25 05:06:59 PM |\t  Step count: 13\n",
      "12/25 05:07:15 PM |\t  loss_w:0.00010887128883041441\n",
      "12/25 05:07:18 PM |\t  v_loss:195.16326904296875\n",
      "12/25 05:07:18 PM |\t  model_w_in_main test loss : 0.837806\n",
      "12/25 05:07:18 PM |\t  model_v_in_main test loss : 0.895039\n",
      "12/25 05:07:18 PM |\t  Step count: 14\n",
      "12/25 05:07:29 PM |\t  loss_w:1.320046249020379e-05\n",
      "12/25 05:07:30 PM |\t  v_loss:68.32789611816406\n",
      "12/25 05:07:30 PM |\t  model_w_in_main test loss : 0.837695\n",
      "12/25 05:07:30 PM |\t  model_v_in_main test loss : 0.912959\n",
      "12/25 05:07:30 PM |\t  Step count: 15\n",
      "12/25 05:07:41 PM |\t  loss_w:5.452481218526373e-06\n",
      "12/25 05:07:42 PM |\t  v_loss:61.9101676940918\n",
      "12/25 05:07:43 PM |\t  model_w_in_main test loss : 0.837778\n",
      "12/25 05:07:43 PM |\t  model_v_in_main test loss : 0.950460\n",
      "12/25 05:07:43 PM |\t  Step count: 16\n",
      "12/25 05:07:54 PM |\t  loss_w:3.573971116566099e-05\n",
      "12/25 05:07:55 PM |\t  v_loss:107.85946655273438\n",
      "12/25 05:07:55 PM |\t  model_w_in_main test loss : 0.837789\n",
      "12/25 05:07:55 PM |\t  model_v_in_main test loss : 0.944061\n",
      "12/25 05:07:55 PM |\t  Step count: 17\n",
      "12/25 05:08:09 PM |\t  loss_w:4.6973482312751e-06\n",
      "12/25 05:08:10 PM |\t  v_loss:180.8990478515625\n",
      "12/25 05:08:11 PM |\t  model_w_in_main test loss : 0.837776\n",
      "12/25 05:08:11 PM |\t  model_v_in_main test loss : 0.912931\n",
      "12/25 05:08:11 PM |\t  Step count: 18\n",
      "12/25 05:08:24 PM |\t  loss_w:2.2036351765564177e-06\n",
      "12/25 05:08:26 PM |\t  v_loss:92.52146911621094\n",
      "12/25 05:08:27 PM |\t  model_w_in_main test loss : 0.837776\n",
      "12/25 05:08:27 PM |\t  model_v_in_main test loss : 0.939659\n",
      "12/25 05:08:27 PM |\t  Step count: 19\n",
      "12/25 05:08:41 PM |\t  loss_w:1.5511142919422127e-05\n",
      "12/25 05:08:43 PM |\t  v_loss:142.01950073242188\n",
      "12/25 05:08:43 PM |\t  model_w_in_main test loss : 0.837709\n",
      "12/25 05:08:43 PM |\t  model_v_in_main test loss : 0.928027\n",
      "12/25 05:08:43 PM |\t  Step count: 20\n",
      "12/25 05:08:57 PM |\t  loss_w:3.401488356757909e-05\n",
      "12/25 05:08:58 PM |\t  v_loss:106.68106079101562\n",
      "12/25 05:08:58 PM |\t  model_w_in_main test loss : 0.837801\n",
      "12/25 05:08:59 PM |\t  model_v_in_main test loss : 0.932936\n",
      "12/25 05:08:59 PM |\t  Step count: 21\n",
      "12/25 05:09:10 PM |\t  loss_w:4.3723008275264874e-05\n",
      "12/25 05:09:11 PM |\t  v_loss:38.077720642089844\n",
      "12/25 05:09:11 PM |\t  model_w_in_main test loss : 0.837717\n",
      "12/25 05:09:11 PM |\t  model_v_in_main test loss : 0.961371\n",
      "12/25 05:09:11 PM |\t  Step count: 22\n",
      "12/25 05:09:21 PM |\t  loss_w:1.103351451092749e-06\n",
      "12/25 05:09:21 PM |\t  v_loss:34.52643585205078\n",
      "12/25 05:09:22 PM |\t  model_w_in_main test loss : 0.837741\n",
      "12/25 05:09:22 PM |\t  model_v_in_main test loss : 0.984266\n",
      "12/25 05:09:22 PM |\t  Step count: 23\n",
      "12/25 05:09:35 PM |\t  loss_w:8.425085979979485e-05\n",
      "12/25 05:09:36 PM |\t  v_loss:96.24021911621094\n",
      "12/25 05:09:37 PM |\t  model_w_in_main test loss : 0.837799\n",
      "12/25 05:09:37 PM |\t  model_v_in_main test loss : 0.949818\n",
      "12/25 05:09:37 PM |\t  Step count: 24\n",
      "12/25 05:09:48 PM |\t  loss_w:1.5749733393022325e-06\n",
      "12/25 05:09:48 PM |\t  v_loss:54.09656524658203\n",
      "12/25 05:09:49 PM |\t  model_w_in_main test loss : 0.837681\n",
      "12/25 05:09:49 PM |\t  model_v_in_main test loss : 1.011402\n",
      "12/25 05:09:49 PM |\t  Step count: 25\n",
      "12/25 05:10:03 PM |\t  loss_w:8.713848365005106e-05\n",
      "12/25 05:10:04 PM |\t  v_loss:147.3315887451172\n",
      "12/25 05:10:05 PM |\t  model_w_in_main test loss : 0.837700\n",
      "12/25 05:10:05 PM |\t  model_v_in_main test loss : 0.998505\n",
      "12/25 05:10:05 PM |\t  Step count: 26\n",
      "12/25 05:10:16 PM |\t  loss_w:1.184666643894161e-06\n",
      "12/25 05:10:17 PM |\t  v_loss:48.96347427368164\n",
      "12/25 05:10:17 PM |\t  model_w_in_main test loss : 0.837680\n",
      "12/25 05:10:17 PM |\t  model_v_in_main test loss : 0.935457\n",
      "12/25 05:10:17 PM |\t  Step count: 27\n",
      "12/25 05:10:27 PM |\t  loss_w:8.2224010839127e-05\n",
      "12/25 05:10:27 PM |\t  v_loss:50.80058288574219\n",
      "12/25 05:10:28 PM |\t  model_w_in_main test loss : 0.837611\n",
      "12/25 05:10:28 PM |\t  model_v_in_main test loss : 0.993098\n",
      "12/25 05:10:28 PM |\t  Step count: 28\n",
      "12/25 05:10:42 PM |\t  loss_w:1.830031214922201e-05\n",
      "12/25 05:10:44 PM |\t  v_loss:124.5927734375\n",
      "12/25 05:10:44 PM |\t  model_w_in_main test loss : 0.837759\n",
      "12/25 05:10:44 PM |\t  model_v_in_main test loss : 1.019274\n",
      "12/25 05:10:44 PM |\t  Step count: 29\n",
      "12/25 05:10:54 PM |\t  loss_w:0.00014213498798198998\n",
      "12/25 05:10:55 PM |\t  v_loss:29.31704330444336\n",
      "12/25 05:10:55 PM |\t  model_w_in_main test loss : 0.837740\n",
      "12/25 05:10:55 PM |\t  model_v_in_main test loss : 1.001963\n",
      "12/25 05:10:55 PM |\t  Step count: 30\n",
      "12/25 05:11:07 PM |\t  loss_w:1.830716610129457e-05\n",
      "12/25 05:11:09 PM |\t  v_loss:97.48507690429688\n",
      "12/25 05:11:09 PM |\t  model_w_in_main test loss : 0.837924\n",
      "12/25 05:11:09 PM |\t  model_v_in_main test loss : 0.968046\n",
      "12/25 05:11:09 PM |\t  Step count: 31\n",
      "12/25 05:11:21 PM |\t  loss_w:2.6750687538878992e-05\n",
      "12/25 05:11:22 PM |\t  v_loss:15.24324893951416\n",
      "12/25 05:11:22 PM |\t  model_w_in_main test loss : 0.837886\n",
      "12/25 05:11:22 PM |\t  model_v_in_main test loss : 0.997801\n",
      "12/25 05:11:22 PM |\t  Step count: 32\n",
      "12/25 05:11:36 PM |\t  loss_w:2.0969067918485962e-05\n",
      "12/25 05:11:37 PM |\t  v_loss:105.4897232055664\n",
      "12/25 05:11:37 PM |\t  model_w_in_main test loss : 0.837904\n",
      "12/25 05:11:38 PM |\t  model_v_in_main test loss : 1.046971\n",
      "12/25 05:11:38 PM |\t  Step count: 33\n",
      "12/25 05:11:49 PM |\t  loss_w:0.0001240108540514484\n",
      "12/25 05:11:50 PM |\t  v_loss:64.3377685546875\n",
      "12/25 05:11:50 PM |\t  model_w_in_main test loss : 0.837773\n",
      "12/25 05:11:50 PM |\t  model_v_in_main test loss : 0.997889\n",
      "12/25 05:11:50 PM |\t  Step count: 34\n",
      "12/25 05:12:00 PM |\t  loss_w:0.00017095920338761061\n",
      "12/25 05:12:01 PM |\t  v_loss:16.632179260253906\n",
      "12/25 05:12:01 PM |\t  model_w_in_main test loss : 0.837803\n",
      "12/25 05:12:01 PM |\t  model_v_in_main test loss : 1.022869\n",
      "12/25 05:12:01 PM |\t  Step count: 35\n",
      "12/25 05:12:17 PM |\t  loss_w:4.735474874451029e-07\n",
      "12/25 05:12:19 PM |\t  v_loss:56.63972473144531\n",
      "12/25 05:12:19 PM |\t  model_w_in_main test loss : 0.837810\n",
      "12/25 05:12:19 PM |\t  model_v_in_main test loss : 0.964959\n",
      "12/25 05:12:19 PM |\t  Step count: 36\n",
      "12/25 05:12:29 PM |\t  loss_w:2.9185324819991365e-05\n",
      "12/25 05:12:29 PM |\t  v_loss:25.564388275146484\n",
      "12/25 05:12:30 PM |\t  model_w_in_main test loss : 0.837845\n",
      "12/25 05:12:30 PM |\t  model_v_in_main test loss : 0.991259\n",
      "12/25 05:12:30 PM |\t  Step count: 37\n",
      "12/25 05:12:44 PM |\t  loss_w:1.5116717804630753e-05\n",
      "12/25 05:12:46 PM |\t  v_loss:107.11442565917969\n",
      "12/25 05:12:47 PM |\t  model_w_in_main test loss : 0.837843\n",
      "12/25 05:12:47 PM |\t  model_v_in_main test loss : 0.929151\n",
      "12/25 05:12:47 PM |\t  Step count: 38\n",
      "12/25 05:12:58 PM |\t  loss_w:4.893025470664725e-05\n",
      "12/25 05:12:58 PM |\t  v_loss:22.11359405517578\n",
      "12/25 05:12:59 PM |\t  model_w_in_main test loss : 0.837798\n",
      "12/25 05:12:59 PM |\t  model_v_in_main test loss : 0.947981\n",
      "12/25 05:12:59 PM |\t  Step count: 39\n",
      "12/25 05:13:14 PM |\t  loss_w:2.2766129404772073e-05\n",
      "12/25 05:13:16 PM |\t  v_loss:178.4034881591797\n",
      "12/25 05:13:16 PM |\t  model_w_in_main test loss : 0.837909\n",
      "12/25 05:13:16 PM |\t  model_v_in_main test loss : 0.931395\n",
      "12/25 05:13:16 PM |\t  Step count: 40\n",
      "12/25 05:13:28 PM |\t  loss_w:0.0001246021711267531\n",
      "12/25 05:13:30 PM |\t  v_loss:43.04435729980469\n",
      "12/25 05:13:30 PM |\t  model_w_in_main test loss : 0.837835\n",
      "12/25 05:13:30 PM |\t  model_v_in_main test loss : 0.947299\n",
      "12/25 05:13:30 PM |\t  Step count: 41\n",
      "12/25 05:13:45 PM |\t  loss_w:3.7631230952683836e-05\n"
     ]
    }
   ],
   "source": [
    "my_train(begin_epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, w_lr,v_lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,  6206,  6667,    27,     1])\n",
    "tokenizer.decode([13959,  1566,    12,  2379,    10, 17608,   994,    27,     1,     0])\n",
    "logging.info(\"vocab size : %d\",model_v.vocab_size)\n",
    "logit = torch.load('logits.pt')\n",
    "target = torch.load('target_ids.pt')\n",
    "tokenizer.decode(target[0])\n",
    "logit.shape\n",
    "_,maxx = torch.max(logit,dim=-1,keepdim=True)\n",
    "maxx.shape\n",
    "tokenizer.decode(maxx[0].squeeze(-1))\n",
    "\n",
    "model_v.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
