{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23488/3883773644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperparams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseed_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# BART as ds model and to produce articles given summaries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BartForConditionalGeneration\n",
    "from hyperparams import seed_\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|██████████| 3/3 [00:00<00:00, 40.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287113"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "Input shape: \n",
      "torch.Size([5, 500]) torch.Size([5, 500])\n",
      "Target shape: \n",
      "torch.Size([5, 74]) torch.Size([5, 74])\n"
     ]
    }
   ],
   "source": [
    "from data_set import *\n",
    "from BART import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from transformers import BartTokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "bart_criterion = torch.nn.CrossEntropyLoss(ignore_index = bart_tokenizer.pad_token_id, reduction='none')\n",
    "bart_criterion = bart_criterion.cuda()\n",
    "bart = BART(bart_criterion,bart_tokenizer)\n",
    "bart = bart.cuda()\n",
    "\n",
    "train_data = get_train_Dataset(dataset,5 , bart_tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), \n",
    "                        batch_size=1, pin_memory=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50265, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.embed_tokens.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 41995, 46116, 4356, 259, 16, 14, 596, 4356, 259, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.encode(\"reason haha im here is that why im here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 41995, 46116,  4356,   259,    16,    14,   596,  4356,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer('reason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im herereason haha im here is that why im here', return_tensors='pt', padding=True, truncation = True, max_length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 1 \tbatch len: 5\n",
      "torch.Size([1, 500])\n",
      "1392\n",
      "STEP: 2 \tbatch len: 5\n",
      "torch.Size([1, 500])\n",
      "2524\n",
      "STEP: 3 \tbatch len: 5\n",
      "torch.Size([1, 500])\n",
      "1269\n",
      "STEP: 4 \tbatch len: 5\n",
      "torch.Size([1, 500])\n",
      "1543\n",
      "STEP: 5 \tbatch len: 5\n",
      "torch.Size([1, 500])\n",
      "2362\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(\"STEP:\",step+1,'\\tbatch len:',len(batch))\n",
    "    # print(batch[-1])\n",
    "\n",
    "    # Input and its attentions\n",
    "    article_bart = Variable(batch[0], requires_grad=False).cuda()\n",
    "    # print(batch[0])\n",
    "    article_bart_attn = Variable(batch[1], requires_grad=False).cuda()    \n",
    "    print(batch[0].shape)\n",
    "    output =bart_tokenizer.batch_decode(bart.generate(article_bart))\n",
    "    print(len(output[0]))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 1.90MB/s]\n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 2.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,T5ForConditionalGeneration,T5Tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "model(t5_tokenizer(\"hi im kevin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16244/820899670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1570\u001b[0m             \u001b[1;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1571\u001b[1;33m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1572\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1573\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"decoder_\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model(inputs_embeds = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 512)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 8)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je fais un projet à l'occasion de l'Action d'\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer('translate English to French: I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.I am doing project on Thanksgiving.', return_tensors='pt').input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation_utils:\n",
      "\n",
      "generate(input_ids: Union[torch.LongTensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, encoder_no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, max_time: Union[float, NoneType] = None, max_new_tokens: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, forced_bos_token_id: Union[int, NoneType] = None, forced_eos_token_id: Union[int, NoneType] = None, remove_invalid_values: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor] method of transformers.models.t5.modeling_t5.T5ForConditionalGeneration instance\n",
      "    Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      "    multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
      "    \n",
      "    Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
      "    attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
      "    indicated are the default values of those config.\n",
      "    \n",
      "    Most of these parameters are explained in more detail in `this blog post\n",
      "    <https://huggingface.co/blog/how-to-generate>`__.\n",
      "    \n",
      "    Parameters:\n",
      "    \n",
      "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            The sequence used as a prompt for the generation. If :obj:`None` the method initializes it with\n",
      "            :obj:`bos_token_id` and a batch size of 1.\n",
      "        max_length (:obj:`int`, `optional`, defaults to :obj:`model.config.max_length`):\n",
      "            The maximum length of the sequence to be generated.\n",
      "        max_new_tokens (:obj:`int`, `optional`, defaults to None):\n",
      "            The maximum numbers of tokens to generate, ignore the current number of tokens. Use either\n",
      "            :obj:`max_new_tokens` or :obj:`max_length` but not both, they serve the same purpose.\n",
      "        min_length (:obj:`int`, `optional`, defaults to 10):\n",
      "            The minimum length of the sequence to be generated.\n",
      "        do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "        early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
      "        num_beams (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of beams for beam search. 1 means no beam search.\n",
      "        temperature (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            The value used to module the next token probabilities.\n",
      "        top_k (:obj:`int`, `optional`, defaults to 50):\n",
      "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "        top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
      "            higher are kept for generation.\n",
      "        repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
      "            <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
      "        pad_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `padding` token.\n",
      "        bos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `beginning-of-sequence` token.\n",
      "        eos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `end-of-sequence` token.\n",
      "        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
      "            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
      "            sequences.\n",
      "        no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to int > 0, all ngrams of that size can only occur once.\n",
      "        encoder_no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to int > 0, all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the\n",
      "            ``decoder_input_ids``.\n",
      "        bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
      "            List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
      "            should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
      "            add_prefix_space=True).input_ids`.\n",
      "        num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
      "            The number of independently computed returned sequences for each element in the batch.\n",
      "        max_time(:obj:`float`, `optional`, defaults to None):\n",
      "            The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      "            finish the current pass after allocated time has been passed.\n",
      "        attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
      "            tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
      "            shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
      "            <../glossary.html#attention-mask>`__\n",
      "        decoder_start_token_id (:obj:`int`, `optional`):\n",
      "            If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
      "        use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "            speed up decoding.\n",
      "        num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
      "            beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
      "        diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
      "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      "            at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
      "            enabled.\n",
      "        prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID :obj:`batch_id` and\n",
      "            :obj:`input_ids`. It has to return a list with the allowed tokens for the next generation step\n",
      "            conditioned on the batch ID :obj:`batch_id` and the previously generated tokens :obj:`inputs_ids`. This\n",
      "            argument is useful for constrained generation conditioned on the prefix, as described in\n",
      "            `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
      "        output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      "            returned tensors for more details.\n",
      "        output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      "            for more details.\n",
      "        output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      "        return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      "        forced_bos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the token to force as the first generated token after the :obj:`decoder_start_token_id`.\n",
      "            Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token\n",
      "            needs to be the target language token.\n",
      "        forced_eos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the token to force as the last generated token when :obj:`max_length` is reached.\n",
      "        remove_invalid_values (:obj:`bool`, `optional`):\n",
      "            Whether to remove possible `nan` and `inf` outputs of the model to prevent the generation method to\n",
      "            crash. Note that using ``remove_invalid_values`` can slow down generation.\n",
      "        synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      "    \n",
      "        model_kwargs:\n",
      "            Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
      "            model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
      "            kwargs should be prefixed with `decoder_`.\n",
      "    \n",
      "    Return:\n",
      "        :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
      "        :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
      "        ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
      "    \n",
      "            If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
      "            possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
      "    \n",
      "            If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
      "            :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
      "    \n",
      "    Examples::\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> # do greedy decoding without providing a prompt\n",
      "        >>> outputs = model.generate(max_length=40)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      "        >>> document = (\n",
      "        ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
      "        ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
      "        ... )\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
      "        >>> # with T5 encoder-decoder model conditioned on short news article.\n",
      "        >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> input_context = \"The dog\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 candidates using sampling\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
      "        >>> # \"Legal\" is one of the control codes for ctrl\n",
      "        >>> input_context = \"Legal My neighbor is\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "        >>> input_context = \"My cute dog\"\n",
      "        >>> # get tokens of words that should not be generated\n",
      "        >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate sequences without allowing bad_words to be generated\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus_euconst (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\opus_euconst\\en-fr\\1.0.0\\d1e611a011f28fdda67a97024820e0a3813b4e4decca194d9a20b3207a39b908)\n",
      "100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset('opus100','en-fr')#large\n",
    "dataset = load_dataset('opus_euconst','en-fr')#small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': [{'en': 'The time now is 05:08 .',\n",
       "   'fr': 'The time now is 05:05 .'},\n",
       "  {'en': 'This Regulation shall enter into force on the seventh day following its publication in the Official Journal of the European Union.',\n",
       "   'fr': \"Le présent règlement entre en vigueur le septième jour suivant celui de sa publication au Journal officiel de l'Union européenne.\"},\n",
       "  {'en': \"Hello, what's that?\", 'fr': \"Qu'est-ce que c'est que ça ?\"},\n",
       "  {'en': 'And then I will teach you everything i know.',\n",
       "   'fr': \"Et alors, je t'apprendrai tout ce que je sais.\"},\n",
       "  {'en': 'Did you find something?', 'fr': 'Par ici !'},\n",
       "  {'en': 'Article 6', 'fr': 'Article 6'},\n",
       "  {'en': \"Oh, honey, it's not your fault.\",\n",
       "   'fr': \"- Tu n'es pas responsable.\"},\n",
       "  {'en': \"I'm onto him now.\", 'fr': 'Je le tiens.'},\n",
       "  {'en': \"DG XVI's Internet site (electronic address: http://www.cec.lu/en/comm/dg16/dg16home.html) now contains detailed information in English on pilot projects on innovation, the Information society, new sources of employment and cultural cooperation; the application forms for these innovatory measures can be downloaded in all languages of the European Union.\",\n",
       "   'fr': \"Par ailleurs, le site Internet de la DG XVI (adresse électronique: http://www.cec.lu/en/comm/dg16/dg16home.html) contient désormais une information détaillée en anglais sur les projets pilotes relatifs à la promotion de l'innovation, à la société de l'information, aux nouveaux gisements d'emploi et à la coopération culturelle; et il permet de décharger les formulaires d'inscription pour ces actions novatrices dans toutes les langues de l'Union européenne.\"},\n",
       "  {'en': 'Here it is.', 'fr': 'Le voilà.'},\n",
       "  {'en': \"Steven, why don't you read it?\",\n",
       "   'fr': 'Steven, pourquoi ne le lis-tu pas ?'},\n",
       "  {'en': 'Western European and Other States Group (monthly meeting)',\n",
       "   'fr': \"Groupe des États d'Europe occidentale et autres États (réunion mensuelle)\"},\n",
       "  {'en': \"I'll take care of the kids.\", 'fr': \"Je m'occuperai des enfants.\"},\n",
       "  {'en': 'Oh shit!', 'fr': 'Oh putain !'},\n",
       "  {'en': 'Oh, sorry.', 'fr': 'Oh, désolé.'},\n",
       "  {'en': \"I tell you, sometimes I can't take it.\",\n",
       "   'fr': \"C'est trop dur, parfois.\"},\n",
       "  {'en': 'How did you know that?',\n",
       "   'fr': '- Comment le savez-vous ? - Vous venez de me le dire.'},\n",
       "  {'en': \"Where's Prue?\", 'fr': 'Où est Prue?'},\n",
       "  {'en': 'But mother, it might have been something important!',\n",
       "   'fr': \"C'est peut-être important.\"},\n",
       "  {'en': \"Irisa's not the coddling type.\",\n",
       "   'fr': \"Elle n'est pas du genre câlineuse.\"}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.zeros((20,20))\n",
    "print(x)\n",
    "x[:,:15].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
